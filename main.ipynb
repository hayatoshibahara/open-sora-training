{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f68b9b2b",
   "metadata": {},
   "source": [
    "# Open-Sora 2.0 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f47a17e",
   "metadata": {},
   "source": [
    "## ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646fd000",
   "metadata": {},
   "source": [
    "è¨“ç·´ã¯3ã¤ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã§æ§‹æˆ:\n",
    "\n",
    "1. ä½è§£åƒåº¦å‹•ç”»ã«ã‚ˆã‚‹T2Vï¼ˆText to Videoï¼‰ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´\n",
    "1. ä½è§£åƒåº¦å‹•ç”»ã«ã‚ˆã‚‹I2Vï¼ˆImage to Videoï¼‰ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´\n",
    "1. é«˜è§£åƒåº¦å‹•ç”»ã«ã‚ˆã‚‹I2Vãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574c78b7",
   "metadata": {},
   "source": [
    "è¨“ç·´ã®ã‚³ã‚¹ãƒˆã®å†…è¨³ã¯ã€ã‚¹ãƒ†ãƒ¼ã‚¸1ã®ã‚³ã‚¹ãƒˆãŒ50%ã‚’å ã‚ã‚‹:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef9cce",
   "metadata": {},
   "source": [
    "![](image/table3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1e9a74",
   "metadata": {},
   "source": [
    "è¨“ç·´ã®åŠ¹ç‡åŒ–ã®æˆ¦ç•¥:\n",
    "\n",
    "- 11Bï¼ˆ110å„„ï¼‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®Fluxã®è’¸ç•™ãƒ¢ãƒ‡ãƒ«ã§ã€T2Vãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–\n",
    "    - Fluxã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹T2Iï¼ˆText to Imageï¼‰ãƒ¢ãƒ‡ãƒ«\n",
    "- é«˜å“è³ªãªå‹•ç”»ãƒ‡ãƒ¼ã‚¿ã§ã€è¨“ç·´åŠ¹ç‡ã‚’å‘ä¸Š\n",
    "    - PixArtã‹ã‚‰ç€æƒ³\n",
    "    - ã‚¹ãƒ†ãƒƒãƒ—1, 2ã§ã¯ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰é«˜å“è³ªãªã‚µãƒ–ã‚»ãƒƒãƒˆã‚’æŠ½å‡º\n",
    "    - ã‚¹ãƒ†ãƒƒãƒ—3ã§ã¯ã€ã‚ˆã‚Šå³ã—ã„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’æŠ½å‡º\n",
    "- ä½è§£åƒåº¦å‹•ç”»ã§ã®å‹•ãã®å­¦ç¿’\n",
    "    - é«˜è§£åƒåº¦å‹•ç”»ã®è¨“ç·´ã‚³ã‚¹ãƒˆã¯é«˜ã„ãŸã‚ã€å¤šæ§˜ãªå‹•ãã¯ä½è§£åƒåº¦å‹•ç”»ã‚’ä¸­å¿ƒã«è¨“ç·´\n",
    "        - 128ãƒ•ãƒ¬ãƒ¼ãƒ 768pxã®å‹•ç”»ã®è¨“ç·´ã¯ã€256pxã®å ´åˆã‚ˆã‚Šã‚‚40å€é…ã„\n",
    "        - ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®è¨ˆç®—è¤‡é›‘åº¦ãŒäºŒæ¬¡é–¢æ•°çš„ã«å¢—å¤§ã™ã‚‹ãŸã‚\n",
    "    - å‡ºåŠ›ãŒã¼ã‚„ã‘ã‚‹ãŸã‚ã€é«˜è§£åƒåº¦å‹•ç”»ã®è¨“ç·´ã§å“è³ªã‚’å‘ä¸Š\n",
    "- ã‚¹ãƒ†ãƒƒãƒ—2ã®ã‚¢ãƒƒãƒ—ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å­¦ç¿’ã¯ã€T2Vã§ã¯ãªãI2Vã‚’æ¡ç”¨ã—åŠ¹ç‡åŒ–:\n",
    "    - é™æ­¢ç”»ã‚’æ¡ä»¶ä»˜ã‘ã™ã‚‹ã“ã¨ã§ã€å‹•ãã®ç”Ÿæˆã«é›†ä¸­ã§ãã‚‹ãŸã‚\n",
    "- é«˜è§£åƒåº¦å‹•ç”»ã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’çŸ­ç¸®ã—ã€è¨“ç·´ã‚’åŠ¹ç‡åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bee32fa",
   "metadata": {},
   "source": [
    "Open-Sora 2.0ã®è¨“ç·´ã‚³ã‚¹ãƒˆã¯ã€5~10å€ä½ã„:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7a4a13",
   "metadata": {},
   "source": [
    "![](image/fig7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50432d84",
   "metadata": {},
   "source": [
    "è¨“ç·´è¨­å®šã¯ã€Open-Sora 1.2ã«åŸºã¥ã:\n",
    "\n",
    "- ç›®çš„é–¢æ•°ã¯Flow Matchingã‚’æ¡ç”¨\n",
    "- ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¯AdamWã‚’ä½¿ç”¨\n",
    "    - $\\beta$å€¤ã¯$(0.9, 0.999)$\n",
    "    - $\\epsilon$ã¯$1\\times 10^{-15}$\n",
    "    - é‡ã¿æ¸›è¡°ã¯ä½¿ç”¨ã—ãªã„\n",
    "    - å­¦ç¿’ç‡\n",
    "        - ã‚¹ãƒ†ãƒ¼ã‚¸1, 2\n",
    "            - æœ€åˆã®4ä¸‡ã‚¹ãƒ†ãƒƒãƒ—ã¯$5\\times 10^{-5}$\n",
    "            - æœ€å¾Œã®4.5ä¸‡ã‚¹ãƒ†ãƒƒãƒ—ã¯$3\\times 10^{-5}$\n",
    "        - ã‚¹ãƒ†ãƒ¼ã‚¸3\n",
    "            - $1\\times 10^{-5}$\n",
    "    - è³¼è²·ãƒãƒ«ãƒ ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®é–¾å€¤ã¯$1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298a6ed7",
   "metadata": {},
   "source": [
    "Flow matchingã¯ã€Stable Diffusion 3ï¼ˆSD3ï¼‰ã¨é¡ä¼¼:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathbb{E}_{t,X_{0},X_{1}}[||f_{\\theta}(X_{t},t,y)-(X_{0}-X_{1})||]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16825ea",
   "metadata": {},
   "source": [
    "- $X_0$: å‹•ç”»ã®æ½œåœ¨è¡¨ç¾\n",
    "- $X_1 \\sim \\mathcal{N}(0,1)$: ã‚¬ã‚¦ã‚¹ãƒã‚¤ã‚º\n",
    "- $t$: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆãƒã‚¤ã‚ºå¼·åº¦ï¼‰\n",
    "- $X_t=(1-t)X_0 + tX_1$: è£œé–“ã•ã‚ŒãŸæ½œåœ¨è¡¨ç¾ï¼ˆãƒã‚¤ã‚ºã‚ã‚Šã®æ½œåœ¨è¡¨ç¾ï¼‰\n",
    "- $y$: ãƒ†ã‚­ã‚¹ãƒˆã‚„ç”»åƒãªã©ã®æ¡ä»¶\n",
    "- $(X_0 - X_1)$: æ­£è§£ã®é€Ÿåº¦ï¼ˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒã‚¤ã‚ºã¸ã®å¤‰åŒ–ã®æ–¹å‘ï¼‰\n",
    "- $\\mathbb{E}_{t, X_0, X_1}$: æ¡ä»¶ã‚’å¤‰ãˆãŸæ™‚ã®å¹³å‡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae6e92",
   "metadata": {},
   "source": [
    "$t$ã¯ã€å¯¾æ•°æ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€$X_0$ã®å½¢çŠ¶ã«åˆã‚ã›ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼š\n",
    "\n",
    "$$\n",
    "t' = \\frac{at}{1 + (a-1)t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cd5f9a",
   "metadata": {},
   "source": [
    "- $a$: $T\\times H\\times W$ã«æ¯”ä¾‹ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "- é«˜è§£åƒåº¦ã§ã®é•·æ™‚é–“ã®ç”»åƒã¯ãƒã‚¤ã‚ºã®å½±éŸ¿ã‚’å—ã‘ã‚„ã™ã„ãŸã‚\n",
    "- æ¨è«–æ™‚ã‚‚åŒæ§˜ã®æ–¹æ³•ãŒé©ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb03915",
   "metadata": {},
   "source": [
    "ãƒãƒ«ãƒãƒã‚±ãƒƒãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ¡ç”¨:\n",
    "\n",
    "- ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãƒ»è§£åƒåº¦ãƒ»ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ãŒä¼¼ãŸå‹•ç”»ã‚’ãƒã‚±ãƒ„ã«åˆ†ã‘ã¦ã€ãƒã‚±ãƒ„ã”ã¨ã«ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’èª¿æ•´ã™ã‚‹æ‰‹æ³•\n",
    "    - ãƒ¡ãƒ¢ãƒªä¸è¶³ï¼ˆOOMï¼‰ã®å›é¿\n",
    "    - ä¸¦åˆ—è¨ˆç®—æ™‚ã®è¨“ç·´æ™‚é–“ã®å‡ä¸€åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580846f9",
   "metadata": {},
   "source": [
    "ã‚¹ãƒ†ãƒ¼ã‚¸1, 2ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ:\n",
    "\n",
    "![](image/table4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece6997d",
   "metadata": {},
   "source": [
    "ã‚¹ãƒ†ãƒ¼ã‚¸3ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼ˆContext parallelism 4ï¼‰:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0decbf9",
   "metadata": {},
   "source": [
    "![](image/table5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f930f7dd",
   "metadata": {},
   "source": [
    "é«˜åœ§ç¸®ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ï¼ˆDC-AEï¼‰ã‚’ä½¿ç”¨ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ã‚¹ãƒˆã‚’ã•ã‚‰ã«å‰Šæ¸›"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382509ce",
   "metadata": {},
   "source": [
    "DC-AEã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€å‹•ç”»ç”Ÿæˆã«å¿…è¦ãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒå¤§å¹…ã«å‰Šæ¸›ã§ãã‚‹:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2af738",
   "metadata": {},
   "source": [
    "$$\n",
    "D_{token} = D_{T} \\times D_{H} \\times D_{W} \\times P_{T} \\times P_{H} \\times P_{W}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c126dd2e",
   "metadata": {},
   "source": [
    "- $D_{\\text{token}}$: ãƒˆãƒ¼ã‚¯ãƒ³ãƒ»ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒ«æ¯”ï¼ˆã©ã®ãã‚‰ã„å°ã•ãåœ§ç¸®ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã™ã‚‹ã‹ã®æ¯”ç‡ï¼‰\n",
    "- $D_T, D_H, H_W$: æ™‚é–“ãƒ»é«˜ã•ãƒ»å¹…ã®æ¬¡å…ƒã«ãŠã‘ã‚‹ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®åœ§ç¸®ç‡\n",
    "- $P_T, P_H, P_W$: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã™ã‚‹éš›ã®ãƒ‘ãƒƒãƒåˆ†å‰²ã‚µã‚¤ã‚º\n",
    "- Hunyuan Videoã®$D_{\\text{token}}$ã¯$4096$ã€DC-AEã®$D_{\\text{token}}$ã¯$1024$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d73b6eb",
   "metadata": {},
   "source": [
    "ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®æ€§èƒ½ã¯ã€æƒ…å ±ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”ã§äºˆæ¸¬ã§ãã‚‹:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc2ad92",
   "metadata": {},
   "source": [
    "$$\n",
    "D_{info} = \\frac{D_{T} \\times D_{H} \\times D_{W} \\times C_{in}}{C_{out}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f77401f",
   "metadata": {},
   "source": [
    "- $D_{\\text{info}}$: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãŒæ½œåœ¨è¡¨ç¾ã«ãªã‚‹ã¨ãã«æƒ…å ±é‡ãŒåœ§ç¸®ã•ã‚ŒãŸæ¯”ç‡\n",
    "- $C_{\\text{in}}$: å…¥åŠ›ãƒãƒ£ãƒ³ãƒãƒ«æ•°ï¼ˆRGBã®å ´åˆ$3$ï¼‰\n",
    "- $C_{\\text{out}}$: å‡ºåŠ›ãƒãƒ£ãƒ³ãƒãƒ«æ•°ï¼ˆæ½œåœ¨è¡¨ç¾ã®ãƒãƒ£ãƒ³ãƒãƒ«æ•°ï¼‰\n",
    "- DC-AEã®ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯ã€Hunyuan Video VAEã¨StepVideo VAEã®$D_{\\text{info}}$ã¨ä¸€è‡´ã™ã‚‹ã‚ˆã†ã«è¨­è¨ˆ\n",
    "- å…·ä½“çš„ã«ã¯å‡ºåŠ›ãƒãƒ£ãƒ³ãƒãƒ«æ•°ã‚’å¢—ã‚„ã™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82e52bc",
   "metadata": {},
   "source": [
    "DC-AEã®ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ã¨ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ã«ã¯ã€å‹¾é…ä¼æ’­ã®å•é¡ŒãŒã‚ã‚‹:\n",
    "\n",
    "- ãƒ”ã‚¯ã‚»ãƒ«ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã¨ãƒ”ã‚¯ã‚»ãƒ«ã‚¢ãƒ³ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã‚’ä½¿ç”¨ã—ãŸæ®‹å·®æ¥ç¶šã‚’å°å…¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4b044c",
   "metadata": {},
   "source": [
    "DC-AEã§å‡ºåŠ›ã—ãŸé«˜åœ§ç¸®ã®æ½œåœ¨å¤‰æ•°ã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹ã¨ã€å‹•ç”»ç”Ÿæˆã®å“è³ªãŒä½ããªã‚‹:\n",
    "\n",
    "- ãƒãƒ£ãƒ³ãƒãƒ«æ•°ãŒå¤šã„ã¨ã€æ½œåœ¨ç©ºé–“æ§‹é€ ï¼ˆlatent space structureï¼‰ã®æœ€é©åŒ–ãŒé›£ã—ããªã‚‹ãŸã‚\n",
    "- DC-AEè¨“ç·´å¾Œã«ã€ç¬¬3å±¤ã®æ½œåœ¨è¡¨ç¾ã‚’DINOv2ã«åˆã‚ã›ã‚‹ãŸã‚ã®è’¸ç•™æå¤±ã‚’é©ç”¨ã—ã€æœ€é©åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8639f8",
   "metadata": {},
   "source": [
    "DC-AEã‚’ä½¿ç”¨ã—ãŸå‹•ç”»ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´:\n",
    "\n",
    "1. 2000ä¸‡ä»¶ã®æœ€å¤§33ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‹•ç”»ã‚’ã€1.7ä¸‡ã‚¹ãƒ†ãƒƒãƒ—ã§è¨“ç·´\n",
    "2. 200ä¸‡ä»¶ã®æœ€å¤§128ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‹•ç”»ã‚’ã€8000ã‚¹ãƒ†ãƒƒãƒ—ã§è¨“ç·´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1697a8a5",
   "metadata": {},
   "source": [
    "DC-AEã‚’ä½¿ç”¨ã—ãŸå‹•ç”»ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯ã€Hunyuan Video VAEã‚ˆã‚Šã‚‚ã€è¨“ç·´ã¨æ¨è«–ã§é«˜ã„åŠ¹ç‡æ€§:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46688e6",
   "metadata": {},
   "source": [
    "![](image/fig8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e26d444",
   "metadata": {},
   "source": [
    "DC-AEã¯ã€256ãƒãƒ£ãƒãƒ«ã¨128ãƒãƒ£ãƒãƒ«ã®2ã¤ã‚’æœ€åˆã‹ã‚‰è¨“ç·´:\n",
    "\n",
    "1. å†æ§‹æˆæå¤±$\\mathcal{L}_1$ã¨çŸ¥è¦šæå¤±$\\mathcal{L}_{\\text{LIPIS}}$ã‚’ä½¿ç”¨ã—ã¦ã€25ä¸‡ã‚¹ãƒ†ãƒƒãƒ—è¨“ç·´:\n",
    "    $$\n",
    "    \\mathcal{L} = \\mathcal{L}_{1} + 0.5\\mathcal{L}_{LPIPS}\n",
    "    $$\n",
    "2. æ•µå¯¾çš„æå¤±$\\mathcal{L}_{\\text{adv}}$ã‚’ä½¿ç”¨ã—ã¦ã€20ä¸‡ã‚¹ãƒ†ãƒƒãƒ—è¨“ç·´:\n",
    "    $$\n",
    "    \\mathcal{L} = \\mathcal{L_1} + 0.5\\mathcal{L_{LPIPS}} + 0.05\\mathcal{L_{\\text{adv}}}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73af25fc",
   "metadata": {},
   "source": [
    "DC-AEã®è¨“ç·´è¨­å®š:\n",
    "\n",
    "- ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒƒãƒã‚µã‚¤ã‚º$1$ã§$8$å€‹ã®GPUã‚’ä½¿ç”¨\n",
    "- ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”$1:1$ã€32ãƒ•ãƒ¬ãƒ¼ãƒ ã€256pxã®å‹•ç”»ã‚’ä½¿ç”¨\n",
    "- ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¯AdamWã‚’ä½¿ç”¨ï¼ˆ$\\beta=(0.9, 0.999)$, $\\epsilon=1\\times 10^{-15}$, é‡ã¿æ¸›è¡°ãªã—ï¼‰\n",
    "- ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å­¦ç¿’ç‡ã¯5e-5\n",
    "- ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒŸãƒãƒ¼ã‚¿ã®å­¦ç¿’ç‡ã¯1e-4\n",
    "- å‹¾é…ãƒãƒ«ãƒ ã‚¯ãƒªãƒƒãƒ—ã¯é–¾å€¤1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd76e520",
   "metadata": {},
   "source": [
    "## æ¡ä»¶ä»˜ã‘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b965deb5",
   "metadata": {},
   "source": [
    "æ¡ä»¶ä»˜ã‘ã¯ã€æ½œåœ¨å¤‰æ•°ã«å¯¾ã—ã¦æ¡ä»¶ãƒ™ã‚¯ãƒˆãƒ«ã¨ã‚¿ã‚¹ã‚¯ã®ç¨®é¡ã‚’ç¤ºã™ãƒãƒ£ãƒ³ãƒãƒ«ã‚’é€£çµã—ã¦è¡Œã†:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2c960a",
   "metadata": {},
   "source": [
    "![](image/fig10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8651a48c",
   "metadata": {},
   "source": [
    "- å·¦: ç”»åƒã‹ã‚‰å‹•ç”»ã‚’ç”Ÿæˆã™ã‚‹ã‚¿ã‚¹ã‚¯\n",
    "- ä¸­: å‹•ç”»ã‚’å»¶é•·ã™ã‚‹ã‚¿ã‚¹ã‚¯\n",
    "- å³: æœ€åˆã¨æœ€å¾Œã®ç”»åƒã®é–“ã‚’è£œå®Œã™ã‚‹ã‚¿ã‚¹ã‚¯\n",
    "- é€£çµå¾Œã®ãƒãƒ£ãƒ³ãƒãƒ«æ•°ã¯$2k + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4afc63",
   "metadata": {},
   "source": [
    "æ±åŒ–æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ã®ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚„ç”»åƒæ¡ä»¶ã®ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’å°å…¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c229f9d",
   "metadata": {},
   "source": [
    "æ¨è«–ã«ã¯ã€Classifier-free Guidanceï¼ˆCFGï¼‰ã‚’æ¡ç”¨:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532dd227",
   "metadata": {},
   "source": [
    "$$\n",
    "v_{t} = v_{\\theta}(x_{t},t,\\emptyset,\\emptyset) + g_{img}\\cdot(v_{\\theta}(x_{t},t,\\emptyset,img)-v_{\\theta}(x_{t},t,\\emptyset,\\emptyset)) + g_{txt}\\cdot(v_{\\theta}(x_{t},t,txt,img)-v_{\\theta}(x_{t},t,\\emptyset,img))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cd2e12",
   "metadata": {},
   "source": [
    "- $g_{\\text{img}}$: ç”»åƒæ¡ä»¶ã®å¼·ã•ã‚’èª¿æ•´ã™ã‚‹ä¿‚æ•°\n",
    "- $g_{\\text{txt}}$: ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ã®å¼·ã•ã‚’èª¿æ•´ã™ã‚‹ä¿‚æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a04814",
   "metadata": {},
   "source": [
    "ç”»åƒã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚’å¼·ãã™ã‚‹ã¨ã€ç”Ÿæˆã•ã‚ŒãŸå‹•ç”»ã«ãƒãƒ©ã¤ãï¼ˆflickerï¼‰ãŒç”Ÿã˜ã‚‹:\n",
    "\n",
    "- Guidance Oscillationã‚’å°å…¥ã—ã¦å®‰å®šåŒ–\n",
    "    - ä¾‹ãˆã°ã€50ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ã†ã¡ã€10ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«$g_{\\text{img}}$ã‚’$1$ã«è½ã¨ã™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b8f8b1",
   "metadata": {},
   "source": [
    "I2Vã§ã¯å‹•çš„ãªç”»åƒã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ï¼ˆdynamic image guidanceï¼‰ã‚’æ¡ç”¨ã—ã€å‹•ç”»ã®å¾ŒåŠã«ãªã‚‹ã«ã¤ã‚Œã¦æ¡ä»¶ä»˜ã‘ã‚’å¼·ãã™ã‚‹:\n",
    "\n",
    "![](image/fig11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ce8cd",
   "metadata": {},
   "source": [
    "- æ¨ªè»¸ã¯ãƒ•ãƒ¬ãƒ¼ãƒ æ•°\n",
    "- ç¸¦è»¸ã¯æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "- ç”»åƒã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«ã¯$1$ã‹ã‚‰$k$ã¾ã§ç·šå½¢ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "- æ¨è«–ãŒçµ‚ã‚ã‚Šã«ãªã‚‹ã«ã¤ã‚Œã¦ã€ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒ$1$ã«ãªã‚‹ã‚ˆã†ã«æ¸›è¡°\n",
    "- ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯$g_{\\text{img}}=3$, $g_{\\text{txt}}=7.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d241562",
   "metadata": {},
   "source": [
    "å‹•ç”»ã®å‹•ãã®å¼·ã•ã¯ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã«è¿½åŠ ã™ã‚‹ã“ã¨ã§åˆ¶å¾¡:\n",
    "\n",
    "![](image/fig12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0abfb4",
   "metadata": {},
   "source": [
    "## ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f02982a",
   "metadata": {},
   "source": [
    "è¨“ç·´ã¯ã€ColossalAIã‚’ä½¿ç”¨:\n",
    "\n",
    "- 141GBã®ãƒ¡ãƒ¢ãƒªå®¹é‡ã‚’æŒã¤H200 GPUã‚’ä½¿ç”¨ã—ã€ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ï¼ˆDP, Data Parallelismï¼‰ã‚’å°å…¥\n",
    "- é¸æŠçš„ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒ†ã‚£ãƒ³ã‚°ï¼ˆSelective Activation Checkpointingï¼‰ã‚’å°å…¥\n",
    "    - ä¸€éƒ¨ã®é †ä¼æ’­ã®è¨ˆç®—çµæœã‚’ç ´æ£„ã—ã€å¿…è¦ã«å¿œã˜ã¦å†è¨ˆç®—ã™ã‚‹æŠ€è¡“ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6636c0c3",
   "metadata": {},
   "source": [
    "é«˜è§£åƒåº¦ã®å‹•ç”»ã‚’åŠ¹ç‡çš„ã«å‡¦ç†ã™ã‚‹ãŸã‚ã«ã€è¤‡æ•°ã®ä¸¦åˆ—åŒ–æŠ€è¡“ã‚’æ¡ç”¨:\n",
    "\n",
    "- AEã§ã¯ã€ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ï¼ˆTP, Tensor Parallelismï¼‰ã‚’ç•³ã¿è¾¼ã¿å±¤ã«é©ç”¨\n",
    "    - å…¥åŠ›ã¾ãŸã¯å‡ºåŠ›ã®ãƒãƒ£ãƒãƒ«æ¬¡å…ƒã§é‡ã¿ã‚’åˆ†å‰²ã™ã‚‹\n",
    "- MMDiTã§ã¯ã€Zero Redundancy Opitimizerï¼ˆZeroDPï¼‰ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸¦åˆ—ï¼ˆCP, Context Parallelismï¼‰ã‚’é©ç”¨\n",
    "    - ZeroDPã¯ã€DeepSpeedã®ZeRO2ã«ã‚ˆã‚Šå‹¾é…ã‚’åˆ†å‰²ã—ã€é‡è¤‡ãªãGPUãŒä¿æŒã™ã‚‹æŠ€è¡“\n",
    "    - CPã¯ã€å‹•ç”»ã¨ãƒ†ã‚­ã‚¹ãƒˆã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åˆ†å‰²ã—ã€å„GPUãŒç‹¬ç«‹ã—ã¦ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³è¨ˆç®—ã™ã‚‹æŠ€è¡“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f902c3e9",
   "metadata": {},
   "source": [
    "ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒ†ã‚£ãƒ³ã‚°ã¯ã€ãƒ–ãƒ­ãƒƒã‚¯ã®å…¥åŠ›ã®ã¿ã‚’ä¿æŒã—ã€ãã‚Œä»¥å¤–ã¯å†è¨ˆç®—ã™ã‚‹:\n",
    "\n",
    "- ã‚¹ãƒ†ãƒ¼ã‚¸1, 2ã§ã¯ã€ãƒ‡ãƒ¥ã‚¢ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ã®8å±¤ã¨ã™ã¹ã¦ã®ã‚·ãƒ³ã‚°ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ã«é©ç”¨\n",
    "- ã‚¹ãƒ†ãƒ¼ã‚¸3ã§ã¯ã€ã™ã¹ã¦ã®ãƒ–ãƒ­ãƒƒã‚¯ã§æœ‰åŠ¹åŒ–ã—ã€CPUã¸ã®ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã‚‚ä½¿ç”¨\n",
    "    - ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã§ã¯ã€Pinned Memoryã¨éåŒæœŸãƒ‡ãƒ¼ã‚¿è»¢é€ã‚’ä½¿ç”¨ã—ã€ã•ã‚‰ã«åŠ¹ç‡åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6969e364",
   "metadata": {},
   "source": [
    "åˆ†æ•£è¨“ç·´ã®å ´åˆã€ç‰©ç†éšœå®³ãŒç™ºç”Ÿã—ã‚„ã™ã„ã®ã§è‡ªå‹•å¾©æ—§æ©Ÿèƒ½ï¼ˆAuto Recoveryï¼‰ã‚’å°å…¥\n",
    "\n",
    "- InfiniBandã®éšœå®³ã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã€NCCLã‚¨ãƒ©ãƒ¼\n",
    "- è¨“ç·´ã®çŠ¶æ…‹ã‚’ç›£è¦–ã—ã€é€Ÿåº¦ä½ä¸‹ãƒ»æå¤±ã®æ€¥æ¿€ãªæ‚ªåŒ–ãƒ»ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒãªã„ã‹ã‚’æ¤œè¨¼\n",
    "- å•é¡ŒãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã¯ã€ã™ã¹ã¦ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’åœæ­¢ã—ã€ãƒãƒ¼ãƒ‰ã‚’è¨ºæ–­\n",
    "- å¿…è¦ã«å¿œã˜ã¦äºˆå‚™ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã€æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰è‡ªå‹•çš„ã«è¨“ç·´ã‚’å†é–‹ã™ã‚‹\n",
    "- GPUç¨¼åƒç‡99%ã§ã€ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ã‚’æœ€å°é™ã«æŠ‘ãˆãŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58002756",
   "metadata": {},
   "source": [
    "CPUã¨GPUé–“ã®ãƒ‡ãƒ¼ã‚¿è»¢é€ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã«PyTorchã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’æœ€é©åŒ–:\n",
    "\n",
    "- PyTorchãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®Pinned Memoryã¯ã€CUDAã‚«ãƒ¼ãƒãƒ«å®Ÿè¡Œã‚’ãƒ–ãƒ­ãƒƒã‚¯ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹\n",
    "- ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ•ã‚¡ï¼ˆpre-allocated pinned memory bufferï¼‰ã‚’ä½¿ç”¨ã—ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å‰Šæ¸›\n",
    "- ãƒ‡ãƒ¼ã‚¿è»¢é€ã¨è¨ˆç®—ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã•ã›ã¦åŠ¹ç‡åŒ–\n",
    "- ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªGCï¼ˆã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ï¼‰ã¯ã€åˆ†æ•£è¨“ç·´ã§ã¯ãƒã‚°ãŒå¤šã„ãŸã‚æ‰‹å‹•ã§ãƒ¡ãƒ¢ãƒªã‹ã‚“ã‚Š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55af23f0",
   "metadata": {},
   "source": [
    "ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜ã¯ã€ãƒ”ãƒ³ç•™ã‚ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ•ã‚¡ã«å¯¾ã—ã¦ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã“ã¨ã§é«˜é€ŸåŒ–:\n",
    "\n",
    "- ãƒ‡ã‚£ã‚¹ã‚¯ã¸ã®æ›¸ãè¾¼ã¿ã¯ã€C++ã«ã‚ˆã‚‹éåŒæœŸãƒ‡ã‚£ã‚¹ã‚¯æ›¸ãè¾¼ã¿ã«ã‚ˆã‚Šè¨“ç·´ãƒ—ãƒ­ã‚»ã‚¹ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã›ãšã«åŠ¹ç‡åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629e8584",
   "metadata": {},
   "source": [
    "## å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaff96bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    !git clone https://github.com/hpcaitech/Open-Sora.git\n",
    "    WORK_DIR = \"/content/Open-Sora\"\n",
    "    %cd $WORK_DIR\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    import os\n",
    "\n",
    "    if not os.path.exists(\"/workspaces/open-sora-training/Open-Sora\"):\n",
    "        !git clone https://github.com/hpcaitech/Open-Sora.git\n",
    "        !cp /workspaces/open-sora-training/ckpt.py /workspaces/open-sora-training/Open-Sora/opensora/utils/ckpt.py\n",
    "\n",
    "    WORK_DIR = \"/workspaces/open-sora-training/Open-Sora\"\n",
    "    %cd $WORK_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298268ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "\n",
    "DEBUG_LOG_PATH = os.path.join(WORK_DIR, \"debug.log\")\n",
    "\n",
    "if os.path.exists(DEBUG_LOG_PATH):\n",
    "    os.remove(DEBUG_LOG_PATH)\n",
    "\n",
    "def custom_format(record):\n",
    "    match record.levelno:\n",
    "        case logging.DEBUG:\n",
    "            level = \"ğŸŸ¦\"\n",
    "        case logging.INFO:\n",
    "            level = \"ğŸŸ©\"\n",
    "        case logging.WARNING:\n",
    "            level = \"ğŸŸ¨\"\n",
    "        case logging.ERROR:\n",
    "            level = \"ğŸŸ¥\"\n",
    "        case logging.CRITICAL:\n",
    "            level = \"ğŸ›‘\"\n",
    "    return f\"{level} {record.getMessage()}\"\n",
    "\n",
    "log_ = logging.getLogger()\n",
    "\n",
    "for handler in log_.handlers:\n",
    "    log_.removeHandler(handler)\n",
    "\n",
    "formatter = logging.Formatter()\n",
    "formatter.format = custom_format\n",
    "\n",
    "file_handler = logging.FileHandler(DEBUG_LOG_PATH)\n",
    "file_handler.setFormatter(formatter)\n",
    "log_.addHandler(file_handler)\n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "log_.addHandler(stream_handler)\n",
    "log_.setLevel(logging.DEBUG)\n",
    "\n",
    "PYTHON_VERSION = platform.python_version()\n",
    "log_.info(f\"Python {PYTHON_VERSION}\")\n",
    "\n",
    "NVIDIA_SMI = subprocess.run(\"nvidia-smi\", capture_output=True, text=True).stdout\n",
    "log_.info(f\"{NVIDIA_SMI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bab8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    %pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "    %pip install \\\n",
    "        accelerate \\\n",
    "        av==13.1.0 \\\n",
    "        colossalai \\\n",
    "        ftfy \\\n",
    "        liger-kernel \\\n",
    "        omegaconf \\\n",
    "        mmengine \\\n",
    "        openai \\\n",
    "        pandas \\\n",
    "        pandarallel \\\n",
    "        pyarrow \\\n",
    "        tensorboard \\\n",
    "        wandb \\\n",
    "        --extra-index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "    %pip install flash-attn --no-build-isolation\n",
    "\n",
    "    %pip install -e . --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4318c538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from collections import OrderedDict, defaultdict\n",
    "from collections.abc import Iterable\n",
    "from colossalai.booster import Booster\n",
    "from colossalai.booster.plugin import HybridParallelPlugin, LowLevelZeroPlugin\n",
    "from colossalai.cluster import DistCoordinator\n",
    "from colossalai.utils import get_current_device\n",
    "from colossalai.utils import set_seed\n",
    "from colossalai.zero.low_level import LowLevelZeroOptimizer\n",
    "from contextlib import nullcontext\n",
    "from copy import deepcopy\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from einops import rearrange\n",
    "from fractions import Fraction\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "from mmengine.config import Config\n",
    "from pandarallel import pandarallel\n",
    "from PIL import Image\n",
    "from pprint import pformat\n",
    "from torch import nn\n",
    "from torch._utils import ExceptionWrapper\n",
    "from torch.distributed import ProcessGroup\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.profiler import ProfilerActivity, profile, schedule\n",
    "from torch.utils.checkpoint import (_DEFAULT_DETERMINISM_MODE, CheckpointFunction, _checkpoint_without_reentrant_generator, checkpoint_sequential, noop_context_fn)\n",
    "from torch.utils.data import DataLoader, _utils\n",
    "from torch.utils.data import Dataset, DistributedSampler\n",
    "from torch.utils.data._utils import MP_STATUS_CHECK_INTERVAL\n",
    "from torch.utils.data.dataloader import (IterDataPipe, MapDataPipe, _BaseDataLoaderIter, _MultiProcessingDataLoaderIter, _sharding_worker_init_fn, _SingleProcessDataLoaderIter)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import get_video_backend\n",
    "from torchvision import models\n",
    "from torchvision import models\n",
    "from torchvision.datasets.folder import IMG_EXTENSIONS, pil_loader\n",
    "from torchvision.io import write_video\n",
    "from torchvision.io.video import _check_av_available\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Callable, Optional, Union, Tuple\n",
    "from typing import Callable, ContextManager, Dict, List, Optional, Tuple\n",
    "from typing import Iterator\n",
    "import argparse\n",
    "import ast\n",
    "import av\n",
    "import collections\n",
    "import cv2\n",
    "import diffusers\n",
    "import functools\n",
    "import gc\n",
    "import hashlib\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numbers\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import queue\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import subprocess\n",
    "import threading\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as multiprocessing\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "gc.disable()\n",
    "\n",
    "# ä»¥å‰ã®ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—ã§å®Ÿè£…æ¸ˆã¿\n",
    "from opensora.models.dc_ae.ae_model_zoo import DCAE_HF\n",
    "from opensora.models.dc_ae.models.dc_ae import DCAE, DCAEConfig, dc_ae_f32\n",
    "from opensora.utils.optimizer import create_lr_scheduler, create_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8a3051",
   "metadata": {},
   "source": [
    "## å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a360a8b",
   "metadata": {},
   "source": [
    "å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã¯[hcpai-tech/open-sora-pexels-45k][1]ã«ã‚ã‚‹\n",
    "\n",
    "[1]: https://modelscope.cn/datasets/hpcai-tech/open-sora-pexels-45k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac0fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 269.53GBã®å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "# https://modelscope.cn/datasets/hpcai-tech/open-sora-pexels-45k\n",
    "\n",
    "if False:\n",
    "    !apt update && apt install git-lfs\n",
    "    !git-lfs install\n",
    "    !git clone \"https://www.modelscope.cn/datasets/hpcai-tech/open-sora-pexels-45k.git\"\n",
    "    !cd open-sora-pexels-45k && \\\n",
    "        cat tar/pexels_45k.tar.* > pexels_45k.tar && \\\n",
    "        mkdir ../datasets && \\\n",
    "        mv pexels_45k ../datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0a57f",
   "metadata": {},
   "source": [
    "é‡ã„ã®ã§æœ€å°é™ã®CSVãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa393d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://modelscope.cn/datasets/hpcai-tech/open-sora-pexels-45k/resolve/master/pexels_45k_necessary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216fe272",
   "metadata": {},
   "outputs": [],
   "source": [
    "pexels_45k_necessary_df = pd.read_csv(\"pexels_45k_necessary.csv\")\n",
    "print(len(pexels_45k_necessary_df)) # 45,817ä»¶\n",
    "pexels_45k_necessary_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9192662",
   "metadata": {},
   "outputs": [],
   "source": [
    "pexels_45k_score_df = pexels_45k_necessary_df.sort_values(by=[\"num_frames\"])\n",
    "pexels_5 = pexels_45k_score_df.head(5).copy()\n",
    "pexels_5['path'] = pexels_5['path'].str.replace('datasets/pexels_45k/', 'datasets/pexels_5/', regex=False)\n",
    "pexels_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8652538",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_DIR = os.path.join(WORK_DIR, \"datasets\")\n",
    "PEXELS_DATASET_DIR = os.path.join(DATASETS_DIR, \"pexels_5\")\n",
    "os.makedirs(PEXELS_DATASET_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e20a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PEXELS_CSV_PATH = os.path.join(WORK_DIR, \"pexels_5.csv\")\n",
    "\n",
    "if not os.path.exists(PEXELS_CSV_PATH):\n",
    "    for path in pexels_5[\"path\"]:\n",
    "        match = re.search(r\"/(\\d+)_\", path)\n",
    "        video_id = match.group(1)\n",
    "        video_url = f\"https://www.pexels.com/download/video/{video_id}\"\n",
    "        video_path = os.path.join(PEXELS_DATASET_DIR, f\"{video_id}.mp4\")\n",
    "        !wget -nc -O $video_path $video_url\n",
    "        pexels_5[\"path\"] = pexels_5[\"path\"].replace(path, video_path)\n",
    "\n",
    "    pexels_5.to_csv(os.path.join(WORK_DIR, \"pexels_5.csv\"), index=False)\n",
    "else:\n",
    "    pexels_5 = pd.read_csv(PEXELS_CSV_PATH)\n",
    "\n",
    "pexels_5.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd75697f",
   "metadata": {},
   "source": [
    "## DC-AEã®è¨“ç·´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ee5cf",
   "metadata": {},
   "source": [
    "[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ][1]ã‚ˆã‚Šä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§é–‹å§‹ã§ãã‚‹\n",
    "\n",
    "```sh\n",
    "torchrun --nproc_per_node 8 scripts/vae/train.py configs/vae/train/video_dc_ae.py\n",
    "```\n",
    "\n",
    "[1]: https://github.com/hpcaitech/Open-Sora/blob/main/docs/hcae.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f070e0e",
   "metadata": {},
   "source": [
    "### CLIãƒ‘ãƒ¼ã‚µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45c0e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args) -> tuple[str, argparse.Namespace]:\n",
    "    \"\"\"\n",
    "    ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã®å¼•æ•°ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹\n",
    "    parse_configsé–¢æ•°ã§ä½¿ç”¨ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        args (list[str]): ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®ãƒªã‚¹ãƒˆ\n",
    "    Returns:\n",
    "        tuple[str, argparse.Namespace]: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã¨ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°\n",
    "    \"\"\"\n",
    "    # argparseã‚’åˆæœŸåŒ–\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # configå¼•æ•°ã‚’è¿½åŠ \n",
    "    parser.add_argument(\"config\", type=str, help=\"model config file path\")\n",
    "\n",
    "    # æ—¢çŸ¥ã®å¼•æ•°ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã€æœªçŸ¥ã®å¼•æ•°ã‚’unknown_argsã¨ã—ã¦å–å¾—\n",
    "    args, unknown_args = parser.parse_known_args(args)\n",
    "\n",
    "    return args.config, unknown_args\n",
    "\n",
    "# æ¤œè¨¼\n",
    "parse_args([\"train.py\", \"-m\", \"dcae\", \"-i\", \"12345\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4e518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_config(config_path: str) -> Config:\n",
    "    \"\"\"\n",
    "    è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "    parse_configsé–¢æ•°ã§ä½¿ç”¨ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        config_path (str): è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
    "    Returns:\n",
    "        Config: è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "    \"\"\"\n",
    "\n",
    "    # mmengineã®Configã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã—ã¦è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "    # configs/vae/train/video_dc_ae.py\n",
    "    # https://mmengine.readthedocs.io/en/latest/advanced_tutorials/config.html#read-the-configuration-file\n",
    "    cfg = Config.fromfile(config_path)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "# æ¤œè¨¼\n",
    "read_config(\"configs/vae/train/video_dc_ae.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80074784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_convert(value: str) -> int | float | bool | list | dict | None:\n",
    "    \"\"\"\n",
    "    æ–‡å­—åˆ—ã‚’é©åˆ‡ãªPythonãƒ‡ãƒ¼ã‚¿å‹ã«è‡ªå‹•å¤‰æ›ã™ã‚‹\n",
    "    intã€floatã€boolã€listã€dictãªã©ã‚’å«ã‚€\n",
    "    merge_argsé–¢æ•°ã§ä½¿ç”¨ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        value (str): å¤‰æ›ã™ã‚‹æ–‡å­—åˆ—\n",
    "    Returns:\n",
    "        int, float, bool, list |  dict: å¤‰æ›å¾Œã®å€¤\n",
    "    \"\"\"\n",
    "\n",
    "    # ç©ºç™½æ–‡å­—ã¯ãã®ã¾ã¾\n",
    "    if value == \"\":\n",
    "        return value\n",
    "\n",
    "    # noneã¯Noneã«å¤‰æ›\n",
    "    if value.lower() == \"none\":\n",
    "        return None\n",
    "\n",
    "    # booleanå€¤ã‚’å‡¦ç†\n",
    "    lower_value = value.lower()\n",
    "    if lower_value == \"true\":\n",
    "        return True\n",
    "    elif lower_value == \"false\":\n",
    "        return False\n",
    "\n",
    "    # æ–‡å­—åˆ—ã‚’intã¾ãŸã¯floatã«å¤‰æ›ã‚’è©¦ã¿ã‚‹\n",
    "    try:\n",
    "        # Integerã«å¤‰æ›\n",
    "        return int(value)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        # Floatã«å¤‰æ›\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    # æ–‡å­—åˆ—ã‚’listã€dictã€tupleãªã©ã«å¤‰æ›ã‚’è©¦ã¿ã‚‹\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        pass\n",
    "\n",
    "    # ãã®ã¾ã¾æ–‡å­—åˆ—ã‚’è¿”ã™\n",
    "    return value\n",
    "\n",
    "# æ¤œè¨¼\n",
    "auto_convert(\"\"), auto_convert(\"true\"), auto_convert(\"123\"), auto_convert(\"1.23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_args(cfg: Config, args: argparse.Namespace) -> Config:\n",
    "    \"\"\"\n",
    "    è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’ãƒãƒ¼ã‚¸ã™ã‚‹é–¢æ•°\n",
    "\n",
    "    Args:\n",
    "        cfg (Config): è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "        args (argparse.Namespace): ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°\n",
    "    Returns:\n",
    "        Config: ãƒãƒ¼ã‚¸å¾Œã®è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "    \"\"\"\n",
    "\n",
    "    # å¼•æ•°ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆå¶æ•°ç•ªç›®ï¼‰ã¨å€¤ï¼ˆå¥‡æ•°ç•ªç›®ï¼‰ã‚’ãƒšã‚¢ã«ã—ã¦å‡¦ç†\n",
    "    for k, v in zip(args[::2], args[1::2]):\n",
    "\n",
    "        # 1) æ¤œè¨¼ã¨å‰å‡¦ç†\n",
    "\n",
    "        assert k.startswith(\"--\"), f\"Invalid argument: {k}\"\n",
    "\n",
    "        # --foo-bar -> foo_bar\n",
    "        k = k[2:].replace(\"-\", \"_\")\n",
    "\n",
    "        # foo.bar -> [\"foo\", \"bar\"]\n",
    "        k_split = k.split(\".\")\n",
    "\n",
    "        # 2) ç½®ãæ›ãˆã‚‹ã‚­ãƒ¼ã‚’å«ã‚€éšå±¤ã®å‚ç…§ã‚’å–å¾—\n",
    "\n",
    "        target = cfg\n",
    "        for key in k_split[:-1]:\n",
    "            assert key in cfg, f\"Key {key} not found in config\"\n",
    "            target = target[key]\n",
    "\n",
    "        # 3) ç½®ãæ›ãˆã‚‹å€¤ã®å‹ã‚’æ¨æ¸¬ã—ã¦å¤‰æ›\n",
    "            \n",
    "        if v.lower() == \"none\":\n",
    "            v = None\n",
    "        elif k in target:\n",
    "            v_type = type(target[k])\n",
    "            if v_type == bool:\n",
    "                v = auto_convert(v)\n",
    "            else:\n",
    "                v = type(target[k])(v)\n",
    "        else:\n",
    "            v = auto_convert(v)\n",
    "\n",
    "        # 4) å€¤ã‚’ç½®ãæ›ãˆ\n",
    "        target[k_split[-1]] = v\n",
    "\n",
    "    return cfg\n",
    "\n",
    "# æ¤œè¨¼\n",
    "merge_args(read_config(\"configs/vae/train/video_dc_ae.py\"), ['--model.type', 'dc_ae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f075d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_configs(args) -> Config:\n",
    "    \"\"\"\n",
    "    è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’ãƒ‘ãƒ¼ã‚¹ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        args (list[str]): ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®ãƒªã‚¹ãƒˆ\n",
    "    Returns:\n",
    "        Config: è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "    \"\"\"\n",
    "\n",
    "    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã®å¼•æ•°ã‚’ãƒ‘ãƒ¼ã‚¹\n",
    "    config, args = parse_args(args)\n",
    "\n",
    "    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "    cfg = read_config(config)\n",
    "\n",
    "    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’ãƒãƒ¼ã‚¸\n",
    "    cfg = merge_args(cfg, args)\n",
    "\n",
    "    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’ä¿å­˜\n",
    "    cfg.config_path = config\n",
    "\n",
    "    # ç’°å¢ƒå¤‰æ•°AE_SPATIAL_COMPRESSIONã‚’è¨­å®š\n",
    "    if cfg.get(\"ae_spatial_compression\", None) is not None:\n",
    "        os.environ[\"AE_SPATIAL_COMPRESSION\"] = str(cfg.ae_spatial_compression)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "cfg = parse_configs([\"configs/vae/train/video_dc_ae.py\"])\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc13ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—ç”¨ã«å€¤ã‚’ä¸Šæ›¸ã\n",
    "\n",
    "cfg.bucket_config = {\n",
    "    \"256px_ar1:1\": {\n",
    "        4: (1.0, 1), # 4ãƒ•ãƒ¬ãƒ¼ãƒ ã®å‹•ç”»ã‚’1ãƒãƒƒãƒ\n",
    "    }\n",
    "}\n",
    "\n",
    "cfg.epochs = 1 # 100ã‹ã‚‰1ã«å¤‰æ›´\n",
    "\n",
    "cfg.num_workers = 0\n",
    "cfg.prefetch_factor = None\n",
    "cfg.pin_memory_cache_pre_alloc_numels = None \n",
    "\n",
    "# cfg.accumulation_steps = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ea3d5c",
   "metadata": {},
   "source": [
    "### ãƒ‡ãƒ¼ã‚¿å‹è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab26330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_torch_dtype(dtype: str | torch.dtype) -> torch.dtype:\n",
    "    \"\"\"\n",
    "    æ–‡å­—åˆ—ã‚’torch.dtypeã«å¤‰æ›ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        dtype (str | torch.dtype): å¤‰æ›ã™ã‚‹æ–‡å­—åˆ—ã¾ãŸã¯torch.dtype\n",
    "    Returns:\n",
    "        torch.dtype: å¤‰æ›å¾Œã®torch.dtype\n",
    "    \"\"\"\n",
    "\n",
    "    # ã™ã§ã«torch.dtypeã®å ´åˆ\n",
    "    # False\n",
    "    if isinstance(dtype, torch.dtype):\n",
    "        return dtype\n",
    "\n",
    "    # æ–‡å­—åˆ—ã®å ´åˆ\n",
    "    # True\n",
    "    elif isinstance(dtype, str):\n",
    "\n",
    "        # æ–‡å­—åˆ—ã‚’torch.dtypeã«ãƒãƒƒãƒ”ãƒ³ã‚°\n",
    "        dtype_mapping = {\n",
    "            \"float64\": torch.float64,\n",
    "            \"float32\": torch.float32,\n",
    "            \"float16\": torch.float16,\n",
    "            \"fp32\": torch.float32,\n",
    "            \"fp16\": torch.float16,\n",
    "            \"half\": torch.float16,\n",
    "            \"bf16\": torch.bfloat16,\n",
    "        }\n",
    "        if dtype not in dtype_mapping:\n",
    "            raise ValueError(f\"Unsupported dtype {dtype}\")\n",
    "\n",
    "        # å¯¾å¿œã™ã‚‹torch.dtypeã‚’è¿”ã™\n",
    "        dtype = dtype_mapping[dtype]\n",
    "        return dtype\n",
    "    \n",
    "    # False\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dtype {dtype}\")\n",
    "\n",
    "dtype = to_torch_dtype(cfg.get(\"dtype\", \"bf16\"))\n",
    "dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d7602",
   "metadata": {},
   "source": [
    "### ãƒ‡ãƒã‚¤ã‚¹è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d7b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['RANK'] = '0'\n",
    "os.environ['WORLD_SIZE'] = '1'\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12345'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e9ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_device() -> tuple[torch.device, DistCoordinator]:\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒã‚¤ã‚¹ã¨åˆ†æ•£ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.device, DistCoordinator]: ãƒ‡ãƒã‚¤ã‚¹ã¨åˆ†æ•£ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼\n",
    "    \"\"\"\n",
    "\n",
    "    assert torch.cuda.is_available(), \"Training currently requires at least one GPU.\"\n",
    "\n",
    "    # NCCLãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§åˆ†æ•£ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’åˆæœŸåŒ–\n",
    "    # NCCL = NVIDIA Collective Communications Library\n",
    "    # NOTE: ãƒ—ãƒ­ã‚»ã‚¹ã®æ—©æœŸçµ‚äº†ã‚’é˜²ããŸã‚ã«éå¸¸ã«å¤§ããªã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’è¨­å®š\n",
    "    dist.init_process_group(backend=\"nccl\", timeout=timedelta(hours=24))\n",
    "\n",
    "    # ã€Œç¾åœ¨ã®ãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ©ãƒ³ã‚¯ID % åˆ©ç”¨å¯èƒ½ãªGPUæ•°ã€ã‚’è¨ˆç®—ã—ã¦ãƒ‡ãƒã‚¤ã‚¹ã‚’è¨­å®š\n",
    "    # ä¾‹ãˆã°ã€ãƒ©ãƒ³ã‚¯IDãŒ3ã§GPUãŒ2å°ã®å ´åˆã€ãƒ‡ãƒã‚¤ã‚¹ã¯1ã«ãªã‚‹\n",
    "    torch.cuda.set_device(dist.get_rank() % torch.cuda.device_count())\n",
    "\n",
    "    # Colossal-AIã®ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–\n",
    "    # https://colossalai.org/docs/features/cluster_utils/#api-reference\n",
    "    coordinator = DistCoordinator()\n",
    "\n",
    "    # ç¾åœ¨ã®ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—\n",
    "    device = get_current_device()\n",
    "\n",
    "    return device, coordinator\n",
    "\n",
    "try:\n",
    "    device, coordinator = setup_device()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass\n",
    "\n",
    "device, coordinator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df22215b",
   "metadata": {},
   "source": [
    "### ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b694d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointIO:\n",
    "    \"\"\"\n",
    "    ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®èª­ã¿è¾¼ã¿ã¨æ›¸ãè¾¼ã¿ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_write_entries: int = 32):\n",
    "        # æ›¸ãè¾¼ã¿ã‚¨ãƒ³ãƒˆãƒªæ•°ã®ä¸Šé™\n",
    "        self.n_write_entries = n_write_entries\n",
    "\n",
    "        # æ›¸ãè¾¼ã¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "        self.writer: Optional[Any] = None\n",
    "\n",
    "        # ãƒ”ãƒ³ç•™ã‚ã•ã‚ŒãŸçŠ¶æ…‹è¾æ›¸\n",
    "        self.pinned_state_dict: Optional[Dict[str, torch.Tensor]] = None\n",
    "\n",
    "        # ãƒã‚¹ã‚¿ãƒ¼ã®ãƒ”ãƒ³ç•™ã‚ã•ã‚ŒãŸçŠ¶æ…‹è¾æ›¸\n",
    "        # ãƒã‚¹ã‚¿ãƒ¼ã¯ã€åˆ†æ•£å­¦ç¿’ã®ä¸»ãƒãƒ¼ãƒ‰\n",
    "        self.master_pinned_state_dict: Optional[Dict[str, torch.Tensor]] = None\n",
    "\n",
    "        # ãƒã‚¹ã‚¿ãƒ¼ã®æ›¸ãè¾¼ã¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "        self.master_writer: Optional[Any] = None\n",
    "\n",
    "    def _sync_io(self):\n",
    "        # æ›¸ãè¾¼ã¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒå­˜åœ¨ã™ã‚‹å ´åˆã€åŒæœŸã—ã¦è§£æ”¾\n",
    "        if self.writer is not None:\n",
    "            self.writer.synchronize()\n",
    "            self.writer = None\n",
    "\n",
    "        # ãƒã‚¹ã‚¿ãƒ¼ã®æ›¸ãè¾¼ã¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒå­˜åœ¨ã™ã‚‹å ´åˆã€åŒæœŸã—ã¦è§£æ”¾\n",
    "        if self.master_writer is not None:\n",
    "            self.master_writer.synchronize()\n",
    "            self.master_writer = None\n",
    "\n",
    "    def __del__(self):\n",
    "        self._sync_io()\n",
    "\n",
    "    def _prepare_pinned_state_dict(self, ema: nn.Module, ema_shape_dict: dict):\n",
    "        if self.pinned_state_dict is None and dist.get_rank() == 0:\n",
    "            self.pinned_state_dict = _prepare_ema_pinned_state_dict(ema, ema_shape_dict)\n",
    "\n",
    "    def _prepare_master_pinned_state_dict(self, model: nn.Module, optimizer: LowLevelZeroOptimizer):\n",
    "        if self.master_pinned_state_dict is None and dist.get_rank() == 0:\n",
    "            sd = {}\n",
    "            w2m = optimizer.get_working_to_master_map()\n",
    "            for n, p in model.named_parameters():\n",
    "                master_p = w2m[id(p)]\n",
    "                sd[n] = torch.empty(p.shape, dtype=master_p.dtype, pin_memory=True, device=\"cpu\")\n",
    "            self.master_pinned_state_dict = sd\n",
    "\n",
    "    def save(\n",
    "        self,\n",
    "        booster: Booster,\n",
    "        save_dir: str,\n",
    "        model: nn.Module = None,\n",
    "        ema: nn.Module = None,\n",
    "        optimizer: Optimizer = None,\n",
    "        lr_scheduler: _LRScheduler = None,\n",
    "        sampler=None,\n",
    "        epoch: int = None,\n",
    "        step: int = None,\n",
    "        global_step: int = None,\n",
    "        batch_size: int = None,\n",
    "        lora: bool = False,\n",
    "        actual_update_step: int = None,\n",
    "        ema_shape_dict: dict = None,\n",
    "        async_io: bool = True,\n",
    "        include_master_weights: bool = False,\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            booster (Booster): Boosterã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "            save_dir (str): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "            model (nn.Module): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹ãƒ¢ãƒ‡ãƒ«\n",
    "            ema (nn.Module): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹EMAãƒ¢ãƒ‡ãƒ«\n",
    "            optimizer (Optimizer): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶\n",
    "            lr_scheduler (_LRScheduler): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©\n",
    "            sampler: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ©ãƒ¼\n",
    "            epoch (int): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚¨ãƒãƒƒã‚¯\n",
    "            step (int): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "            global_step (int): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ãƒ†ãƒƒãƒ—\n",
    "            batch_size (int): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "            lora (bool): ãƒ¢ãƒ‡ãƒ«ãŒLoRAã§è¨“ç·´ã•ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹\n",
    "            actual_update_step (int): å®Ÿéš›ã®æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—\n",
    "            ema_shape_dict (dict): EMAãƒ¢ãƒ‡ãƒ«ã®å½¢çŠ¶è¾æ›¸\n",
    "            async_io (bool): éåŒæœŸI/Oã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            include_master_weights (bool): ãƒã‚¹ã‚¿ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆã‚’å«ã‚ã‚‹ã‹ã©ã†ã‹\n",
    "\n",
    "        Returns:\n",
    "            str: ä¿å­˜ã•ã‚ŒãŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ãƒ‘ã‚¹\n",
    "        \"\"\"\n",
    "\n",
    "        # æ›¸ãè¾¼ã¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒå­˜åœ¨ã™ã‚‹å ´åˆã€åŒæœŸã—è§£æ”¾\n",
    "        self._sync_io()\n",
    "\n",
    "        # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã‚’ä½œæˆ\n",
    "        save_dir = os.path.join(save_dir, f\"epoch{epoch}-global_step{actual_update_step}\")\n",
    "\n",
    "        # éåŒæœŸI/Oã®ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ãƒ‘ã‚¹ã‚’è¨­å®š\n",
    "        os.environ[\"TENSORNVME_DEBUG_LOG\"] = os.path.join(save_dir, \"async_file_io.log\")\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "        if model is not None:\n",
    "\n",
    "            # LoRAã§ãªã„å ´åˆ\n",
    "            if not lora:\n",
    "                # ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "                os.makedirs(os.path.join(save_dir, \"model\"), exist_ok=True)\n",
    "\n",
    "                # Colossal-AIã®Boosterã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "                # https://colossalai.org/docs/basics/booster_api/\n",
    "                # NOTE: ãƒ—ãƒ­ã‚»ã‚¹ã”ã¨ã«ç‹¬ç«‹ã—ã¦å‡¦ç†\n",
    "                booster.save_model(\n",
    "                    model,\n",
    "                    os.path.join(save_dir, \"model\"),\n",
    "                    shard=True, # ã‚·ãƒ£ãƒ¼ãƒ‰ã«åˆ†å‰²ã—ã¦ä¿å­˜\n",
    "                    use_safetensors=True, # safetensorså½¢å¼ã§ä¿å­˜\n",
    "                    size_per_shard=4096, # ã‚·ãƒ£ãƒ¼ãƒ‰ã‚ãŸã‚Šã®ã‚µã‚¤ã‚ºã‚’4096MBã«è¨­å®š\n",
    "                    use_async=async_io, # éåŒæœŸI/Oã‚’ä½¿ç”¨\n",
    "                )\n",
    "\n",
    "            # LoRAã®å ´åˆ\n",
    "            else:\n",
    "                # LoRAãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "                os.makedirs(os.path.join(save_dir, \"lora\"), exist_ok=True)\n",
    "\n",
    "                # LoRAãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "                booster.save_lora_as_pretrained(model, os.path.join(save_dir, \"lora\"))\n",
    "\n",
    "        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "        if optimizer is not None:\n",
    "\n",
    "            # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’ä¿å­˜\n",
    "            booster.save_optimizer(\n",
    "                optimizer,\n",
    "                os.path.join(save_dir, \"optimizer\"),\n",
    "                shard=True,\n",
    "                size_per_shard=4096,\n",
    "                use_async=async_io\n",
    "            )\n",
    "\n",
    "            # ãƒã‚¹ã‚¿ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆã‚’å«ã‚ã‚‹å ´åˆ\n",
    "            # ãƒã‚¹ã‚¿ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆã¯ã€é«˜ç²¾åº¦ã§åˆ†æ•£ã—ã¦ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã‚¦ã‚§ã‚¤ãƒˆ\n",
    "            if include_master_weights:\n",
    "\n",
    "                # ãƒã‚¹ã‚¿ãƒ¼ã®ãƒ”ãƒ³ç•™ã‚ã•ã‚ŒãŸçŠ¶æ…‹è¾æ›¸ã‚’æº–å‚™\n",
    "                self._prepare_master_pinned_state_dict(model, optimizer)\n",
    "\n",
    "                # ãƒã‚¹ã‚¿ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆã‚’åé›†\n",
    "                master_weights_gathering(model, optimizer, self.master_pinned_state_dict)\n",
    "\n",
    "        # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "        if lr_scheduler is not None:\n",
    "\n",
    "            # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’ä¿å­˜\n",
    "            booster.save_lr_scheduler(lr_scheduler, os.path.join(save_dir, \"lr_scheduler\"))\n",
    "\n",
    "        # EMAãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "        # EMA = Exponential Moving Averageï¼ˆç§»å‹•å¹³å‡ï¼‰\n",
    "        if ema is not None:\n",
    "\n",
    "            # EMAãƒ¢ãƒ‡ãƒ«ã®ãƒ”ãƒ³ç•™ã‚ã•ã‚ŒãŸçŠ¶æ…‹è¾æ›¸ã‚’æº–å‚™\n",
    "            self._prepare_pinned_state_dict(ema, ema_shape_dict)\n",
    "\n",
    "            # EMAãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹ã‚’åé›†\n",
    "            model_gathering(ema, ema_shape_dict, self.pinned_state_dict)\n",
    "\n",
    "        # ãƒ©ãƒ³ã‚¯0ã®ãƒ—ãƒ­ã‚»ã‚¹ã®ã¿ãŒå®Ÿè¡Œ\n",
    "        if dist.get_rank() == 0:\n",
    "\n",
    "            # è¨“ç·´ã®é€²è¡ŒçŠ¶æ³ã®è¾æ›¸ã‚’ä½œæˆ\n",
    "            running_states = {\n",
    "                \"epoch\": epoch,\n",
    "                \"step\": step,\n",
    "                \"global_step\": global_step,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"actual_update_step\": actual_update_step,\n",
    "            }\n",
    "\n",
    "            # é€²è¡ŒçŠ¶æ³ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜\n",
    "            save_json(running_states, os.path.join(save_dir, \"running_states.json\"))\n",
    "\n",
    "            # EMAãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "            if ema is not None:\n",
    "\n",
    "                # éåŒæœŸI/Oã®å ´åˆ\n",
    "                if async_io:\n",
    "                    # safetensorså½¢å¼ã§EMAãƒ¢ãƒ‡ãƒ«ã‚’éåŒæœŸä¿å­˜\n",
    "                    self.writer = async_save(os.path.join(save_dir, \"ema.safetensors\"), self.pinned_state_dict)\n",
    "\n",
    "                # é€šå¸¸I/Oã®å ´åˆ\n",
    "                else:\n",
    "                    # EMAãƒ¢ãƒ‡ãƒ«ã‚’é€šå¸¸ã®PyTorchå½¢å¼ã§ä¿å­˜\n",
    "                    torch.save(ema.state_dict(), os.path.join(save_dir, \"ema.pt\"))\n",
    "\n",
    "            # ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "            if sampler is not None:\n",
    "                # ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã®çŠ¶æ…‹ã‚’ä¿å­˜\n",
    "                # ï¼ˆç¾åœ¨ã¯VariableVideoBatchSamplerã®ã¿å¯¾å¿œï¼‰\n",
    "                torch.save(sampler.state_dict(step), os.path.join(save_dir, \"sampler\"))\n",
    "\n",
    "            # ãƒã‚¹ã‚¿ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "            if optimizer is not None and include_master_weights:\n",
    "\n",
    "                # ãƒã‚¹ã‚¿ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆã‚’éåŒæœŸä¿å­˜\n",
    "                self.master_writer = async_save(\n",
    "                    os.path.join(save_dir, \"master.safetensors\"), self.master_pinned_state_dict\n",
    "                )\n",
    "\n",
    "        # ã™ã¹ã¦ã®ãƒ—ãƒ­ã‚»ã‚¹ãŒå®Œäº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ\n",
    "        dist.barrier()\n",
    "\n",
    "        return save_dir\n",
    "\n",
    "    def load(\n",
    "        self,\n",
    "        booster: Booster,\n",
    "        load_dir: str,\n",
    "        model: nn.Module = None,\n",
    "        ema: nn.Module = None,\n",
    "        optimizer: Optimizer = None,\n",
    "        lr_scheduler: _LRScheduler = None,\n",
    "        sampler=None,\n",
    "        strict: bool = False,\n",
    "        include_master_weights: bool = False,\n",
    "    ) -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€\n",
    "\n",
    "        Args:\n",
    "            booster (Booster): Boosterã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "            load_dir (str): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "            model (nn.Module): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€ãƒ¢ãƒ‡ãƒ«\n",
    "            ema (nn.Module): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€EMAãƒ¢ãƒ‡ãƒ«\n",
    "            optimizer (Optimizer): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶\n",
    "            lr_scheduler (_LRScheduler): ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©\n",
    "            sampler: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚€ã‚µãƒ³ãƒ—ãƒ©ãƒ¼\n",
    "            strict (bool): å³å¯†ã«èª­ã¿è¾¼ã‚€ã‹ã©ã†ã‹\n",
    "            include_master_weights (bool): ãƒã‚¹ã‚¿ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆã‚’å«ã‚ã‚‹ã‹ã©ã†ã‹\n",
    "        Returns:\n",
    "            tuple[int, int]: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚¨ãƒãƒƒã‚¯ã¨ã‚¹ãƒ†ãƒƒãƒ—\n",
    "        \"\"\"\n",
    "\n",
    "        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ã‚’ç¢ºèª\n",
    "        assert os.path.exists(load_dir), f\"Checkpoint directory {load_dir} does not exist\"\n",
    "        assert os.path.exists(os.path.join(load_dir, \"running_states.json\")), \"running_states.json does not exist\"\n",
    "\n",
    "        # è¨“ç·´ã®é€²è¡ŒçŠ¶æ³ã‚’èª­ã¿è¾¼ã‚€\n",
    "        running_states = load_json(os.path.join(load_dir, \"running_states.json\"))\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "        if model is not None:\n",
    "\n",
    "            # ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "            booster.load_model(\n",
    "                model,\n",
    "                _search_valid_path(os.path.join(load_dir, \"model\")),\n",
    "                strict=strict, # ã‚­ãƒ¼ãŒå³å¯†ã«ä¸€è‡´ã™ã‚‹ã‹ã©ã†ã‹\n",
    "                low_cpu_mem_mode=False, # CPUãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„ã™ã‚‹ãƒ¢ãƒ¼ãƒ‰\n",
    "                num_threads=32, # èª­ã¿è¾¼ã¿ã«ä½¿ç”¨ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰æ•°\n",
    "            )\n",
    "\n",
    "        # EMAãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "        if ema is not None:\n",
    "            # safetensorså½¢å¼ã®å ´åˆ\n",
    "            if os.path.exists(os.path.join(load_dir, \"ema.safetensors\")):\n",
    "                ema_state_dict = load_file(os.path.join(load_dir, \"ema.safetensors\"))\n",
    "\n",
    "            # é€šå¸¸ã®PyTorchå½¢å¼ã®å ´åˆ\n",
    "            else:\n",
    "                ema_state_dict = torch.load(os.path.join(load_dir, \"ema.pt\"), map_location=torch.device(\"cpu\"))\n",
    "\n",
    "            # EMAã¯Boosterã§å¼·åŒ–ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€booster.load_modelã¯ä½¿ç”¨ã—ãªã„\n",
    "            ema.load_state_dict(ema_state_dict, strict=strict, assign=True)\n",
    "\n",
    "        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "        if optimizer is not None:\n",
    "\n",
    "            # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’èª­ã¿è¾¼ã‚€\n",
    "            booster.load_optimizer(\n",
    "                optimizer, os.path.join(load_dir, \"optimizer\"), low_cpu_mem_mode=False, num_threads=32\n",
    "            )\n",
    "\n",
    "            # ãƒã‚¹ã‚¿ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆã‚’å«ã‚ã‚‹å ´åˆ\n",
    "            if include_master_weights:\n",
    "                master_state_dict = load_file(os.path.join(load_dir, \"master.safetensors\"))\n",
    "                # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã«ãƒã‚¹ã‚¿ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆã‚’èª­ã¿è¾¼ã‚€\n",
    "                load_master_weights(model, optimizer, master_state_dict)\n",
    "\n",
    "        # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "        if lr_scheduler is not None:\n",
    "            # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’èª­ã¿è¾¼ã‚€\n",
    "            booster.load_lr_scheduler(lr_scheduler, os.path.join(load_dir, \"lr_scheduler\"))\n",
    "\n",
    "        # ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "        if sampler is not None:\n",
    "            # ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã®çŠ¶æ…‹ã‚’èª­ã¿è¾¼ã‚€\n",
    "            sampler.load_state_dict(torch.load(os.path.join(load_dir, \"sampler\")))\n",
    "\n",
    "        # ã™ã¹ã¦ã®ãƒ—ãƒ­ã‚»ã‚¹ãŒå®Œäº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ\n",
    "        dist.barrier()\n",
    "\n",
    "        return (running_states[\"epoch\"], running_states[\"step\"])\n",
    "\n",
    "checkpoint_io = CheckpointIO()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb418ffc",
   "metadata": {},
   "source": [
    "### ã‚·ãƒ¼ãƒ‰å›ºå®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb12a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(cfg.get(\"seed\", 1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715793e",
   "metadata": {},
   "source": [
    "### ãƒ”ãƒ³ãƒ¡ãƒ¢ãƒªãƒ¼è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b60426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tensor = torch.randn(3, 3)\n",
    "id(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c15fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PinMemoryCache:\n",
    "    \"\"\"\n",
    "    ãƒ”ãƒ³ãƒ¡ãƒ¢ãƒªã‚’å†åˆ©ç”¨ã™ã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒ©ã‚¹\n",
    "    ãƒ”ãƒ³ãƒ¡ãƒ¢ãƒªã¯ã€CPUãƒ¡ãƒ¢ãƒªä¸Šã§ãƒšãƒ¼ã‚¸ãƒ³ã‚°ã•ã‚Œãªã„ãƒ¡ãƒ¢ãƒªé ˜åŸŸï¼ˆãƒ”ãƒ³ç•™ã‚ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªï¼‰\n",
    "    ãƒ”ãƒ³ãƒ¡ãƒ¢ãƒªã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€CPUã«ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å•ã„åˆã‚ã›ãšã«GPUã«ãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€ã§ãã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ãƒ†ãƒ³ã‚½ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’çŸ¯æ­£ã™ã‚‹\n",
    "    force_dtype: Optional[torch.dtype] = None\n",
    "\n",
    "    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ãƒ†ãƒ³ã‚½ãƒ«ã®æœ€å°è¦ç´ æ•°\n",
    "    min_cache_numel: int = 0\n",
    "\n",
    "    # äº‹å‰ã«ç¢ºä¿ã™ã‚‹ãƒ†ãƒ³ã‚½ãƒ«ã®è¦ç´ æ•°ãƒªã‚¹ãƒˆ\n",
    "    pre_alloc_numels: List[int] = []\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        # IDã‚’ã‚­ãƒ¼ã«ã—ãŸãƒ†ãƒ³ã‚½ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥\n",
    "        self.cache: Dict[int, torch.Tensor] = {}\n",
    "\n",
    "        # \n",
    "        self.output_to_cache: Dict[int, int] = {}\n",
    "\n",
    "        # \n",
    "        self.cache_to_output: Dict[int, int] = {}\n",
    "\n",
    "        # ã‚¹ãƒ¬ãƒƒãƒ‰å®‰å…¨ã®ãŸã‚ã®ãƒ­ãƒƒã‚¯\n",
    "        self.lock = threading.Lock()\n",
    "        self.total_cnt = 0\n",
    "        self.hit_cnt = 0\n",
    "\n",
    "        if len(self.pre_alloc_numels) > 0 and self.force_dtype is not None:\n",
    "            for n in self.pre_alloc_numels:\n",
    "                cache_tensor = torch.empty(n, dtype=self.force_dtype, device=\"cpu\", pin_memory=True)\n",
    "                with self.lock:\n",
    "                    self.cache[id(cache_tensor)] = cache_tensor\n",
    "\n",
    "    def get(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        CPUãƒ†ãƒ³ã‚½ãƒ«ã‚’å—ã‘å–ã‚Šã€å¯¾å¿œã™ã‚‹ãƒ”ãƒ³ç•™ã‚ã•ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«ã‚’è¿”ã™\n",
    "        ãƒ¡ãƒ¢ãƒªã®å‰²ã‚Šå½“ã¦ã®ã¿ã‚’ç®¡ç†ã—ã€å†…å®¹ã¯ã‚³ãƒ”ãƒ¼ã—ãªã„\n",
    "\n",
    "        Args:\n",
    "            tensor (torch.Tensor): ãƒ”ãƒ³ç•™ã‚ã™ã‚‹ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        Returns:\n",
    "            torch.Tensor: ãƒ”ãƒ³ç•™ã‚ã•ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«\n",
    "        \"\"\"\n",
    "\n",
    "        # å‘¼ã³å‡ºã—å›æ•°ã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ\n",
    "        self.total_cnt += 1\n",
    "\n",
    "        # ãƒ­ãƒƒã‚¯ã‚’å–å¾—\n",
    "        with self.lock:\n",
    "\n",
    "            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®IDã¨å€¤ã§ãƒ«ãƒ¼ãƒ—\n",
    "            for cache_id, cache_tensor in self.cache.items():\n",
    "\n",
    "                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœªä½¿ç”¨ã§ã€ã‚µã‚¤ã‚ºãŒååˆ†ã‚ã‚‹å ´åˆ\n",
    "                if cache_id not in self.cache_to_output and cache_tensor.numel() >= tensor.numel():\n",
    "\n",
    "                    # å¯¾å¿œã™ã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆ\n",
    "                    target_cache_tensor = cache_tensor[: tensor.numel()].view(tensor.shape)\n",
    "\n",
    "                    # å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆIDã‚’å–å¾—\n",
    "                    # NOTE: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç”Ÿå­˜ä¸­ã®ã¿IDã¯ä¸€æ„\n",
    "                    out_id = id(target_cache_tensor)\n",
    "\n",
    "                    # å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ãƒ³ã‚½ãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä¿å­˜\n",
    "                    self.output_to_cache[out_id] = cache_id\n",
    "\n",
    "                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ãƒ³ã‚½ãƒ«ã¨å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä¿å­˜\n",
    "                    self.cache_to_output[cache_id] = out_id\n",
    "\n",
    "                    # ãƒ’ãƒƒãƒˆå›æ•°ã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ\n",
    "                    self.hit_cnt += 1\n",
    "\n",
    "                    # å¯¾å¿œã™ã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ãƒ³ã‚½ãƒ«ã‚’è¿”ã™\n",
    "                    return target_cache_tensor\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿å‹ã‚’å–å¾—\n",
    "        dtype = self.force_dtype if self.force_dtype is not None else tensor.dtype\n",
    "\n",
    "        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ãƒ†ãƒ³ã‚½ãƒ«ã®è¦ç´ æ•°ã‚’æ±ºå®š\n",
    "        cache_numel = max(tensor.numel(), self.min_cache_numel)\n",
    "\n",
    "        # æ–°ã—ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆ\n",
    "        # NOTE: pin_memory=Trueã§ãƒ”ãƒ³ãƒ¡ãƒ¢ãƒªã«é…ç½®\n",
    "        cache_tensor = torch.empty(cache_numel, dtype=dtype, device=\"cpu\", pin_memory=True)\n",
    "\n",
    "        # å…¥åŠ›ã®ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶ã«åˆã‚ã›ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ãƒ³ã‚½ãƒ«ã‚’æ•´å½¢\n",
    "        target_cache_tensor = cache_tensor[: tensor.numel()].view(tensor.shape)\n",
    "\n",
    "        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ãƒ³ã‚½ãƒ«ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆIDã‚’å–å¾—\n",
    "        out_id = id(target_cache_tensor)\n",
    "\n",
    "        # ãƒ­ãƒƒã‚¯ã‚’å–å¾—\n",
    "        with self.lock:\n",
    "\n",
    "            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«æ–°ã—ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ãƒ³ã‚½ãƒ«ã‚’è¿½åŠ \n",
    "            self.cache[id(cache_tensor)] = cache_tensor\n",
    "\n",
    "            # å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ãƒ³ã‚½ãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä¿å­˜\n",
    "            self.output_to_cache[out_id] = id(cache_tensor)\n",
    "\n",
    "            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ãƒ³ã‚½ãƒ«ã¨å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä¿å­˜\n",
    "            self.cache_to_output[id(cache_tensor)] = out_id\n",
    "\n",
    "        # å¯¾å¿œã™ã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ãƒ³ã‚½ãƒ«ã‚’è¿”ã™\n",
    "        return target_cache_tensor\n",
    "\n",
    "    def remove(self, output_tensor: torch.Tensor) -> None:\n",
    "        \"\"\"Release corresponding cache tensor.\n",
    "\n",
    "        Args:\n",
    "            output_tensor (torch.Tensor): The tensor to be released.\n",
    "        \"\"\"\n",
    "        out_id = id(output_tensor)\n",
    "        with self.lock:\n",
    "            if out_id not in self.output_to_cache:\n",
    "                raise ValueError(\"Tensor not found in cache.\")\n",
    "            cache_id = self.output_to_cache.pop(out_id)\n",
    "            del self.cache_to_output[cache_id]\n",
    "\n",
    "    def __str__(self):\n",
    "        with self.lock:\n",
    "            num_cached = len(self.cache)\n",
    "            num_used = len(self.output_to_cache)\n",
    "            total_cache_size = sum([v.numel() * v.element_size() for v in self.cache.values()])\n",
    "        return f\"PinMemoryCache(num_cached={num_cached}, num_used={num_used}, total_cache_size={total_cache_size / 1024**3:.2f} GB, hit rate={self.hit_cnt / self.total_cnt:.2f})\"\n",
    "\n",
    "PinMemoryCache.force_dtype = dtype\n",
    "pin_memory_cache_pre_alloc_numels = cfg.get(\"pin_memory_cache_pre_alloc_numels\", None)\n",
    "PinMemoryCache.pre_alloc_numels = pin_memory_cache_pre_alloc_numels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d0950f",
   "metadata": {},
   "source": [
    "### CollosalAI Boosterã®åˆæœŸåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be0847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã®è¾æ›¸ã‚’ä½œæˆ\n",
    "# ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã¯ã€åˆ†æ•£å­¦ç¿’ã§é€šä¿¡ãƒ»åŒæœŸã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã®é›†åˆ\n",
    "_GLOBAL_PARALLEL_GROUPS = dict()\n",
    "\n",
    "def set_data_parallel_group(group: dist.ProcessGroup):\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«è¨­å®šã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        group (dist.ProcessGroup): ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—\n",
    "    \"\"\"\n",
    "    _GLOBAL_PARALLEL_GROUPS[\"data\"] = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fac278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_colossalai_plugin(\n",
    "    plugin: str,\n",
    "    dtype: str,\n",
    "    grad_clip: float,\n",
    "    **kwargs,\n",
    ") -> LowLevelZeroPlugin | HybridParallelPlugin:\n",
    "    \"\"\"\n",
    "    ColossalAIã®ä¸¦åˆ—ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚’ä½œæˆã™ã‚‹\n",
    "    ä¸¦åˆ—åŒ–ã®è¨­å®šã‚’ç®¡ç†ã™ã‚‹é‡è¦ãªã‚¯ãƒ©ã‚¹\n",
    "    Zero1ã€Zero2ã€HybridParallelã®ã„ãšã‚Œã‹ã‚’é¸æŠå¯èƒ½\n",
    "\n",
    "    Args:\n",
    "        plugin (str): ãƒ—ãƒ©ã‚°ã‚¤ãƒ³å\n",
    "        dtype (str): ãƒ‡ãƒ¼ã‚¿å‹\n",
    "        grad_clip (float): å‹¾é…ã‚¯ãƒªãƒƒãƒ—å€¤\n",
    "    Returns:\n",
    "        LowLevelZeroPlugin | HybridParallelPlugin: ãƒ—ãƒ©ã‚°ã‚¤ãƒ³\n",
    "    \"\"\"\n",
    "    # ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®å¼•æ•°ã‚’è¨­å®š\n",
    "    plugin_kwargs = dict(\n",
    "        precision=dtype, # bf16\n",
    "        initial_scale=2**16,\n",
    "        max_norm=grad_clip, # 1.0\n",
    "        overlap_allgather=True,\n",
    "        cast_inputs=False,\n",
    "        reduce_bucket_size_in_m=20,\n",
    "    )\n",
    "\n",
    "    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°ã§ä¸Šæ›¸ã\n",
    "    plugin_kwargs.update(kwargs)\n",
    "\n",
    "    # sequence parallelismã®ã‚µã‚¤ã‚ºã‚’å–å¾—\n",
    "    # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸¦åˆ—ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’è¤‡æ•°ã®GPUã«åˆ†å‰²ã—ã¦å‡¦ç†ã™ã‚‹æ‰‹æ³•\n",
    "    sp_size = plugin_kwargs.get(\"sp_size\", 1)\n",
    "\n",
    "    # True\n",
    "    if plugin == \"zero1\" or plugin == \"zero2\":\n",
    "        assert sp_size == 1, \"Zero plugin does not support sequence parallelism\"\n",
    "\n",
    "        # zero2 -> 2\n",
    "        stage = 1 if plugin == \"zero1\" else 2\n",
    "\n",
    "        # LowLevelZeroPluginã‚’ä½œæˆ\n",
    "        # https://colossalai.org/docs/basics/booster_plugins/#low-level-zero-plugin\n",
    "        plugin = LowLevelZeroPlugin(\n",
    "            stage=stage, # 2\n",
    "            **plugin_kwargs,\n",
    "        )\n",
    "\n",
    "        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’è¨­å®š\n",
    "        set_data_parallel_group(dist.group.WORLD)\n",
    "\n",
    "    # False\n",
    "    elif plugin == \"hybrid\":\n",
    "        plugin_kwargs[\"find_unused_parameters\"] = True\n",
    "        reduce_bucket_size_in_m = plugin_kwargs.pop(\"reduce_bucket_size_in_m\")\n",
    "        if \"zero_bucket_size_in_m\" not in plugin_kwargs:\n",
    "            plugin_kwargs[\"zero_bucket_size_in_m\"] = reduce_bucket_size_in_m\n",
    "        plugin_kwargs.pop(\"cast_inputs\")\n",
    "        plugin_kwargs[\"enable_metadata_cache\"] = False\n",
    "\n",
    "        custom_policy = plugin_kwargs.pop(\"custom_policy\", None)\n",
    "        if custom_policy is not None:\n",
    "            custom_policy = custom_policy()\n",
    "        plugin = HybridParallelPlugin(\n",
    "            custom_policy=custom_policy,\n",
    "            **plugin_kwargs,\n",
    "        )\n",
    "        set_tensor_parallel_group(plugin.tp_group)\n",
    "        set_sequence_parallel_group(plugin.sp_group)\n",
    "        set_data_parallel_group(plugin.dp_group)\n",
    "\n",
    "    # False\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown plugin {plugin}\")\n",
    "    return plugin\n",
    "\n",
    "# zero2\n",
    "plugin_type = cfg.get(\"plugin\", \"zero2\")\n",
    "\n",
    "# dtype=bf16, grad_clip=1.0\n",
    "plugin_config = cfg.get(\"plugin_config\", {})\n",
    "\n",
    "# ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚’ä½œæˆ\n",
    "plugin = (\n",
    "    create_colossalai_plugin(\n",
    "        plugin=plugin_type,\n",
    "        dtype=cfg.get(\"dtype\", \"bf16\"),\n",
    "        grad_clip=cfg.get(\"grad_clip\", 0),\n",
    "        **plugin_config,\n",
    "    )\n",
    "    if plugin_type != \"none\"\n",
    "    else None\n",
    ")\n",
    "\n",
    "plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f02f646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚’æŒ‡å®šã—ã¦Boosterã‚’åˆæœŸåŒ–\n",
    "# Boosterã¯ã€ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã«å¯¾ã—ä¸¦åˆ—åŒ–æ©Ÿèƒ½ã‚’æ³¨å…¥ã™ã‚‹ãƒ¡ã‚¤ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "# https://colossalai.org/docs/basics/booster_api\n",
    "booster = Booster(plugin=plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd105e65",
   "metadata": {},
   "source": [
    "### å®Ÿé¨“ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ae1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_distributed() -> bool:\n",
    "    \"\"\"\n",
    "    åˆ†æ•£è¨­å®šã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹\n",
    "\n",
    "    Returns:\n",
    "        bool: åˆ†æ•£è¨­å®šã®å ´åˆã¯Trueã€ãã†ã§ãªã„å ´åˆã¯False\n",
    "    \"\"\"\n",
    "\n",
    "    # WORLD_SIZEç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹ã§åˆ¤å®š\n",
    "    return os.environ.get(\"WORLD_SIZE\", None) is not None\n",
    "\n",
    "# æ¤œè¨¼\n",
    "is_distributed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d68e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_main_process() -> bool:\n",
    "    \"\"\"\n",
    "    ç¾åœ¨ã®ãƒ—ãƒ­ã‚»ã‚¹ãŒãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹\n",
    "\n",
    "    Returns:\n",
    "        bool: ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®å ´åˆã¯Trueã€ãã†ã§ãªã„å ´åˆã¯False\n",
    "    \"\"\"\n",
    "\n",
    "    return not is_distributed() or dist.get_rank() == 0\n",
    "\n",
    "# æ¤œè¨¼\n",
    "is_main_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce900e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_to_name(cfg: Config) -> str:\n",
    "    filename = cfg._filename\n",
    "    filename = filename.replace(\"configs/\", \"\")\n",
    "    filename = filename.replace(\".py\", \"\")\n",
    "    filename = filename.replace(\"/\", \"_\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b1b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_string(value: str):\n",
    "    \"\"\"\n",
    "    å…¨ãƒ—ãƒ­ã‚»ã‚¹é–“ã§æ–‡å­—åˆ—ã‚’åŒæœŸã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        value (str): åŒæœŸã™ã‚‹æ–‡å­—åˆ—\n",
    "    Returns:\n",
    "        str: åŒæœŸã•ã‚ŒãŸæ–‡å­—åˆ—\n",
    "    \"\"\"\n",
    "\n",
    "    # åˆ†æ•£è¨­å®šã§ãªã„å ´åˆã€ãã®ã¾ã¾è¿”ã™\n",
    "    # True\n",
    "    if not is_distributed():\n",
    "        return value\n",
    "\n",
    "    # æ–‡å­—åˆ—ã‚’ãƒã‚¤ãƒˆåˆ—ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "    bytes_value = value.encode(\"utf-8\")\n",
    "\n",
    "    # æœ€å¤§é•·256ãƒã‚¤ãƒˆã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆ\n",
    "    max_len = 256\n",
    "    bytes_tensor = torch.zeros(max_len, dtype=torch.uint8).cuda()\n",
    "\n",
    "    # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãŸãƒã‚¤ãƒˆåˆ—ã‚’ç¢ºä¿ã—ãŸãƒ†ãƒ³ã‚½ãƒ«ã«ã‚³ãƒ”ãƒ¼\n",
    "    bytes_tensor[: len(bytes_value)] = torch.tensor(\n",
    "        list(bytes_value), dtype=torch.uint8\n",
    "    )\n",
    "\n",
    "    # ãƒ©ãƒ³ã‚¯0ã®ãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰å…¨ãƒ—ãƒ­ã‚»ã‚¹ã«ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ\n",
    "    torch.distributed.broadcast(bytes_tensor, 0)\n",
    "\n",
    "    # ãƒã‚¤ãƒˆåˆ—ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦æ–‡å­—åˆ—ã«å¤‰æ›\n",
    "    synced_value = bytes_tensor.cpu().numpy().tobytes().decode(\"utf-8\").rstrip(\"\\x00\")\n",
    "\n",
    "    # åŒæœŸã•ã‚ŒãŸæ–‡å­—åˆ—ã‚’è¿”ã™\n",
    "    return synced_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951af3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment_workspace(\n",
    "    output_dir: str, model_name: str = None, config: dict = None, exp_name: str = None\n",
    ") -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    å®Ÿé¨“ã®ãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã™ã‚‹\n",
    "    config.txtã‚„log.txtãªã©ã®å®Ÿé¨“ã”ã¨ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå…¥ã‚‹\n",
    "\n",
    "    Args:\n",
    "        output_dir (str): å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹\n",
    "        model_name (str): ãƒ¢ãƒ‡ãƒ«ã®åå‰\n",
    "        exp_name (str): å®Ÿé¨“ã®åå‰ã€Noneã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨\n",
    "    Returns:\n",
    "        tuple[str, str]: å®Ÿé¨“ã®åå‰ã¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹\n",
    "    \"\"\"\n",
    "    log_.info(f\"create_experiment_workspace {output_dir=} {model_name=} {config=} {exp_name=}\")\n",
    "\n",
    "    # å®Ÿé¨“ã®åå‰ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆ\n",
    "    # True\n",
    "    if exp_name is None:\n",
    "        # ç¾åœ¨ã®æ—¥æ™‚ã‚’yyMMdd_HHmmsså½¢å¼ã§å–å¾—\n",
    "        experiment_index = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "\n",
    "        # ã™ã¹ã¦ã®ãƒ—ãƒ­ã‚»ã‚¹ã§åŒæœŸ\n",
    "        experiment_index = sync_string(experiment_index)\n",
    "\n",
    "        # ãƒ¢ãƒ‡ãƒ«åã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ\n",
    "        # vae_train_video_dc_ae -> -vae_train_video_dc_ae\n",
    "        model_name = (\n",
    "            \"-\" + model_name.replace(\"/\", \"-\") if model_name is not None else \"\"\n",
    "        )\n",
    "\n",
    "        # å®Ÿé¨“ã®åå‰ã‚’ä½œæˆ\n",
    "        # NOTE: ãƒ—ãƒ­ã‚»ã‚¹é–“ã§ä¸€æ„\n",
    "        exp_name = f\"{experiment_index}{model_name}\"\n",
    "\n",
    "    # å®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã‚’ä½œæˆ\n",
    "    exp_dir = f\"{output_dir}/{exp_name}\"\n",
    "\n",
    "    # ãƒ©ãƒ³ã‚¯0ã®ãƒ—ãƒ­ã‚»ã‚¹ã®å ´åˆ\n",
    "    # True\n",
    "    if is_main_process():\n",
    "\n",
    "        # å®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "        os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "        # è¨“ç·´ã®è¨­å®šã‚’config.txtã¨ã—ã¦ä¿å­˜\n",
    "        with open(f\"{exp_dir}/config.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            # JSONå½¢å¼æ›¸ãå‡ºã—\n",
    "            json.dump(config, f, indent=4)\n",
    "\n",
    "    return exp_name, exp_dir\n",
    "\n",
    "\n",
    "exp_name, exp_dir = create_experiment_workspace(\n",
    "    cfg.get(\"outputs\", \"./outputs\"),\n",
    "    model_name=config_to_name(cfg),\n",
    "    config=cfg.to_dict(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebad3b6",
   "metadata": {},
   "source": [
    "### å®Ÿé¨“ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ¨©é™ã‚’å¤‰æ›´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1184869f",
   "metadata": {},
   "source": [
    "ãƒ—ãƒ­ã‚»ã‚¹é–“ã§ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å…±æœ‰ãƒ»ç·¨é›†å¯èƒ½ã«ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb11acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pipeline_enabled(plugin_type: str, plugin_config: dict) -> bool:\n",
    "    \"\"\"\n",
    "    ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—ãŒæœ‰åŠ¹ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹\n",
    "    ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’å±¤å˜ä½ã§åˆ†å‰²ã—ã€å„å±¤ã‚’ç•°ãªã‚‹GPUã§å‡¦ç†ã™ã‚‹æ‰‹æ³•\n",
    "\n",
    "    Args:\n",
    "        plugin_type (str): ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®ã‚¿ã‚¤ãƒ—\n",
    "        plugin_config (dict): ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®è¨­å®š\n",
    "    Returns:\n",
    "        bool: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—ãŒæœ‰åŠ¹ãªå ´åˆã¯Trueã€ãã†ã§ãªã„å ´åˆã¯False\n",
    "    \"\"\"\n",
    "\n",
    "    # zero2ã®å ´åˆã¯ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ãªã®ã§False\n",
    "    return plugin_type == \"hybrid\" and plugin_config.get(\"pp_size\", 1) > 1\n",
    "\n",
    "is_pipeline_enabled(plugin_type, plugin_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_log_process(plugin_type: str, plugin_config: dict) -> bool:\n",
    "    \"\"\"\n",
    "    ãƒ­ã‚°ã‚’å‡ºåŠ›ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        plugin_type (str): ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®ã‚¿ã‚¤ãƒ—\n",
    "        plugin_config (dict): ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã®è¨­å®š\n",
    "    Returns:\n",
    "        bool: ãƒ­ã‚°ã‚’å‡ºåŠ›ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã®å ´åˆã¯Trueã€ãã†ã§ãªã„å ´åˆã¯False\n",
    "    \"\"\"\n",
    "    # False\n",
    "    if is_pipeline_enabled(plugin_type, plugin_config):\n",
    "        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—ã®æœ€å¾Œã®ãƒ©ãƒ³ã‚¯ãŒãƒ­ã‚°ãƒ—ãƒ­ã‚»ã‚¹\n",
    "        # NOTE: æœ€çµ‚å±¤ã‚’æ‹…å½“ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ãŒãƒ­ã‚°ã‚’å‡ºåŠ›ã™ã‚‹ãŸã‚\n",
    "        return dist.get_rank() == dist.get_world_size() - 1\n",
    "\n",
    "    # ãã‚Œä»¥å¤–ã¯ãƒ©ãƒ³ã‚¯0ãŒãƒ­ã‚°ãƒ—ãƒ­ã‚»ã‚¹\n",
    "    return dist.get_rank() == 0\n",
    "\n",
    "is_log_process(plugin_type, plugin_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49473b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True\n",
    "if is_log_process(plugin_type, plugin_config):\n",
    "    log_.info(f\"changing {exp_dir} to share\")\n",
    "\n",
    "    # ã‚°ãƒ«ãƒ¼ãƒ—æ‰€æœ‰æ¨©ã‚’'share'ã«å¤‰æ›´\n",
    "    os.system(f\"chgrp -R share {exp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bde602",
   "metadata": {},
   "source": [
    "### ãƒ­ã‚¬ãƒ¼ã®åˆæœŸåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdd85d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logger(logging_dir: str = None) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã¨stdoutã«æ›¸ãè¾¼ã‚€ãƒ­ã‚¬ãƒ¼ã‚’ä½œæˆã™ã‚‹ã€‚ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®ã¿ãƒ­ã‚°ã‚’å‡ºåŠ›ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        logging_dir (str): ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "    Returns:\n",
    "        logging.Logger: ãƒ­ã‚¬ãƒ¼\n",
    "    \"\"\"\n",
    "\n",
    "    # ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®å ´åˆ\n",
    "    # True\n",
    "    if is_main_process():\n",
    "\n",
    "        # ãƒ­ã‚°ã®è¿½åŠ å¼•æ•°ã‚’è¨­å®š\n",
    "        additional_args = dict()\n",
    "        if logging_dir is not None:\n",
    "            additional_args[\"handlers\"] = [\n",
    "                # logging.StreamHandler(), # stdoutã«å‡ºåŠ›\n",
    "                logging.FileHandler(f\"{logging_dir}/log.txt\"), # ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›\n",
    "            ]\n",
    "\n",
    "        # ãƒ­ã‚°ã®åŸºæœ¬è¨­å®š\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            # ä¾‹: [2025-01-01 12:34:56] ã“ã“ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
    "            format=\"[\\033[34m%(asctime)s\\033[0m] %(message)s\",\n",
    "            datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "            **additional_args,\n",
    "        )\n",
    "\n",
    "        # ãƒ­ã‚¬ãƒ¼ã‚’å–å¾—\n",
    "        logger = logging.getLogger(__name__)\n",
    "\n",
    "        # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "        if logging_dir is not None:\n",
    "            # ãƒ­ã‚°ã‚’å®Ÿè¡Œ\n",
    "            logger.info(\"Experiment directory created at %s\", logging_dir)\n",
    "\n",
    "    # ãã‚Œä»¥å¤–ã®ãƒ—ãƒ­ã‚»ã‚¹ã®å ´åˆ\n",
    "    else:\n",
    "        # ãƒ­ã‚¬ãƒ¼ã‚’å–å¾—\n",
    "        logger = logging.getLogger(__name__)\n",
    "\n",
    "        # NullHandlerã‚’è¿½åŠ ã—ã¦ãƒ­ã‚°ã‚’ç„¡åŠ¹åŒ–\n",
    "        logger.addHandler(logging.NullHandler())\n",
    "\n",
    "    return logger\n",
    "\n",
    "logger = create_logger(exp_dir)\n",
    "\n",
    "# è¨­å®šã‚’ãƒ­ã‚°ã«å‡ºåŠ›\n",
    "logger.info(\"Training configuration:\\n %s\", pformat(cfg.to_dict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b832b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_message(*args, level: str = \"info\"):\n",
    "    \"\"\"\n",
    "    ãƒ­ã‚¬ãƒ¼ã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ãƒ­ã‚°ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        *args: ãƒ­ã‚°ã«è¨˜éŒ²ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
    "        level (str): ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    if level == \"info\":\n",
    "        logger.info(*args)\n",
    "    elif level == \"warning\":\n",
    "        logger.warning(*args)\n",
    "    elif level == \"error\":\n",
    "        logger.error(*args)\n",
    "    elif level == \"print\":\n",
    "        print(*args)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid logging level: {level}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3471e241",
   "metadata": {},
   "source": [
    "### Tensorboardã¨WandBã®åˆæœŸåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19da0ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensorboard_writer(exp_dir: str) -> SummaryWriter:\n",
    "    \"\"\"\n",
    "    ãƒ†ãƒ³ã‚½ãƒ«ãƒœãƒ¼ãƒ‰ã®ãƒ©ã‚¤ã‚¿ãƒ¼ã‚’ä½œæˆã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        exp_dir (str): ãƒ†ãƒ³ã‚½ãƒ«ãƒœãƒ¼ãƒ‰ãƒ­ã‚°ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "    Returns:\n",
    "        SummaryWriter: ãƒ†ãƒ³ã‚½ãƒ«ãƒœãƒ¼ãƒ‰ã®ãƒ©ã‚¤ã‚¿ãƒ¼\n",
    "    \"\"\"\n",
    "    # ãƒ†ãƒ³ã‚½ãƒ«ãƒœãƒ¼ãƒ‰ã®ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "    tensorboard_dir = f\"{exp_dir}/tensorboard\"\n",
    "    os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "\n",
    "    # ãƒ†ãƒ³ã‚½ãƒ«ãƒœãƒ¼ãƒ‰ã®ãƒ©ã‚¤ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–\n",
    "    writer = SummaryWriter(tensorboard_dir)\n",
    "    return writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab51d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoardã®ãƒ©ã‚¤ã‚¿ãƒ¼ã®å¤‰æ•°ã‚’åˆæœŸåŒ–\n",
    "tb_writer = None\n",
    "\n",
    "# ãƒã‚¹ã‚¿ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã®å ´åˆ\n",
    "# True\n",
    "if coordinator.is_master():\n",
    "\n",
    "    # TensorBoardã®ãƒ©ã‚¤ã‚¿ãƒ¼ã‚’ä½œæˆ\n",
    "    tb_writer = create_tensorboard_writer(exp_dir)\n",
    "\n",
    "    # wandbã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
    "    # False\n",
    "    if cfg.get(\"wandb\", False):\n",
    "\n",
    "        # wandbã‚’åˆæœŸåŒ–\n",
    "        wandb.init(\n",
    "            project=cfg.get(\"wandb_project\", \"Open-Sora\"),\n",
    "            name=cfg.get(\"wandb_expr_name\", exp_name),\n",
    "            config=cfg.to_dict(),\n",
    "            dir=exp_dir,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84fc906",
   "metadata": {},
   "source": [
    "### ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b89691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä¸Šæ›¸ã\n",
    "cfg.dataset[\"data_path\"] = PEXELS_DATASET_DIR\n",
    "cfg.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1ae75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(input_path, memory_efficient=False):\n",
    "    \"\"\"\n",
    "    ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "    TextDatasetã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–æ™‚ã«ä½¿ç”¨\n",
    "\n",
    "    Args:\n",
    "        input_path (str): å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
    "        memory_efficient (bool): ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ¢ãƒ¼ãƒ‰ã§èª­ã¿è¾¼ã‚€ã‹ã©ã†ã‹\n",
    "    Returns:\n",
    "        pd.DataFrame: èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ \n",
    "    \"\"\"\n",
    "\n",
    "    log_.info(f\"read_file {input_path=} {memory_efficient=}\")\n",
    "\n",
    "    # True\n",
    "    if input_path.endswith(\".csv\"):\n",
    "        assert not memory_efficient, \"Memory efficient mode is not supported for CSV files\"\n",
    "\n",
    "        # Pandasã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "        # /workspaces/open-sora/Open-Sora/pexels_5.csv\n",
    "        return pd.read_csv(input_path)\n",
    "\n",
    "    # False\n",
    "    elif input_path.endswith(\".parquet\"):\n",
    "        columns = None\n",
    "        if memory_efficient:\n",
    "            columns = [\"path\", \"num_frames\", \"height\", \"width\", \"aspect_ratio\", \"fps\", \"resolution\"]\n",
    "        if SUPPORT_DASK:\n",
    "            ret = dd.read_parquet(input_path, columns=columns).compute()\n",
    "        else:\n",
    "            ret = pd.read_parquet(input_path, columns=columns)\n",
    "        return ret\n",
    "\n",
    "    # False\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unsupported file format: {input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127799df",
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_KEYS = (\"neg\", \"path\")\n",
    "\n",
    "# @DATASETS.register_module(\"text\")\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”¨ã®PyTorchã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹\n",
    "    VideoTextDatasetã§ç¶™æ‰¿ã•ã‚Œã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str = None, tokenize_fn: callable = None, fps_max: int = 16, vmaf: bool = False, memory_efficient: bool = False, **kwargs):\n",
    "        log_.info(f\"TextDataset.__init__ {data_path=} {tokenize_fn=} {fps_max=} {vmaf=} {memory_efficient=} {kwargs=}\")\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¹ã‚’èª­ã¿è¾¼ã¿\n",
    "        self.data_path = data_path\n",
    "        self.data = read_file(data_path, memory_efficient=memory_efficient)\n",
    "\n",
    "        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ¢ãƒ¼ãƒ‰ã‚’è¨­å®š\n",
    "        self.memory_efficient = memory_efficient\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºé–¢æ•°ã‚’è¨­å®š\n",
    "        self.tokenize_fn = tokenize_fn\n",
    "\n",
    "        self.vmaf = vmaf\n",
    "\n",
    "        # æœ€å¤§FPSãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "        if fps_max is not None:\n",
    "            # æœ€å¤§FPSã‚’è¨­å®š\n",
    "            self.fps_max = fps_max\n",
    "        else:\n",
    "            self.fps_max = 999999999\n",
    "\n",
    "    def to_efficient(self):\n",
    "        if self.memory_efficient:\n",
    "            addition_data_path = self.data_path.split(\".\")[0]\n",
    "            self._data = self.data\n",
    "            self.data = EfficientParquet(self._data, addition_data_path)\n",
    "\n",
    "    def getitem(self, index: int) -> dict:\n",
    "        log_.info(f\"TextDataset.getitem {index=}\")\n",
    "        ret = dict()\n",
    "        sample = self.data.iloc[index].to_dict()\n",
    "        sample_fps = sample.get(\"fps\", np.nan)\n",
    "        new_fps, sampling_interval = map_target_fps(sample_fps, self.fps_max)\n",
    "        ret.update({\"sampling_interval\": sampling_interval})\n",
    "\n",
    "        if \"text\" in sample:\n",
    "            ret[\"text\"] = sample.pop(\"text\")\n",
    "            postfixs = []\n",
    "            if new_fps != 0 and self.fps_max < 999:\n",
    "                postfixs.append(f\"{new_fps} FPS\")\n",
    "            if self.vmaf and \"score_vmafmotion\" in sample and not np.isnan(sample[\"score_vmafmotion\"]):\n",
    "                postfixs.append(f\"{int(sample['score_vmafmotion'] + 0.5)} motion score\")\n",
    "            postfix = \" \" + \", \".join(postfixs) + \".\" if postfixs else \"\"\n",
    "            ret[\"text\"] = ret[\"text\"] + postfix\n",
    "            if self.tokenize_fn is not None:\n",
    "                ret.update({k: v.squeeze(0) for k, v in self.tokenize_fn(ret[\"text\"]).items()})\n",
    "\n",
    "        if \"ref\" in sample:  # i2v & v2v reference\n",
    "            ret[\"ref\"] = sample.pop(\"ref\")\n",
    "\n",
    "        # name of the generated sample\n",
    "        if \"name\" in sample:  # sample name (`dataset_idx`)\n",
    "            ret[\"name\"] = sample.pop(\"name\")\n",
    "        else:\n",
    "            ret[\"index\"] = index  # use index for name\n",
    "        valid_sample = {k: v for k, v in sample.items() if k in VALID_KEYS}\n",
    "        ret.update(valid_sample)\n",
    "        return ret\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.getitem(index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715289f",
   "metadata": {},
   "source": [
    "### å‹•ç”»ã‚’èª­ã¿è¾¼ã‚€é–¢æ•°ã‚’ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f3be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_from_stream(video_frames, container: \"av.container.Container\", start_offset: float, end_offset: float, pts_unit: str, stream: \"av.stream.Stream\", stream_name: dict[str, int | tuple[int, ...] | list[int] | None], filename: str | None = None) -> list[\"av.frame.Frame\"]:\n",
    "    \"\"\"\n",
    "    æŒ‡å®šã•ã‚ŒãŸã‚¹ãƒˆãƒªãƒ¼ãƒ ã‹ã‚‰ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€\n",
    "    read_video_avé–¢æ•°ã§ä½¿ç”¨\n",
    "\n",
    "    Args:\n",
    "        video_frames: ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ\n",
    "        container (av.container.Container): ãƒ¡ãƒ‡ã‚£ã‚¢ã‚³ãƒ³ãƒ†ãƒŠ\n",
    "        start_offset (float): èª­ã¿è¾¼ã¿é–‹å§‹ã‚ªãƒ•ã‚»ãƒƒãƒˆ\n",
    "        end_offset (float): èª­ã¿è¾¼ã¿çµ‚äº†ã‚ªãƒ•ã‚»ãƒƒãƒˆ\n",
    "        pts_unit (str): PTSã®å˜ä½ (\"sec\"ã¾ãŸã¯\"pts\")\n",
    "        stream (av.stream.Stream): èª­ã¿è¾¼ã‚€ã‚¹ãƒˆãƒªãƒ¼ãƒ \n",
    "        stream_name (dict): ã‚¹ãƒˆãƒªãƒ¼ãƒ åã®è¾æ›¸\n",
    "        filename (str | None): ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰\n",
    "    Returns:\n",
    "        list[av.frame.Frame]: èª­ã¿è¾¼ã‚“ã ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒªã‚¹ãƒˆ\n",
    "    \"\"\"\n",
    "    log_.info(f\"_read_from_stream {start_offset=} {end_offset=} {pts_unit=} {stream=} {stream_name=} {filename=}\")\n",
    "\n",
    "    if pts_unit == \"sec\":\n",
    "        # TODO: we should change all of this from ground up to simply take\n",
    "        # sec and convert to MS in C++\n",
    "        start_offset = int(math.floor(start_offset * (1 / stream.time_base)))\n",
    "        if end_offset != float(\"inf\"):\n",
    "            end_offset = int(math.ceil(end_offset * (1 / stream.time_base)))\n",
    "    else:\n",
    "        warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
    "\n",
    "    should_buffer = True\n",
    "    max_buffer_size = 5\n",
    "    if stream.type == \"video\":\n",
    "        # DivX-style packed B-frames can have out-of-order pts (2 frames in a single pkt)\n",
    "        # so need to buffer some extra frames to sort everything\n",
    "        # properly\n",
    "        extradata = stream.codec_context.extradata\n",
    "        # overly complicated way of finding if `divx_packed` is set, following\n",
    "        # https://github.com/FFmpeg/FFmpeg/commit/d5a21172283572af587b3d939eba0091484d3263\n",
    "        if extradata and b\"DivX\" in extradata:\n",
    "            # can't use regex directly because of some weird characters sometimes...\n",
    "            pos = extradata.find(b\"DivX\")\n",
    "            d = extradata[pos:]\n",
    "            o = re.search(rb\"DivX(\\d+)Build(\\d+)(\\w)\", d)\n",
    "            if o is None:\n",
    "                o = re.search(rb\"DivX(\\d+)b(\\d+)(\\w)\", d)\n",
    "            if o is not None:\n",
    "                should_buffer = o.group(3) == b\"p\"\n",
    "    seek_offset = start_offset\n",
    "    # some files don't seek to the right location, so better be safe here\n",
    "    seek_offset = max(seek_offset - 1, 0)\n",
    "    if should_buffer:\n",
    "        # FIXME this is kind of a hack, but we will jump to the previous keyframe\n",
    "        # so this will be safe\n",
    "        seek_offset = max(seek_offset - max_buffer_size, 0)\n",
    "    try:\n",
    "        # TODO check if stream needs to always be the video stream here or not\n",
    "        container.seek(seek_offset, any_frame=False, backward=True, stream=stream)\n",
    "    except av.AVError as e:\n",
    "        print(f\"[Warning] Error while seeking video {filename}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # == main ==\n",
    "    buffer_count = 0\n",
    "    frames_pts = []\n",
    "    cnt = 0\n",
    "    try:\n",
    "        for _idx, frame in enumerate(container.decode(**stream_name)):\n",
    "            frames_pts.append(frame.pts)\n",
    "            video_frames[cnt] = frame.to_rgb().to_ndarray()\n",
    "            cnt += 1\n",
    "            if cnt >= len(video_frames):\n",
    "                break\n",
    "            if frame.pts >= end_offset:\n",
    "                if should_buffer and buffer_count < max_buffer_size:\n",
    "                    buffer_count += 1\n",
    "                    continue\n",
    "                break\n",
    "    except av.AVError as e:\n",
    "        print(f\"[Warning] Error while reading video {filename}: {e}\")\n",
    "\n",
    "    # garbage collection for thread leakage\n",
    "    container.close()\n",
    "    del container\n",
    "    # NOTE: manually garbage collect to close pyav threads\n",
    "    gc.collect()\n",
    "\n",
    "    # ensure that the results are sorted wrt the pts\n",
    "    # NOTE: here we assert frames_pts is sorted\n",
    "    start_ptr = 0\n",
    "    end_ptr = cnt\n",
    "    while start_ptr < end_ptr and frames_pts[start_ptr] < start_offset:\n",
    "        start_ptr += 1\n",
    "    while start_ptr < end_ptr and frames_pts[end_ptr - 1] > end_offset:\n",
    "        end_ptr -= 1\n",
    "    if start_offset > 0 and start_offset not in frames_pts[start_ptr:end_ptr]:\n",
    "        # if there is no frame that exactly matches the pts of start_offset\n",
    "        # add the last frame smaller than start_offset, to guarantee that\n",
    "        # we will have all the necessary data. This is most useful for audio\n",
    "        if start_ptr > 0:\n",
    "            start_ptr -= 1\n",
    "    result = video_frames[start_ptr:end_ptr].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ecae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_av(filename: str, start_pts: float | Fraction = 0, end_pts: float | Fraction | None = None, pts_unit: str = \"pts\", output_format: str = \"THWC\") -> tuple[torch.Tensor, torch.Tensor, dict]:\n",
    "    \"\"\"\n",
    "    ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å‹•ç”»ã‚’èª­ã¿è¾¼ã¿ã€å‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ ã¨éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä¸¡æ–¹ã‚’è¿”ã™\n",
    "\n",
    "    torchvision.io.video.read_videoã‹ã‚‰ä¿®æ­£ã•ã‚ŒãŸå®Ÿè£…:\n",
    "    1. éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æŠ½å‡ºã›ãšã€aframesã¯ç©ºã§è¿”ã™\n",
    "    2. ãƒã‚§ãƒƒã‚¯ã‚’å‰Šé™¤ã—ã€pyavã®ã¿ã‚’ã‚µãƒãƒ¼ãƒˆ\n",
    "    3. container.close()ã¨gc.collect()ã‚’è¿½åŠ ã—ã¦ã‚¹ãƒ¬ãƒƒãƒ‰ãƒªãƒ¼ã‚¯ã‚’å›é¿\n",
    "    4. ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‚’ã§ãã‚‹ã ã‘å›é¿\n",
    "\n",
    "    Args:\n",
    "        filename (str): å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹\n",
    "        start_pts (int if pts_unit = 'pts', float / Fraction if pts_unit = 'sec', optional):\n",
    "            å‹•ç”»ã®é–‹å§‹ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ™‚é–“\n",
    "        end_pts (int if pts_unit = 'pts', float / Fraction if pts_unit = 'sec', optional):\n",
    "            çµ‚äº†ãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ™‚é–“\n",
    "        pts_unit (str, optional): start_ptsã¨end_ptsã®å€¤ãŒè§£é‡ˆã•ã‚Œã‚‹å˜ä½ã€'pts'ã¾ãŸã¯'sec'ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯'pts'ã€‚\n",
    "        output_format (str, optional): å‡ºåŠ›å‹•ç”»ãƒ†ãƒ³ã‚½ãƒ«ã®å½¢å¼ã€‚\"THWC\"ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ã¾ãŸã¯\"TCHW\"ã®ã„ãšã‚Œã‹ã€‚\n",
    "\n",
    "    Returns:\n",
    "        vframes (Tensor[T, H, W, C] or Tensor[T, C, H, W]): `T`å‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ \n",
    "        aframes (Tensor[K, L]): éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ã€`K`ã¯ãƒãƒ£ãƒãƒ«æ•°ã€`L`ã¯ãƒã‚¤ãƒ³ãƒˆæ•°\n",
    "        info (dict): å‹•ç”»ã¨éŸ³å£°ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã€‚video_fpsï¼ˆfloatï¼‰ã¨audio_fpsï¼ˆintï¼‰ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å«ã‚€å ´åˆãŒã‚ã‚‹\n",
    "    \"\"\"\n",
    "    log_.info(f\"read_video_av {filename=} {start_pts=} {end_pts=} {pts_unit=} {output_format=}\")\n",
    "\n",
    "    # 1) å‡ºåŠ›å½¢å¼ã®ãƒã‚§ãƒƒã‚¯\n",
    "\n",
    "    output_format = output_format.upper()\n",
    "\n",
    "    # THWCã‹TCHWã®ã¿ã‚µãƒãƒ¼ãƒˆ\n",
    "    # T = time, H = height, W = width, C = channels\n",
    "    if output_format not in (\"THWC\", \"TCHW\"):\n",
    "        raise ValueError(f\"output_format should be either 'THWC' or 'TCHW', got {output_format}.\")\n",
    "\n",
    "    # 2) ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯\n",
    "\n",
    "    if not os.path.exists(filename):\n",
    "        raise RuntimeError(f\"File not found: {filename}\")\n",
    "\n",
    "    # 3) pyavãŒä½¿ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯\n",
    "\n",
    "    # torchvisionã®ãƒ¡ã‚½ãƒƒãƒ‰\n",
    "    assert get_video_backend() == \"pyav\", \"pyav backend is required for read_video_av\"\n",
    "\n",
    "    # torchvision.datasets.videoã®ãƒ¡ã‚½ãƒƒãƒ‰\n",
    "    _check_av_available()\n",
    "\n",
    "    # 4) é–‹å§‹PTSã¨çµ‚äº†PTSã‚’æ¤œè¨¼\n",
    "\n",
    "    # pts = Presentation Time Stamp\n",
    "    # å„ãƒ•ãƒ¬ãƒ¼ãƒ ã®å†ç”Ÿæ™‚åˆ»ã‚’ç¤ºã™ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—\n",
    "\n",
    "    # çµ‚äº†PTSãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã€ç„¡é™å¤§ã«è¨­å®š\n",
    "    if end_pts is None:\n",
    "        end_pts = float(\"inf\")\n",
    "\n",
    "    # çµ‚äº†PTSãŒé–‹å§‹PTSã‚ˆã‚Šå°ã•ã„å ´åˆã€ã‚¨ãƒ©ãƒ¼ã‚’ã‚¹ãƒ­ãƒ¼\n",
    "    if end_pts < start_pts:\n",
    "        raise ValueError(f\"end_pts should be larger than start_pts, got start_pts={start_pts} and end_pts={end_pts}\")\n",
    "\n",
    "    # 5) å‹•ç”»æƒ…å ±ã®å–å¾—\n",
    "\n",
    "    info = {}\n",
    "\n",
    "    # PyAVã§ã‚³ãƒ³ãƒ†ãƒŠã‚’é–‹ã\n",
    "    # ã‚³ãƒ³ãƒ†ãƒŠã¯ã€å‹•ç”»ã‚„éŸ³å£°ãªã©ã®ãƒ¡ãƒ‡ã‚£ã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼\n",
    "    container = av.open(filename, metadata_errors=\"ignore\")\n",
    "\n",
    "    # ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰å¹³å‡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—\n",
    "    video_fps = container.streams.video[0].average_rate\n",
    "\n",
    "    # floatã«å¤‰æ›ã—ã¦å‹•ç”»æƒ…å ±ã«è¿½åŠ \n",
    "    if video_fps is not None:\n",
    "        info[\"video_fps\"] = float(video_fps)\n",
    "\n",
    "    # 6) å‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ ã®èª­ã¿è¾¼ã¿\n",
    "\n",
    "    # 0ç•ªç›®ã®å‹•ç”»ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ã‚’ä½œæˆ\n",
    "    iter_video = container.decode(**{\"video\": 0})\n",
    "\n",
    "    # æœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã—ã¦RGBå½¢å¼ã«å¤‰æ›ã—ã€NumPyé…åˆ—ã«å¤‰æ›\n",
    "    frame = next(iter_video).to_rgb().to_ndarray()\n",
    "\n",
    "    # å½¢çŠ¶ã¯(H, W, C)ãªã®ã§ã€é«˜ã•ã¨å¹…ã‚’å–å¾—\n",
    "    height, width = frame.shape[:2]\n",
    "\n",
    "    # å‹•ç”»ã‚¹ãƒˆãƒªãƒ¼ãƒ ã®ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚’å–å¾—\n",
    "    total_frames = container.streams.video[0].frames\n",
    "\n",
    "    # ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãŒ0ã®å ´åˆã€æœ€å¤§ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚’ä½¿ç”¨\n",
    "    if total_frames == 0:\n",
    "        total_frames = MAX_NUM_FRAMES\n",
    "        warnings.warn(f\"total_frames is 0, using {MAX_NUM_FRAMES} as a fallback\")\n",
    "\n",
    "    # ã‚³ãƒ³ãƒ†ãƒŠã‚’é–‰ã˜ã¦ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‚’å›é¿\n",
    "    container.close()\n",
    "    del container\n",
    "\n",
    "    # å‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ ¼ç´ã™ã‚‹NumPyé…åˆ—ã‚’åˆæœŸåŒ–\n",
    "    # NOTE: np.zerosã¯å®Ÿéš›ã«ã¯ãƒ¡ãƒ¢ãƒªã‚’å‰²ã‚Šå½“ã¦ãªã„ãŸã‚ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‚’å›é¿ã§ãã‚‹\n",
    "    video_frames = np.zeros((total_frames, height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    try:\n",
    "        # PyAVã§ã‚³ãƒ³ãƒ†ãƒŠã‚’å†åº¦é–‹ã\n",
    "        container = av.open(filename, metadata_errors=\"ignore\")\n",
    "\n",
    "        assert container.streams.video is not None\n",
    "\n",
    "        # å‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‹ã‚‰èª­ã¿è¾¼ã‚€\n",
    "        video_frames = _read_from_stream(\n",
    "            video_frames,\n",
    "            container,\n",
    "            start_pts,\n",
    "            end_pts,\n",
    "            pts_unit,\n",
    "            container.streams.video[0],\n",
    "            {\"video\": 0},\n",
    "            filename=filename,\n",
    "        )\n",
    "    except av.AVError as e:\n",
    "        print(f\"[Warning] Error while reading video {filename}: {e}\")\n",
    "\n",
    "    # 7) å‡ºåŠ›ãƒ†ãƒ³ã‚½ãƒ«ã®ä½œæˆ\n",
    "\n",
    "    # NumPyé…åˆ—ã‹ã‚‰PyTorchãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆ\n",
    "    vframes = torch.from_numpy(video_frames).clone()\n",
    "    del video_frames\n",
    "\n",
    "    # å‡ºåŠ›å½¢å¼ãŒTCHWã®å ´åˆã€ãƒ†ãƒ³ã‚½ãƒ«ã®æ¬¡å…ƒã‚’ä¸¦ã¹æ›¿ãˆ\n",
    "    if output_format == \"TCHW\":\n",
    "        # [T,H,W,C] --> [T,C,H,W]\n",
    "        vframes = vframes.permute(0, 3, 1, 2)\n",
    "\n",
    "    # 8) ç©ºã®éŸ³å£°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ†ãƒ³ã‚½ãƒ«ã‚’ä½œæˆ\n",
    "    aframes = torch.empty((1, 0), dtype=torch.float32)\n",
    "\n",
    "    return vframes, aframes, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef793e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video(video_path, backend=\"av\"):\n",
    "    \"\"\"\n",
    "    å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€å‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ ã¨å‹•ç”»æƒ…å ±ã‚’è¿”ã™\n",
    "\n",
    "    Args:\n",
    "        video_path (str): å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ‘ã‚¹\n",
    "        backend (str): ä½¿ç”¨ã™ã‚‹ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã€\"cv2\"ã¾ãŸã¯\"av\"ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯\"av\"ã€‚\n",
    "    Returns:\n",
    "        vframes (Tensor): å‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        vinfo (dict): å‹•ç”»ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿\n",
    "    \"\"\"\n",
    "    log_.info(f\"read_video {video_path=} {backend=}\")\n",
    "\n",
    "    # False\n",
    "    if backend == \"cv2\":\n",
    "        vframes, vinfo = read_video_cv2(video_path)\n",
    "\n",
    "    # True\n",
    "    elif backend == \"av\":\n",
    "        vframes, _, vinfo = read_video_av(filename=video_path, pts_unit=\"sec\", output_format=\"TCHW\")\n",
    "\n",
    "    # False\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    return vframes, vinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa342c75",
   "metadata": {},
   "source": [
    "### å‹•ç”»ã‚’æ™‚ç³»åˆ—ã§ã‚¯ãƒ­ãƒƒãƒ—ã™ã‚‹é–¢æ•°ã‚’ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaf5872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalRandomCrop(object):\n",
    "    \"\"\"Temporally crop the given frame indices at a random location.\n",
    "\n",
    "    Args:\n",
    "            size (int): Desired length of frames will be seen in the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, total_frames):\n",
    "        rand_end = max(0, total_frames - self.size - 1)\n",
    "        begin_index = random.randint(0, rand_end)\n",
    "        end_index = min(begin_index + self.size, total_frames)\n",
    "        return begin_index, end_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ad364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_random_crop(\n",
    "    vframes: torch.Tensor, num_frames: int, frame_interval: int, return_frame_indices: bool = False\n",
    ") -> torch.Tensor | tuple[torch.Tensor, np.ndarray]:\n",
    "    # temporal_sample = video_transforms.temporalrandomcrop(num_frames * frame_interval)\n",
    "    temporal_sample = TemporalRandomCrop(num_frames * frame_interval)\n",
    "\n",
    "    total_frames = len(vframes)\n",
    "    start_frame_ind, end_frame_ind = temporal_sample(total_frames)\n",
    "\n",
    "    assert (\n",
    "        end_frame_ind - start_frame_ind >= num_frames\n",
    "    ), f\"Not enough frames to sample, {end_frame_ind} - {start_frame_ind} < {num_frames}\"\n",
    "\n",
    "    frame_indices = np.linspace(start_frame_ind, end_frame_ind - 1, num_frames, dtype=int)\n",
    "    video = vframes[frame_indices]\n",
    "    if return_frame_indices:\n",
    "        return video, frame_indices\n",
    "    else:\n",
    "        return video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94925240",
   "metadata": {},
   "source": [
    "### å‹•ç”»ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›ã™ã‚‹é–¢æ•°ã‚’ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8377620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_tensor_video_clip(clip):\n",
    "    \"\"\"\n",
    "    å‹•ç”»ã‚¯ãƒªãƒƒãƒ—ãŒ4Dãƒ†ãƒ³ã‚½ãƒ«ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹\n",
    "    (T, H, W, C)ã¾ãŸã¯(T, C, H, W)ã®å½¢å¼\n",
    "\n",
    "    Args:\n",
    "        clip: å‹•ç”»ã‚¯ãƒªãƒƒãƒ—\n",
    "    Returns:\n",
    "        bool: 4Dãƒ†ãƒ³ã‚½ãƒ«ã®å ´åˆã¯Trueã€ãã†ã§ãªã„å ´åˆã¯False\n",
    "    \"\"\"\n",
    "\n",
    "    # ãƒ†ãƒ³ã‚½ãƒ«ã§ãªã„å ´åˆã€ã‚¨ãƒ©ãƒ¼ã‚’ã‚¹ãƒ­ãƒ¼\n",
    "    if not torch.is_tensor(clip):\n",
    "        raise TypeError(\"clip should be Tensor. Got %s\" % type(clip))\n",
    "\n",
    "    # 4æ¬¡å…ƒãƒ†ãƒ³ã‚½ãƒ«ã§ãªã„å ´åˆã€ã‚¨ãƒ©ãƒ¼ã‚’ã‚¹ãƒ­ãƒ¼\n",
    "    if not clip.ndimension() == 4:\n",
    "        raise ValueError(\"clip should be 4D. Got %dD\" % clip.dim())\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9071d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(clip):\n",
    "    \"\"\"\n",
    "    uint8ãƒ†ãƒ³ã‚½ãƒ«ï¼ˆ0ã€œ255ã®ç¯„å›²ï¼‰ã‚’floatãƒ†ãƒ³ã‚½ãƒ«ï¼ˆ0.0ã€œ1.0ã®ç¯„å›²ï¼‰ã«å¤‰æ›ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        clip (torch.tensor, dtype=torch.uint8): ã‚µã‚¤ã‚ºã¯(T, C, H, W)\n",
    "    Returns:\n",
    "        clip (torch.tensor, dtype=torch.float): ã‚µã‚¤ã‚ºã¯(T, C, H, W)\n",
    "    \"\"\"\n",
    "\n",
    "    # å‹•ç”»ã‚¯ãƒªãƒƒãƒ—ãŒ4Dãƒ†ãƒ³ã‚½ãƒ«ã‹ã©ã†ã‹ã‚’åˆ¤å®š\n",
    "    _is_tensor_video_clip(clip)\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿å‹ãŒuint8ã§ãªã„å ´åˆã€ã‚¨ãƒ©ãƒ¼ã‚’ã‚¹ãƒ­ãƒ¼\n",
    "    if not clip.dtype == torch.uint8:\n",
    "        raise TypeError(\"clip tensor should have data type uint8. Got %s\" % str(clip.dtype))\n",
    "\n",
    "    # 255.0ã§å‰²ã‚‹\n",
    "    # ã“ã‚Œã«ã‚ˆã‚Šã€å€¤ãŒ0.0ã‹ã‚‰1.0ã®ç¯„å›²ã«æ­£è¦åŒ–ã•ã‚Œã‚‹\n",
    "    return clip.float() / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8923557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensorVideo:\n",
    "    \"\"\"\n",
    "    uint8ãƒ†ãƒ³ã‚½ãƒ«ï¼ˆ0ã€œ255ã®ç¯„å›²ï¼‰ã‚’floatãƒ†ãƒ³ã‚½ãƒ«ï¼ˆ0.0ã€œ1.0ã®ç¯„å›²ï¼‰ã«å¤‰æ›ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "    to_tensoré–¢æ•°ã®ãƒ©ãƒƒãƒ‘ãƒ¼\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            clip (torch.tensor, dtype=torch.uint8): ã‚µã‚¤ã‚ºãŒ(T, C, H, W)\n",
    "        Return:\n",
    "            clip (torch.tensor, dtype=torch.float): ã‚µã‚¤ã‚ºãŒ(T, C, H, W)\n",
    "        \"\"\"\n",
    "        return to_tensor(clip)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abebb08",
   "metadata": {},
   "source": [
    "### ãƒªã‚µã‚¤ã‚ºã¨ã‚¯ãƒ­ãƒƒãƒ—ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a274c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(clip, target_size, interpolation_mode):\n",
    "    \"\"\"\n",
    "    å‹•ç”»ã‚¯ãƒªãƒƒãƒ—ã‚’æŒ‡å®šã•ã‚ŒãŸã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚ºã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        clip (torch.tensor): ã‚µã‚¤ã‚ºãŒ(T, C, H, W)\n",
    "        target_size (tuple): ç›®æ¨™ã®ã‚µã‚¤ã‚º (height, width)\n",
    "        interpolation_mode (str): è£œé–“ãƒ¢ãƒ¼ãƒ‰ã€'nearest'ã€'bilinear'ã€'bicubic'ãªã©\n",
    "    Returns:\n",
    "        torch.tensor: ãƒªã‚µã‚¤ã‚ºã•ã‚ŒãŸå‹•ç”»ã‚¯ãƒªãƒƒãƒ—ã€ã‚µã‚¤ã‚ºã¯(T, C, target_height, target_width)\n",
    "    \"\"\"\n",
    "    log_.info(f\"resize {target_size=} {interpolation_mode=}\")\n",
    "\n",
    "    if len(target_size) != 2:\n",
    "        raise ValueError(f\"target size should be tuple (height, width), instead got {target_size}\")\n",
    "\n",
    "    # å‹•ç”»ã‚¯ãƒªãƒƒãƒ—ã‚’ãƒªã‚µã‚¤ã‚º\n",
    "    return torch.nn.functional.interpolate(clip, size=target_size, mode=interpolation_mode, align_corners=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906de606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(clip, i, j, h, w):\n",
    "    \"\"\"\n",
    "    å‹•ç”»ã‚¯ãƒªãƒƒãƒ—ã‚’æŒ‡å®šã•ã‚ŒãŸä½ç½®ã§ã‚¯ãƒ­ãƒƒãƒ—ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        clip (torch.tensor): å‹•ç”»ã‚¯ãƒªãƒƒãƒ—ã€ã‚µã‚¤ã‚ºã¯ (T, C, H, W)\n",
    "        i (int): ã‚¯ãƒ­ãƒƒãƒ—ã®å·¦ä¸Šéš…ã®é«˜ã•ã®ä½ç½®\n",
    "        j (int): ã‚¯ãƒ­ãƒƒãƒ—ã®å·¦ä¸Šéš…ã®å¹…ã®ä½ç½®\n",
    "        h (int): ã‚¯ãƒ­ãƒƒãƒ—ã®é«˜ã•\n",
    "        w (int): ã‚¯ãƒ­ãƒƒãƒ—ã®å¹…\n",
    "    Returns:\n",
    "        torch.tensor: ã‚¯ãƒ­ãƒƒãƒ—ã•ã‚ŒãŸå‹•ç”»ã‚¯ãƒªãƒƒãƒ—ã€ã‚µã‚¤ã‚ºã¯ (T, C, h, w)\n",
    "    \"\"\"\n",
    "    log_.info(f\"crop {i=} {j=} {h=} {w=}\")\n",
    "\n",
    "    # 4Dãƒ†ãƒ³ã‚½ãƒ«ã§ãªã„å ´åˆã€ã‚¨ãƒ©ãƒ¼ã‚’ã‚¹ãƒ­ãƒ¼\n",
    "    if len(clip.size()) != 4:\n",
    "        raise ValueError(\"clip should be a 4D tensor\")\n",
    "\n",
    "    # Hã¨Wã®æ¬¡å…ƒã«å¯¾ã—ã¦ã‚¯ãƒ­ãƒƒãƒ—ã‚’é©ç”¨\n",
    "    return clip[..., i : i + h, j : j + w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f342bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_crop_to_fill(clip, target_size):\n",
    "    \"\"\"\n",
    "    å‹•ç”»ã‚¯ãƒªãƒƒãƒ—ã‚’æŒ‡å®šã•ã‚ŒãŸã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚ºã—ã€ä¸­å¤®ã§ã‚¯ãƒ­ãƒƒãƒ—ã™ã‚‹\n",
    "    resizeé–¢æ•°ã¨cropé–¢æ•°ã®ãƒ©ãƒƒãƒ‘ãƒ¼\n",
    "\n",
    "    Args:\n",
    "        clip (torch.tensor): å‹•ç”»ã‚¯ãƒªãƒƒãƒ—ã€ã‚µã‚¤ã‚ºã¯ (T, C, H, W)\n",
    "        target_size (tuple): ç›®æ¨™ã®ã‚µã‚¤ã‚º (height, width)\n",
    "    Returns:\n",
    "        torch.tensor: ãƒªã‚µã‚¤ã‚ºãŠã‚ˆã³ã‚¯ãƒ­ãƒƒãƒ—ã•ã‚ŒãŸå‹•ç”»ã‚¯ãƒªãƒƒãƒ—ã€ã‚µã‚¤ã‚ºã¯ (T, C, target_height, target_width)\n",
    "    \"\"\"\n",
    "    log_.info(f\"resize_crop_to_fill {target_size=}\")\n",
    "\n",
    "    # 4Dãƒ†ãƒ³ã‚½ãƒ«ã§ãªã„å ´åˆã€ã‚¨ãƒ©ãƒ¼ã‚’ã‚¹ãƒ­ãƒ¼\n",
    "    if not _is_tensor_video_clip(clip):\n",
    "        raise ValueError(\"clip should be a 4D torch.tensor\")\n",
    "\n",
    "    # å…ƒã®é«˜ã•ã¨å¹…ã‚’å–å¾—\n",
    "    h, w = clip.size(-2), clip.size(-1)\n",
    "\n",
    "    # ç›®æ¨™ã®é«˜ã•ã¨å¹…ã‚’å–å¾—\n",
    "    th, tw = target_size[0], target_size[1]\n",
    "\n",
    "    # ãƒªã‚µã‚¤ã‚ºå¾Œã®æ¯”ç‡ã‚’è¨ˆç®—\n",
    "    rh, rw = th / h, tw / w\n",
    "\n",
    "    # é«˜ã•ã®æ¯”ç‡ãŒå¹…ã®æ¯”ç‡ã‚ˆã‚Šå¤§ãã„å ´åˆ\n",
    "    if rh > rw:\n",
    "        # ç›®æ¨™ã®é«˜ã•ã¯ç¶­æŒã—ã€å¹…ã‚’èª¿æ•´\n",
    "        sh, sw = th, round(w * rh)\n",
    "\n",
    "        # å‹•ç”»ã‚¯ãƒªãƒƒãƒ—ã‚’ãƒªã‚µã‚¤ã‚º\n",
    "        clip = resize(clip, (sh, sw), \"bilinear\")\n",
    "\n",
    "        # ã‚¯ãƒ­ãƒƒãƒ—ã®å·¦ä¸Šã®ä½ç½®ã‚’è¨ˆç®—\n",
    "        i = 0\n",
    "        j = int(round(sw - tw) / 2.0)\n",
    "\n",
    "    # å¹…ã®æ¯”ç‡ãŒé«˜ã•ã®æ¯”ç‡ã‚ˆã‚Šå¤§ãã„å ´åˆ\n",
    "    else:\n",
    "        # ç›®æ¨™ã®å¹…ã¯ç¶­æŒã—ã€é«˜ã•ã‚’èª¿æ•´\n",
    "        sh, sw = round(h * rw), tw\n",
    "\n",
    "        # å‹•ç”»ã‚¯ãƒªãƒƒãƒ—ã‚’ãƒªã‚µã‚¤ã‚º\n",
    "        clip = resize(clip, (sh, sw), \"bilinear\")\n",
    "\n",
    "        # ã‚¯ãƒ­ãƒƒãƒ—ã®å·¦ä¸Šã®ä½ç½®ã‚’è¨ˆç®—\n",
    "        i = int(round(sh - th) / 2.0)\n",
    "        j = 0\n",
    "\n",
    "    assert i + th <= clip.size(-2) and j + tw <= clip.size(-1)\n",
    "\n",
    "    # ã‚¯ãƒ­ãƒƒãƒ—ã‚’é©ç”¨ã—ã¦è¿”ã™\n",
    "    return crop(clip, i, j, th, tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65868e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeCrop:\n",
    "    \"\"\"\n",
    "    å‹•ç”»ã‚¯ãƒªãƒƒãƒ—ã‚’æŒ‡å®šã•ã‚ŒãŸã‚µã‚¤ã‚ºã«ãƒªã‚µã‚¤ã‚ºã—ã€ã‚¯ãƒ­ãƒƒãƒ—ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "    resize_crop_to_fillé–¢æ•°ã®ãƒ©ãƒƒãƒ‘ãƒ¼\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = size\n",
    "\n",
    "    def __call__(self, clip):\n",
    "        clip = resize_crop_to_fill(clip, self.size)\n",
    "        return clip\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(size={self.size})\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac14138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms_video(name=\"center\", image_size=(256, 256)):\n",
    "    \"\"\"\n",
    "    å‹•ç”»ã®å‰å‡¦ç†å¤‰æ›ç”¨ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å–å¾—ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        name (str): å¤‰æ›æ–¹æ³•\n",
    "            - center: ä¸­å¤®ã‚¯ãƒ­ãƒƒãƒ—\n",
    "            - resize_crop: ãƒªã‚µã‚¤ã‚ºã—ã¦ã‚¯ãƒ­ãƒƒãƒ—\n",
    "            - rand_size_crop: ãƒ©ãƒ³ãƒ€ãƒ ã‚µã‚¤ã‚ºã‚¯ãƒ­ãƒƒãƒ—\n",
    "        image_size (tuple): å¤‰æ›å¾Œã®ç”»åƒã‚µã‚¤ã‚º (height, width)\n",
    "    Returns:\n",
    "        transform_video (torchvision.transforms.Compose): å‹•ç”»ã®å‰å‡¦ç†å¤‰æ›ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    "    \"\"\"\n",
    "    log_.info(f\"get_transforms_video {name=} {image_size=}\")\n",
    "\n",
    "    # False\n",
    "    if name is None:\n",
    "        return None\n",
    "\n",
    "    # False\n",
    "    elif name == \"center\":\n",
    "        assert image_size[0] == image_size[1], \"image_size must be square for center crop\"\n",
    "        transform_video = transforms.Compose(\n",
    "            [\n",
    "                ToTensorVideo(),  # TCHW\n",
    "                UCFCenterCropVideo(image_size[0]),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # True\n",
    "    elif name == \"resize_crop\":\n",
    "\n",
    "        # Composeã§å¤‰æ›ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’é€£çµ\n",
    "        transform_video = transforms.Compose(\n",
    "            [\n",
    "                ToTensorVideo(),  # 0~255ã®å€¤ã‚’0~1ã®ç¯„å›²ã«æ­£è¦åŒ–\n",
    "                ResizeCrop(image_size), # ãƒªã‚µã‚¤ã‚ºã—ã¦ã‚¯ãƒ­ãƒƒãƒ—\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.5, 0.5, 0.5],\n",
    "                    std=[0.5, 0.5, 0.5],\n",
    "                    inplace=True\n",
    "                ), # å¹³å‡0.5ã€æ¨™æº–åå·®0.5ã§æ­£è¦åŒ–\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # False\n",
    "    elif name == \"rand_size_crop\":\n",
    "        transform_video = transforms.Compose(\n",
    "            [\n",
    "                ToTensorVideo(),  # TCHW\n",
    "                video_transforms.RandomSizedCrop(image_size),\n",
    "                transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Transform {name} not implemented\")\n",
    "    return transform_video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bc5a1f",
   "metadata": {},
   "source": [
    "### å‹•ç”»ã¨ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce94cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_img(path):\n",
    "    \"\"\"\n",
    "    ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‹ã©ã†ã‹ã‚’åˆ¤å®šã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        path (str): ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n",
    "    Returns:\n",
    "        bool: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯Trueã€ãã†ã§ãªã„å ´åˆã¯False\n",
    "    \"\"\"\n",
    "    # æ‹¡å¼µå­ã‚’å–å¾—\n",
    "    ext = os.path.splitext(path)[-1].lower()\n",
    "\n",
    "    # torchvision.datasets.forlder.IMG_EXTENSIONSã§æ¤œè¨¼\n",
    "    return ext in IMG_EXTENSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca40e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @DATASETS.register_module(\"video_text\")\n",
    "class VideoTextDataset(TextDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform_name: str = None,\n",
    "        bucket_class: str = \"Bucket\",\n",
    "        rand_sample_interval: int = None,  # random sample_interval value from [1, min(rand_sample_interval, video_allowed_max)]\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.transform_name = transform_name\n",
    "        self.bucket_class = bucket_class\n",
    "        self.rand_sample_interval = rand_sample_interval\n",
    "\n",
    "    def get_image(self, index: int, height: int, width: int) -> dict:\n",
    "        sample = self.data.iloc[index]\n",
    "        path = sample[\"path\"]\n",
    "        # loading\n",
    "        image = pil_loader(path)\n",
    "\n",
    "        # transform\n",
    "        transform = get_transforms_image(self.transform_name, (height, width))\n",
    "        image = transform(image)\n",
    "\n",
    "        # CHW -> CTHW\n",
    "        video = image.unsqueeze(1)\n",
    "\n",
    "        return {\"video\": video}\n",
    "\n",
    "    def get_video(self, index: int, num_frames: int, height: int, width: int, sampling_interval: int) -> dict:\n",
    "        \"\"\"\n",
    "        å‹•ç”»ã‚’å–å¾—ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            index (int): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "            num_frames (int): å–å¾—ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ æ•°\n",
    "            height (int): å‹•ç”»ã®é«˜ã•\n",
    "            width (int): å‹•ç”»ã®å¹…\n",
    "            sampling_interval (int): ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”\n",
    "        Returns:\n",
    "            dict: å‹•ç”»ã®ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€è¾æ›¸\n",
    "        \"\"\"\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¯¾å¿œã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—\n",
    "        sample = self.data.iloc[index]\n",
    "\n",
    "        # pathã‚«ãƒ©ãƒ ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—\n",
    "        path = sample[\"path\"]\n",
    "\n",
    "        # å‹•ç”»ã‚’èª­ã¿è¾¼ã‚€\n",
    "        vframes, vinfo = read_video(path, backend=\"av\")\n",
    "\n",
    "        # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "        if self.rand_sample_interval is not None:\n",
    "            # æœ€å¤§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”ã‚’è¨ˆç®—\n",
    "            video_allowed_max = min(len(vframes) // num_frames, self.rand_sample_interval)\n",
    "            # 1ã‹ã‚‰æœ€å¤§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”ã¾ã§ã®ãƒ©ãƒ³ãƒ€ãƒ ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”ã‚’é¸æŠï¼ˆintå‹ï¼‰\n",
    "            sampling_interval = random.randint(1, video_allowed_max)\n",
    "\n",
    "        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”ã«åŸºã¥ã„ã¦å‹•ç”»ã‚’æ™‚é–“çš„ã«ãƒ©ãƒ³ãƒ€ãƒ ã‚¯ãƒ­ãƒƒãƒ—\n",
    "        video = temporal_random_crop(vframes, num_frames, sampling_interval)\n",
    "\n",
    "        # vframesã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã¦ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‚’å›é¿\n",
    "        video = video.clone()\n",
    "        del vframes\n",
    "\n",
    "        # å‹•ç”»å¤‰æ›ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å–å¾—\n",
    "        transform = get_transforms_video(self.transform_name, (height, width))\n",
    "\n",
    "        # å‹•ç”»ã«å¤‰æ›ã‚’é©ç”¨\n",
    "        # 0~1ã®ç¯„å›²ã«å¤‰æ›ã•ã‚Œã€ãƒªã‚µã‚¤ã‚ºãŠã‚ˆã³ã‚¯ãƒ­ãƒƒãƒ—ã•ã‚Œã€å¹³å‡ã¨æ¨™æº–åå·®ã§æ­£è¦åŒ–ã•ã‚Œã‚‹\n",
    "        video = transform(video)\n",
    "\n",
    "        # æ¬¡å…ƒã‚’ä¸¦ã¹æ›¿ãˆ\n",
    "        # TCHW -> CTHW\n",
    "        video = video.permute(1, 0, 2, 3)\n",
    "\n",
    "        # å‡ºåŠ›ã‚’è¾æ›¸ã«æ ¼ç´\n",
    "        ret = {\"video\": video}\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def get_image_or_video(self, index: int, num_frames: int, height: int, width: int, sampling_interval: int) -> dict:\n",
    "        \"\"\"\n",
    "        ç”»åƒã¾ãŸã¯å‹•ç”»ã‚’å–å¾—ã™ã‚‹\n",
    "        get_videoã®ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°\n",
    "\n",
    "        Args:\n",
    "            index (int): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "            num_frames (int): å–å¾—ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ï¼ˆå‹•ç”»ã®å ´åˆï¼‰\n",
    "            height (int): ç”»åƒ/å‹•ç”»ã®é«˜ã•\n",
    "            width (int): ç”»åƒ/å‹•ç”»ã®å¹…\n",
    "            sampling_interval (int): ãƒ•ãƒ¬ãƒ¼ãƒ é–“ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”ï¼ˆå‹•ç”»ã®å ´åˆï¼‰\n",
    "        Returns:\n",
    "            dict: ç”»åƒã¾ãŸã¯å‹•ç”»ã®ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€è¾æ›¸\n",
    "        \"\"\"\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¯¾å¿œã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—\n",
    "        sample = self.data.iloc[index]\n",
    "\n",
    "        # pathã‚«ãƒ©ãƒ ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—\n",
    "        path = sample[\"path\"]\n",
    "\n",
    "        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚ã‚‹å ´åˆ\n",
    "        if is_img(path):\n",
    "\n",
    "            # ç”»åƒã‚’å–å¾—ã—ã¦è¿”ã™\n",
    "            return self.get_image(index, height, width)\n",
    "\n",
    "        # å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚ã‚‹å ´åˆã€å‹•ç”»ã‚’å–å¾—ã—ã¦è¿”ã™\n",
    "        return self.get_video(index, num_frames, height, width, sampling_interval)\n",
    "\n",
    "    def getitem(self, index: str) -> dict:\n",
    "        # a hack to pass in the (time, height, width) info from sampler\n",
    "        index, num_frames, height, width = [int(val) for val in index.split(\"-\")]\n",
    "        ret = dict()\n",
    "        ret.update(super().getitem(index))\n",
    "        try:\n",
    "            ret.update(self.get_image_or_video(index, num_frames, height, width, ret[\"sampling_interval\"]))\n",
    "        except Exception as e:\n",
    "            path = self.data.iloc[index][\"path\"]\n",
    "            print(f\"video {path}: {e}\")\n",
    "            return None\n",
    "        return ret\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.getitem(index)\n",
    "\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰\n",
    "dataset = VideoTextDataset(\n",
    "    type = \"video_text\",\n",
    "    transform_name = \"resize_crop\",\n",
    "    data_path = PEXELS_CSV_PATH,\n",
    "    fps_max = 24,\n",
    ")\n",
    "\n",
    "logger.info(\"Dataset contains %s samples.\", len(dataset))\n",
    "\n",
    "# æ¤œè¨¼\n",
    "sample = dataset.get_image_or_video(0, num_frames=16, height=256, width=256, sampling_interval=2)\n",
    "\n",
    "# (3, 16, 256, 256)\n",
    "sample[\"video\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660de462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8æšã®å‹•ç”»ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚°ãƒªãƒƒãƒ‰å½¢å¼ã§è¡¨ç¤º\n",
    "grid = torchvision.utils.make_grid(sample[\"video\"][:, ::2, :, :].permute(1, 0, 2, 3), nrow=8, padding=2)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea80876b",
   "metadata": {},
   "source": [
    "### ãƒã‚±ãƒ„ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203455c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‹•ç”»ã®ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ãƒªã‚¹ãƒˆ\n",
    "# width:height\n",
    "ASPECT_RATIO_LD_LIST = [\n",
    "    \"2.39:1\",  # cinemascope, 2.39\n",
    "    \"2:1\",  # rare, 2\n",
    "    \"16:9\",  # rare, 1.89\n",
    "    \"1.85:1\",  # american widescreen, 1.85\n",
    "    \"9:16\",  # popular, 1.78\n",
    "    \"5:8\",  # rare, 1.6\n",
    "    \"3:2\",  # rare, 1.5\n",
    "    \"4:3\",  # classic, 1.33\n",
    "    \"1:1\",  # square\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6d7177",
   "metadata": {},
   "outputs": [],
   "source": [
    "((256 * 256 * (16 / 9))**(1/2) // (16 * 16)) * 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df9739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_ratios_dict(total_pixels: int = 256 * 256, training: bool = True) -> dict[str, tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    æŒ‡å®šã•ã‚ŒãŸç·ãƒ”ã‚¯ã‚»ãƒ«æ•°ã«åŸºã¥ã„ã¦ã€äº‹å‰å®šç¾©ã•ã‚ŒãŸã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®è¾æ›¸ã‚’å–å¾—ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        total_pixels (int): ç›®æ¨™ã®ç·ãƒ”ã‚¯ã‚»ãƒ«æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯256 * 256ï¼‰\n",
    "        training (bool): ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã‹ã©ã†ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Trueï¼‰\n",
    "    Returns:\n",
    "        dict[str, tuple[int, int]]:\n",
    "            ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®è¾æ›¸ã€ã‚­ãƒ¼ã¯ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®æ–‡å­—åˆ—ã€å€¤ã¯(é«˜ã•, å¹…)ã®ã‚¿ãƒ—ãƒ«\n",
    "    \"\"\"\n",
    "    log_.info(f\"get_aspect_ratios_dict {total_pixels=} {training=}\")\n",
    "\n",
    "    # ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ç©ºé–“åœ§ç¸®ç‡ã‚’å–å¾—\n",
    "    # 16\n",
    "    D = int(os.environ.get(\"AE_SPATIAL_COMPRESSION\", 16))\n",
    "\n",
    "    # å‡ºåŠ›ç”¨ã®è¾æ›¸\n",
    "    aspect_ratios_dict = {}\n",
    "    aspect_ratios_vertical_dict = {}\n",
    "\n",
    "    # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®ãƒªã‚¹ãƒˆã§ãƒ«ãƒ¼ãƒ—\n",
    "    for ratio in ASPECT_RATIO_LD_LIST:\n",
    "\n",
    "        # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®æ–‡å­—åˆ—ã‚’åˆ†å‰²ã—ã¦æ•°å€¤ã«å¤‰æ›\n",
    "        # ä¾‹: \"16:9\" -> 16.0, 9.0\n",
    "        width_ratio, height_ratio = map(float, ratio.split(\":\"))\n",
    "\n",
    "        # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ç¶­æŒã—ã¤ã¤ã€é¢ç©ãŒtotal_pixelsã«è¿‘ã„å¹…ã¨é«˜ã•ã‚’è¨ˆç®—\n",
    "        # 16:9ã®å ´åˆã€width=336px, height=192px\n",
    "        width = int(math.sqrt(total_pixels * (width_ratio / height_ratio)) // D) * D\n",
    "        height = int((total_pixels / width) // D) * D\n",
    "\n",
    "        # è¨“ç·´ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€ã‚ˆã‚Šèª¤å·®ã®å°‘ãªã„è§£åƒåº¦ã‚’æ¡ç”¨ã™ã‚‹\n",
    "        if training:\n",
    "            # ç›®æ¨™ã®ç·ãƒ”ã‚¯ã‚»ãƒ«æ•°ã¨ã®èª¤å·®\n",
    "            diff = abs(height * width - total_pixels)\n",
    "\n",
    "            # ä¸Šä¸‹å·¦å³ã«16ãƒ”ã‚¯ã‚»ãƒ«ãšã¤èª¿æ•´ã—ãŸå€™è£œã‚’ç”Ÿæˆ\n",
    "            candidate = [\n",
    "                (height - D, width),\n",
    "                (height + D, width),\n",
    "                (height, width - D),\n",
    "                (height, width + D),\n",
    "            ]\n",
    "\n",
    "            # æœ€ã‚‚èª¤å·®ã®å°‘ãªã„å€™è£œã‚’æ¢ç´¢ã—ã¦æ¡ç”¨\n",
    "            for h, w in candidate:\n",
    "                if abs(h * w - total_pixels) < diff:\n",
    "                    height, width = h, w\n",
    "                    diff = abs(h * w - total_pixels)\n",
    "\n",
    "        # é‡è¤‡ã—ãªã„å ´åˆã€è¾æ›¸ã«è¿½åŠ \n",
    "        if (height, width) not in aspect_ratios_dict.values() or not training:\n",
    "            # è¾æ›¸ã«ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã¨å¯¾å¿œã™ã‚‹(é«˜ã•, å¹…)ã‚’è¿½åŠ \n",
    "            aspect_ratios_dict[ratio] = (height, width)\n",
    "\n",
    "            # ç¸¦å‘ãã®ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚‚è¨ˆç®—ã—ã¦è¾æ›¸ã«è¿½åŠ \n",
    "            vertial_ratios = \":\".join(ratio.split(\":\")[::-1])\n",
    "            aspect_ratios_vertical_dict[vertial_ratios] = (width, height)\n",
    "\n",
    "    # ç¸¦å‘ãã®ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ãƒ¡ã‚¤ãƒ³ã®è¾æ›¸ã«çµ±åˆ\n",
    "    aspect_ratios_dict.update(aspect_ratios_vertical_dict)\n",
    "\n",
    "    return aspect_ratios_dict\n",
    "\n",
    "# æ¤œè¨¼\n",
    "get_aspect_ratios_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba124ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_pexels_from_name(resolution: str) -> int:\n",
    "    \"\"\"\n",
    "    Pexelsã®å‹•ç”»è§£åƒåº¦ã®åå‰ã‹ã‚‰ç·ãƒ”ã‚¯ã‚»ãƒ«æ•°ã‚’å–å¾—ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        resolution (str): è§£åƒåº¦ã®åå‰ï¼ˆä¾‹: \"256px_ar1:1\")\n",
    "    Returns:\n",
    "        int: ç·ãƒ”ã‚¯ã‚»ãƒ«æ•°\n",
    "    \"\"\"\n",
    "\n",
    "    # è§£åƒåº¦ã®åå‰ã‹ã‚‰ãƒ”ã‚¯ã‚»ãƒ«æ•°ã®éƒ¨åˆ†ã‚’æŠ½å‡º\n",
    "    resolution = resolution.split(\"_\")[0]\n",
    "\n",
    "    # pxã§çµ‚ã‚ã‚‹å ´åˆ\n",
    "    # True\n",
    "    if resolution.endswith(\"px\"):\n",
    "\n",
    "        # ãƒ”ã‚¯ã‚»ãƒ«æ•°ã‚’æ•´æ•°ã«å¤‰æ›\n",
    "        size = int(resolution[:-2])\n",
    "\n",
    "        # ç·ãƒ”ã‚¯ã‚»ãƒ«æ•°ã‚’è¨ˆç®—\n",
    "        num_pexels = size * size\n",
    "\n",
    "    # pã§çµ‚ã‚ã‚‹å ´åˆ\n",
    "    # False\n",
    "    elif resolution.endswith(\"p\"):\n",
    "        size = int(resolution[:-1])\n",
    "        num_pexels = int(size * size / 9 * 16)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid resolution {resolution}\")\n",
    "    return num_pexels\n",
    "\n",
    "# æ¤œè¨¼\n",
    "get_num_pexels_from_name(\"256px\")  # 65536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7ff455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resolution_with_aspect_ratio(\n",
    "    resolution: str,\n",
    ") -> tuple[int, dict[str, tuple[int, int]]]:\n",
    "    \"\"\"\n",
    "    ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’è€ƒæ…®ã—ãŸè§£åƒåº¦ã‚’å–å¾—ã™ã‚‹\n",
    "    bucket_configã‹ã‚‰è§£åƒåº¦ã‚’æŠ½å‡ºã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "\n",
    "    Args:\n",
    "        resolution (str):\n",
    "            è§£åƒåº¦ã®åå‰\n",
    "            å½¢å¼ã¯\"name\"ã¾ãŸã¯\"{name}_{setting}\"\n",
    "            nameã¯\"256px\"ã¾ãŸã¯\"360p\"ã‚’ã‚µãƒãƒ¼ãƒˆ\n",
    "            settingã¯\"ar1:1\"ã¾ãŸã¯\"max\"ã‚’ã‚µãƒãƒ¼ãƒˆ\n",
    "    Returns:\n",
    "        tuple[int, dict[str, tuple[int, int]]]: ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’è€ƒæ…®ã—ãŸè§£åƒåº¦\n",
    "    \"\"\"\n",
    "    log_.info(f\"get_resolution_with_aspect_ratio {resolution=}\")\n",
    "\n",
    "    # 1) è§£åƒåº¦ã¨è¨­å®šã‚’åˆ†å‰²\n",
    "\n",
    "    # 256px_ar1:1 -> [\"256px\", \"ar1:1\"]\n",
    "    keys = resolution.split(\"_\")\n",
    "\n",
    "    if len(keys) == 1:\n",
    "        resolution = keys[0]\n",
    "        setting = \"\"\n",
    "    else:\n",
    "        resolution, setting = keys\n",
    "        assert setting == \"max\" or setting.startswith(\n",
    "            \"ar\"\n",
    "        ), f\"Invalid setting {setting}\"\n",
    "\n",
    "    # 2) ç·ãƒ”ã‚¯ã‚»ãƒ«æ•°ã‚’å–å¾—\n",
    "\n",
    "    # 256px -> 65536\n",
    "    num_pexels = get_num_pexels_from_name(resolution)\n",
    "\n",
    "    # 3) ç·ãƒ”ã‚¯ã‚»ãƒ«æ•°ã«å¯¾å¿œã™ã‚‹ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®è¾æ›¸ã‚’å–å¾—\n",
    "\n",
    "    # 65536 -> {\"16:9\": (192, 336), ...}\n",
    "    aspect_ratio_dict = get_aspect_ratios_dict(num_pexels)\n",
    "\n",
    "    # 4) è¨­å®šã«åŸºã¥ã„ã¦ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "\n",
    "    # False\n",
    "    if setting == \"max\":\n",
    "        aspect_ratio = max(\n",
    "            aspect_ratio_dict,\n",
    "            key=lambda x: aspect_ratio_dict[x][0] * aspect_ratio_dict[x][1],\n",
    "        )\n",
    "        aspect_ratio_dict = {aspect_ratio: aspect_ratio_dict[aspect_ratio]}\n",
    "\n",
    "    # ar1:1\n",
    "    # True\n",
    "    elif setting.startswith(\"ar\"):\n",
    "\n",
    "        # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’æŠ½å‡º\n",
    "        # 1:1\n",
    "        aspect_ratio = setting[2:]\n",
    "        assert (\n",
    "            aspect_ratio in aspect_ratio_dict\n",
    "        ), f\"Aspect ratio {aspect_ratio} not found\"\n",
    "\n",
    "        # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®è¾æ›¸ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "        aspect_ratio_dict = {aspect_ratio: aspect_ratio_dict[aspect_ratio]}\n",
    "\n",
    "    return num_pexels, aspect_ratio_dict\n",
    "\n",
    "get_resolution_with_aspect_ratio(\"256px_ar1:1\")  # (65536, {'16:9': (192, 336)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb2f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_target_fps(\n",
    "    fps: float,\n",
    "    max_fps: float,\n",
    ") -> tuple[float, int]:\n",
    "    \"\"\"\n",
    "    fpsãŒé«˜ã„å ´åˆã€æœ€å¤§fpsæœªæº€ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        fps (float): å…ƒã®fps\n",
    "        max_fps (float): æœ€å¤§fps\n",
    "    Returns:\n",
    "        tuple[float, int]: æ–°ã—ã„fpsã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”\n",
    "    \"\"\"\n",
    "    log_.info(f\"map_target_fps {fps=} {max_fps=}\")\n",
    "\n",
    "    # fpsãŒNaNã®å ´åˆã€0fpsãƒ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”1ã‚’è¿”ã™\n",
    "    if math.isnan(fps):\n",
    "        return 0, 1\n",
    "\n",
    "    # fpsãŒmax_fpsæœªæº€ã®å ´åˆã€ãã®ã¾ã¾è¿”ã™\n",
    "    if fps < max_fps:\n",
    "        return fps, 1\n",
    "\n",
    "    # fpsãŒmax_fpsä»¥ä¸Šã®å ´åˆã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”ã‚’è¨ˆç®—\n",
    "    sampling_interval = math.ceil(fps / max_fps)\n",
    "    new_fps = math.floor(fps / sampling_interval)\n",
    "\n",
    "    return new_fps, sampling_interval\n",
    "\n",
    "# æ¤œè¨¼\n",
    "# 60fpsã‚’24fpsæœªæº€ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹å ´åˆã€20fpsãƒ»ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”3ãŒå¾—ã‚‰ã‚Œã‚‹\n",
    "# (20.0, 3)\n",
    "map_target_fps(60.0, 24.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575d7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(name: str) -> float:\n",
    "    \"\"\"\n",
    "    ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®åå‰ã‹ã‚‰æ¯”ç‡ã‚’å–å¾—ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        name (str): ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®åå‰ï¼ˆä¾‹: \"16:9\"ï¼‰\n",
    "    Returns:\n",
    "        float: ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®æ¯”ç‡ï¼ˆé«˜ã•/å¹…ï¼‰\n",
    "    \"\"\"\n",
    "\n",
    "    # æ–‡å­—åˆ—ã‚’åˆ†å‰²ã—ã¦æ•°å€¤ã«å¤‰æ›\n",
    "    width, height = map(float, name.split(\":\"))\n",
    "\n",
    "    # æ¯”ç‡ã‚’è¨ˆç®—ã—ã¦è¿”ã™\n",
    "    return height / width\n",
    "\n",
    "# æ¤œè¨¼\n",
    "get_ratio(\"16:9\")  # 0.5625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538c19c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_ratio(height: float, width: float, ratios: dict) -> str:\n",
    "    \"\"\"\n",
    "    æŒ‡å®šã•ã‚ŒãŸé«˜ã•ã¨å¹…ã«æœ€ã‚‚è¿‘ã„ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®åå‰ã‚’å–å¾—ã™ã‚‹\n",
    "    Args:\n",
    "        height (float): é«˜ã•\n",
    "        width (float): å¹…\n",
    "        ratios (dict): ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®è¾æ›¸ã€ã‚­ãƒ¼ã¯ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®åå‰ã€å€¤ã¯(é«˜ã•, å¹…)ã®ã‚¿ãƒ—ãƒ«\n",
    "    Returns:\n",
    "        str: æœ€ã‚‚è¿‘ã„ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®åå‰\n",
    "    \"\"\"\n",
    "\n",
    "    # é«˜ã•ã¨å¹…ã‹ã‚‰ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’è¨ˆç®—\n",
    "    aspect_ratio = height / width\n",
    "\n",
    "    # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®è¾æ›¸ã‹ã‚‰æœ€ã‚‚è¿‘ã„ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®åå‰ã‚’å–å¾—\n",
    "    closest_ratio = min(\n",
    "        ratios.keys(), key=lambda ratio: abs(aspect_ratio - get_ratio(ratio))\n",
    "    )\n",
    "\n",
    "    return closest_ratio\n",
    "\n",
    "# æ¤œè¨¼\n",
    "# 192:336ã«æœ€ã‚‚è¿‘ã„ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã¯16:9\n",
    "get_closest_ratio(192, 336, get_aspect_ratios_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844b85c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.bucket_config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc0865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bucket:\n",
    "    \"\"\"\n",
    "    å‹•ç”»ã‚’ãƒã‚±ãƒƒãƒˆã«åˆ†é¡ã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "    è¨“ç·´ã®æœ€åˆã«å‹•ç”»ã‚’ãƒã‚±ãƒ„ã«åˆ†é¡ã—ã€ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒã‚±ãƒ„ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹\n",
    "    ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®å ´åˆã¯å˜ä¸€ã®ãƒã‚±ãƒ„\n",
    "    MMDiTã®å ´åˆã¯è¤‡æ•°ã®ãƒã‚±ãƒ„ãŒã‚ã‚Šã€è§£åƒåº¦ã”ã¨ã«æœ€é©ãªãƒãƒƒãƒã‚µã‚¤ã‚ºãŒæ±ºã‚ã‚‰ã‚Œã¦ã„ã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bucket_config: dict[str, dict[int, tuple[float, int] | tuple[tuple[float, float], int]]]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            bucket_config (dict): ãƒã‚±ãƒƒãƒˆæ§‹æˆã‚’å«ã‚€è¾æ›¸\n",
    "\n",
    "                è¾æ›¸ã®å½¢å¼ã¯ä»¥ä¸‹ã®é€šã‚Š:\n",
    "                {\n",
    "                    \"bucket_name\": {\n",
    "                        \"time\": (probability, batch_size),\n",
    "                        \"time\": (probability, batch_size),\n",
    "                        ...\n",
    "                    },\n",
    "                    ...\n",
    "                }\n",
    "\n",
    "                ã¾ãŸã¯ä»¥ä¸‹ã®å½¢å¼:\n",
    "\n",
    "                {\n",
    "                    \"bucket_name\": {\n",
    "                        \"time\": ((probability, next_probability), batch_size),\n",
    "                        \"time\": ((probability, next_probability), batch_size),\n",
    "                        ...\n",
    "                    },\n",
    "                    ...\n",
    "                }\n",
    "                bucket_nameã¯ãƒã‚±ãƒƒãƒˆã®åå‰ã€timeã¯å‹•ç”»ã®ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚’ç¤ºã™ã€‚\n",
    "                probabilityã¯0ã‹ã‚‰1ã®é–“ã®æµ®å‹•å°æ•°ç‚¹æ•°ã€batch_sizeã¯æ•´æ•°ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚\n",
    "                probabilityãŒã‚¿ãƒ—ãƒ«ã®å ´åˆã€2ç•ªç›®ã®å€¤ã¯æ¬¡ã®timeã«ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ç¢ºç‡ã‚’ç¤ºã™ã€‚\n",
    "        \"\"\"\n",
    "        log_.info(f\"Bucket.__init__ {bucket_config=}\")\n",
    "\n",
    "        # bucket_configã‹ã‚‰ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’å–å¾—\n",
    "        # \"256px_ar1:1\" -> {\"256px_ar1:1\": (65536, {\"16:9\": (192, 336)}), ...}\n",
    "        aspect_ratios = {key: get_resolution_with_aspect_ratio(key) for key in bucket_config.keys()}\n",
    "\n",
    "        # ãƒã‚±ãƒƒãƒˆã®ç¢ºç‡ã¨ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ ¼ç´ã™ã‚‹OrderedDictã§åˆæœŸåŒ–\n",
    "        bucket_probs = OrderedDict()\n",
    "        bucket_bs = OrderedDict()\n",
    "\n",
    "        # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã«åŸºã¥ã„ã¦ãƒã‚±ãƒƒãƒˆåã‚’ã‚½ãƒ¼ãƒˆï¼ˆè§£åƒåº¦ã®é«˜ã„é †ï¼‰\n",
    "        # [\"256px_ar16:9\", \"256px_ar4:3\", ...]\n",
    "        bucket_names = sorted(bucket_config.keys(), key=lambda x: aspect_ratios[x][0], reverse=True)\n",
    "\n",
    "        # ãƒã‚±ãƒƒãƒˆåã§ãƒ«ãƒ¼ãƒ—ã—ã¦ç¢ºç‡ã¨ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’è¨­å®š\n",
    "        for key in bucket_names:\n",
    "            log_.info(f\"{key=}, {aspect_ratios[key]=}\")\n",
    "\n",
    "            # ãƒã‚±ãƒƒãƒˆã®å‹•ç”»ã®ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚’ã‚½ãƒ¼ãƒˆï¼ˆãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã®é•·ã„é †ï¼‰\n",
    "            # [32, 8,...]\n",
    "            bucket_time_names = sorted(bucket_config[key].keys(), key=lambda x: x, reverse=True)\n",
    "\n",
    "            # ç¢ºç‡ã‚’OrderedDictã«æ ¼ç´\n",
    "            # {32: 0.5, 8: 0.5, ...}\n",
    "            bucket_probs[key] = OrderedDict({k: bucket_config[key][k][0] for k in bucket_time_names})\n",
    "\n",
    "            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’OrderedDictã«æ ¼ç´\n",
    "            # {32: 16, 8: 16, ...}\n",
    "            bucket_bs[key] = OrderedDict({k: bucket_config[key][k][1] for k in bucket_time_names})\n",
    "\n",
    "        # 2) ãƒã‚±ãƒƒãƒˆã®åŸºæº–ã‚’è¨­å®š\n",
    "\n",
    "        # è§£åƒåº¦ã®åŸºæº–\n",
    "        self.hw_criteria = {k: aspect_ratios[k][0] for k in bucket_names}\n",
    "\n",
    "        # ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã®åŸºæº–\n",
    "        self.t_criteria = {k1: {k2: k2 for k2 in bucket_config[k1].keys()} for k1 in bucket_names}\n",
    "\n",
    "        # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®åŸºæº–\n",
    "        self.ar_criteria = {\n",
    "            k1: {k2: {k3: v3 for k3, v3 in aspect_ratios[k1][1].items()} for k2 in bucket_config[k1].keys()}\n",
    "            for k1 in bucket_names\n",
    "        }\n",
    "\n",
    "        bucket_id_cnt = num_bucket = 0\n",
    "        bucket_id = dict()\n",
    "        for k1, v1 in bucket_probs.items():\n",
    "            bucket_id[k1] = dict()\n",
    "            for k2, _ in v1.items():\n",
    "                bucket_id[k1][k2] = bucket_id_cnt\n",
    "                bucket_id_cnt += 1\n",
    "                num_bucket += len(aspect_ratios[k1][1])\n",
    "\n",
    "        self.bucket_probs = bucket_probs\n",
    "        self.bucket_bs = bucket_bs\n",
    "        self.bucket_id = bucket_id\n",
    "        self.num_bucket = num_bucket\n",
    "\n",
    "        log_message(\"Number of buckets: %s\", num_bucket)\n",
    "\n",
    "    def get_bucket_id(self, T: int, H: int, W: int, fps: float, path: str | None = None, seed: int | None = None, fps_max: int = 16) -> tuple[str, int, int] | None:\n",
    "        \"\"\"\n",
    "        å‹•ç”»ã®ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã€é«˜ã•ã€å¹…ã€fpsã«åŸºã¥ã„ã¦é©åˆ‡ãªãƒã‚±ãƒƒãƒˆã«åˆ†é¡ã™ã‚‹\n",
    "\n",
    "        Args:\n",
    "            T (int): å‹•ç”»ã®ãƒ•ãƒ¬ãƒ¼ãƒ æ•°\n",
    "            H (int): å‹•ç”»ã®é«˜ã•\n",
    "            W (int): å‹•ç”»ã®å¹…\n",
    "            fps (float): å‹•ç”»ã®fps\n",
    "            path (str | None): å‹•ç”»ã®ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Noneï¼‰\n",
    "            seed (int | None): ä¹±æ•°ã‚·ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯Noneï¼‰\n",
    "            fps_max (int): æœ€å¤§fpsï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯16ï¼‰\n",
    "        Returns:\n",
    "            tuple[str, int, int] | None:\n",
    "                é©åˆ‡ãªãƒã‚±ãƒƒãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒã‚±ãƒƒãƒˆåã€ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã€ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®åå‰ï¼‰ã®ã‚¿ãƒ—ãƒ«\n",
    "                é©åˆ‡ãªãƒã‚±ãƒƒãƒˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯Noneã‚’è¿”ã™\n",
    "        \"\"\"\n",
    "        log_.info(f\"Bucket.get_bucket_id {T=} {H=} {W=} {fps=} {path=} {seed=} {fps_max=}\")\n",
    "    \n",
    "        # è¨±å®¹ç¯„å›²ï¼ˆ80%ä»¥ä¸Šã®è§£åƒåº¦ãŒã‚ã‚Œã°è‰¯ã„ï¼‰\n",
    "        approx = 0.8\n",
    "\n",
    "        # fpsã‚’max_fpsæœªæº€ã«ãƒãƒƒãƒ”ãƒ³ã‚°\n",
    "        _, sampling_interval = map_target_fps(fps, fps_max)\n",
    "\n",
    "        # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”ã«åŸºã¥ã„ã¦ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚’èª¿æ•´\n",
    "        T = T // sampling_interval\n",
    "\n",
    "        # å‹•ç”»ã®è§£åƒåº¦ã‚’è¨ˆç®—\n",
    "        resolution = H * W\n",
    "\n",
    "        # ä¹±æ•°ç”Ÿæˆå™¨ã‚’åˆæœŸåŒ–\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "        # ãƒã‚±ãƒƒãƒˆã®ç¢ºç‡, é«˜ã•ãƒ»å¹…ã®åŸºæº–, ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã®åŸºæº–ã‚’å–å¾—\n",
    "        bucket_probs = self.bucket_probs\n",
    "        hw_criteria = self.hw_criteria\n",
    "        ar_criteria = self.ar_criteria\n",
    "        log_.debug(f\"{bucket_probs=} {hw_criteria=} {ar_criteria=}\")\n",
    "\n",
    "        # ãƒã‚±ãƒƒãƒˆã‚’å¤§ãã„é †ã«ãƒ«ãƒ¼ãƒ—\n",
    "        for hw_id, t_criteria in bucket_probs.items():\n",
    "            log_.debug(f\"trying bucket {hw_id=} {t_criteria=}\")\n",
    "\n",
    "            # å‹•ç”»ã®è§£åƒåº¦ãŒãƒã‚±ãƒƒãƒˆã®è§£åƒåº¦åŸºæº–ã®80%æœªæº€ã®å ´åˆã€ã‚¹ã‚­ãƒƒãƒ—\n",
    "            if resolution < hw_criteria[hw_id] * approx:\n",
    "                continue\n",
    "\n",
    "            # ã‚µãƒ³ãƒ—ãƒ«ãŒç”»åƒã®å ´åˆ\n",
    "            # False\n",
    "            if T == 1:\n",
    "                if 1 in t_criteria:\n",
    "                    if rng.random() < t_criteria[1]:\n",
    "                        return hw_id, 1, get_closest_ratio(H, W, ar_criteria[hw_id][1])\n",
    "                continue\n",
    "\n",
    "            # å‹•ç”»ã®ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãŒé•·ã„é †ã«ãƒ«ãƒ¼ãƒ—\n",
    "            for t_id, prob in t_criteria.items():\n",
    "                log_.debug(f\"trying {t_id=} {prob=}\")\n",
    "                if T >= t_id and t_id != 1:\n",
    "                    # probãŒã‚¿ãƒ—ãƒ«ã®å ´åˆã€2ç•ªç›®ã®å€¤ã‚’æ¬¡ã®t_idã«ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹é–¾å€¤ã¨ã—ã¦ä½¿ç”¨\n",
    "                    if isinstance(prob, tuple):\n",
    "                        next_hw_prob, next_t_prob = prob\n",
    "                        if next_t_prob >= 1 or rng.random() <= next_t_prob:\n",
    "                            continue\n",
    "\n",
    "                    # probãŒã‚¿ãƒ—ãƒ«ã§ãªã„å ´åˆã€ãã®ã¾ã¾ä½¿ç”¨\n",
    "                    else:\n",
    "                        next_hw_prob = prob\n",
    "\n",
    "                    # ç¢ºç‡ãŒé–¾å€¤ã‚’è¶…ãˆãŸå ´åˆã€ãƒã‚±ãƒƒãƒˆã‚’è¿”ã™\n",
    "                    if next_hw_prob >= 1 or rng.random() <= next_hw_prob:\n",
    "\n",
    "                        # å‹•ç”»ã®ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã«æœ€ã‚‚è¿‘ã„ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã‚’å–å¾—\n",
    "                        ar_id = get_closest_ratio(H, W, ar_criteria[hw_id][t_id])\n",
    "                        return hw_id, t_id, ar_id\n",
    "\n",
    "                    # ç¢ºç‡ãŒé–¾å€¤ã‚’è¶…ãˆãªã‹ã£ãŸå ´åˆã€æ¬¡ã®ãƒã‚±ãƒƒãƒˆã‚’è©¦ã™\n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "        return None\n",
    "\n",
    "    def get_thw(self, bucket_idx: tuple[str, int, int]) -> tuple[int, int, int]:\n",
    "        assert len(bucket_idx) == 3\n",
    "        T = self.t_criteria[bucket_idx[0]][bucket_idx[1]]\n",
    "        H, W = self.ar_criteria[bucket_idx[0]][bucket_idx[1]][bucket_idx[2]]\n",
    "        return T, H, W\n",
    "\n",
    "    def get_prob(self, bucket_idx: tuple[str, int]) -> float:\n",
    "        return self.bucket_probs[bucket_idx[0]][bucket_idx[1]]\n",
    "\n",
    "    def get_batch_size(self, bucket_idx: tuple[str, int]) -> int:\n",
    "        return self.bucket_bs[bucket_idx[0]][bucket_idx[1]]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_bucket\n",
    "\n",
    "# æ¤œè¨¼\n",
    "\n",
    "# MMViTã®ã‚¹ãƒ†ãƒ¼ã‚¸2ã®ãƒã‚±ãƒƒãƒˆè¨­å®š\n",
    "# ãƒã‚±ãƒ„IDã¯ã€AEã®åœ§ç¸®ç‡ã«åˆã‚ã›ã¦4N+1ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ï¼ˆ+1ã¯ã€æœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰\n",
    "stage2_bucket_config = {\n",
    "    \"256px\": { 1: (1.0, 130), 5: (1.0, 14), 9: (1.0, 14), 13: (1.0, 14), 17: (1.0, 14), 21: (1.0, 14), 25: (1.0, 14), 29: (1.0, 14), 33: (1.0, 14), 37: (1.0, 10), 41: (1.0, 10), 45: (1.0, 10), 49: (1.0, 10), 53: (1.0, 10), 57: (1.0, 10), 61: (1.0, 10), 65: (1.0, 10), 73: (1.0, 7), 77: (1.0, 7), 81: (1.0, 7), 85: (1.0, 7), 89: (1.0, 7), 93: (1.0, 7), 97: (1.0, 7), 101: (1.0, 6), 105: (1.0, 6), 109: (1.0, 6), 113: (1.0, 6), 117: (1.0, 6), 121: (1.0, 6), 125: (1.0, 6), 129: (1.0, 6), },\n",
    "    \"768px\": { 1: (1.0, 38), 5: (1.0, 6), 9: (1.0, 6), 13: (1.0, 6), 17: (1.0, 6), 21: (1.0, 6), 25: (1.0, 6), 29: (1.0, 6), 33: (1.0, 6), 37: (1.0, 4), 41: (1.0, 4), 45: (1.0, 4), 49: (1.0, 4), 53: (1.0, 4), 57: (1.0, 4), 61: (1.0, 4), 65: (1.0, 4), 69: (1.0, 3), 73: (1.0, 3), 77: (1.0, 3), 81: (1.0, 3), 85: (1.0, 3), 89: (1.0, 3), 93: (1.0, 3), 97: (1.0, 3), 101: (1.0, 2), 105: (1.0, 2), 109: (1.0, 2), 113: (1.0, 2), 117: (1.0, 2), 121: (1.0, 2), 125: (1.0, 2), 129: (1.0, 2), },\n",
    "}\n",
    "\n",
    "bucket = Bucket(stage2_bucket_config)\n",
    "\n",
    "# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é–“éš”2ã€å®Ÿè³ªãƒ•ãƒ¬ãƒ¼ãƒ æ•°30ã€224*492*0.8ã¯768*768ã‚ˆã‚Šå°ã•ãã€256pxãƒã‚±ãƒ„ã«åˆ†é¡\n",
    "# T=30ã‚ˆã‚Šå°ã•ã„IDã§æœ€ã‚‚å¤§ãã„ã®ã¯29ã€å¾“ã£ã¦ãƒãƒƒãƒã‚µã‚¤ã‚º14ã§è¨“ç·´\n",
    "bucket.get_bucket_id(T=60, H=224, W=492, fps=30.0, fps_max=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb02409f",
   "metadata": {},
   "source": [
    "### ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã‚’ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78779c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandarallel to accelerate bucket processing\n",
    "# NOTE: pandarallel should only access local variables\n",
    "def apply(data, method=None, seed=None, num_bucket=None, fps_max=16):\n",
    "    \"\"\"\n",
    "    ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã§ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰bucket.get_bucket_idãƒ¡ã‚½ãƒƒãƒ‰ã®å¼•æ•°ã‚’ä½œæˆã™ã‚‹\n",
    "    ä¸¦åˆ—å‡¦ç†ã§ã‚·ãƒ¼ãƒ‰ã‚’å¤‰ãˆã‚‹ãŸã‚ã«ã€å‹•ç”»ã”ã¨ã«ç•°ãªã‚‹ã‚·ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹\n",
    "    \"\"\"\n",
    "    return method(\n",
    "        data[\"num_frames\"],\n",
    "        data[\"height\"],\n",
    "        data[\"width\"],\n",
    "        data[\"fps\"],\n",
    "        data[\"path\"],\n",
    "        seed + data[\"id\"] * num_bucket, \n",
    "        fps_max,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcc8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_object_across_devices(obj: Any, rank: int = 0):\n",
    "    \"\"\"\n",
    "    åˆ†æ•£ç’°å¢ƒã®ãƒ‡ãƒã‚¤ã‚¹é–“ã§ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’åŒæœŸã™ã‚‹\n",
    "    è¨ˆç®—ã—ãŸãƒã‚±ãƒ„IDã‚’å…¨ãƒ‡ãƒã‚¤ã‚¹ã§å…±æœ‰ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        obj (Any):\n",
    "            åŒæœŸã™ã‚‹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "            ãƒªã‚¹ãƒˆã€è¾æ›¸ã€ã‚«ã‚¹ã‚¿ãƒ ã‚¯ãƒ©ã‚¹ãªã©ã€ãƒ”ã‚¯ãƒ«å¯èƒ½ãªä»»æ„ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒä½¿ç”¨å¯èƒ½\n",
    "        rank (int): \n",
    "            ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®çŠ¶æ…‹ã‚’ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã®ãƒ©ãƒ³ã‚¯\n",
    "            ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0\n",
    "    Returns:\n",
    "        Any: åŒæœŸã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ\n",
    "    \"\"\"\n",
    "\n",
    "    # ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ãƒªã‚¹ãƒˆã«æ ¼ç´\n",
    "    object_list = [obj]\n",
    "\n",
    "    # ã‚½ãƒ¼ã‚¹ãƒ©ãƒ³ã‚¯ã‹ã‚‰ä»–ã®ã™ã¹ã¦ã®ãƒ©ãƒ³ã‚¯ã«ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒªã‚¹ãƒˆã‚’ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ\n",
    "    dist.broadcast_object_list(object_list, src=rank, device=\"cuda\")\n",
    "\n",
    "    # åŒæœŸã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—\n",
    "    obj = object_list[0]\n",
    "\n",
    "    return obj\n",
    "\n",
    "sync_object_across_devices(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c95e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_numel_str(numel: int) -> str:\n",
    "    \"\"\"\n",
    "    æ•°å€¤ã‚’äººé–“ãŒèª­ã¿ã‚„ã™ã„å½¢å¼ã®æ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹\n",
    "    ä¾‹ãˆã°ã€12345678 -> '11.77 M'ãªã©\n",
    "    ãƒã‚±ãƒ„ã®çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        numel (int): è¦ç´ æ•°\n",
    "    Returns:\n",
    "        str: ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸæ–‡å­—åˆ—\n",
    "    \"\"\"\n",
    "    B = 1024**3 # billion\n",
    "    M = 1024**2 # million\n",
    "    K = 1024 # thousand\n",
    "\n",
    "    if numel >= B:\n",
    "        return f\"{numel / B:.2f} B\"\n",
    "    elif numel >= M:\n",
    "        return f\"{numel / M:.2f} M\"\n",
    "    elif numel >= K:\n",
    "        return f\"{numel / K:.2f} K\"\n",
    "    else:\n",
    "        return f\"{numel}\"\n",
    "\n",
    "format_numel_str(12345678)  # '11.77 M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c85a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableVideoBatchSampler(DistributedSampler):\n",
    "    \"\"\"\n",
    "    å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãŸã‚ã®å¯å¤‰ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚µãƒ³ãƒ—ãƒ©ãƒ¼\n",
    "    ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "    ãƒã‚±ãƒ„ã«åŸºã¥ã„ã¦å‹•ç”»ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€å„ãƒã‚±ãƒ„ã«å¯¾ã—ã¦ç•°ãªã‚‹ãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹\n",
    "    PyTorchã®DistributedSamplerã‚’ç¶™æ‰¿ã—ã€å„ãƒ—ãƒ­ã‚»ã‚¹ãŒé‡è¤‡ãªãå‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ãã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: VideoTextDataset,\n",
    "        bucket_config: dict,\n",
    "        num_replicas: int | None = None,\n",
    "        rank: int | None = None,\n",
    "        shuffle: bool = True,\n",
    "        seed: int = 0,\n",
    "        drop_last: bool = False,\n",
    "        verbose: bool = False,\n",
    "        num_bucket_build_workers: int = 1,\n",
    "        num_groups: int = 1,\n",
    "    ) -> None:\n",
    "\n",
    "        # DistributedSamplerã®åˆæœŸåŒ–\n",
    "        super().__init__(\n",
    "            dataset=dataset, num_replicas=num_replicas, rank=rank, shuffle=shuffle, seed=seed, drop_last=drop_last\n",
    "        )\n",
    "        self.dataset = dataset\n",
    "        assert dataset.bucket_class == \"Bucket\", \"Only support Bucket class for now\"\n",
    "\n",
    "        # ãƒã‚±ãƒ„ã‚’åˆæœŸåŒ–\n",
    "        self.bucket = Bucket(bucket_config)\n",
    "        self.verbose = verbose\n",
    "        self.last_micro_batch_access_index = 0\n",
    "        self.num_bucket_build_workers = num_bucket_build_workers\n",
    "        self._cached_bucket_sample_dict = None\n",
    "        self._cached_num_total_batch = None\n",
    "        self.num_groups = num_groups\n",
    "\n",
    "        if dist.get_rank() == 0:\n",
    "            # pandarallelã®åˆæœŸåŒ–\n",
    "            # å¤§é‡ã®å‹•ç”»ã‚’ãƒã‚±ãƒ„ã«åˆ†é¡ã™ã‚‹å‡¦ç†ã‚’åˆ†æ•£ã™ã‚‹\n",
    "            pandarallel.initialize(\n",
    "                nb_workers=self.num_bucket_build_workers,\n",
    "                progress_bar=False,\n",
    "                verbose=0,\n",
    "                use_memory_fs=False,\n",
    "            )\n",
    "\n",
    "    def __iter__(self) -> Iterator[list[int]]:\n",
    "        \"\"\"\n",
    "        ãƒã‚±ãƒ„ã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ã‚¤ãƒ†ãƒ¬ãƒ¼ãƒˆã™ã‚‹\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã•ã‚ŒãŸãƒã‚±ãƒ„ã‚µãƒ³ãƒ—ãƒ«ã®è¾æ›¸ã‚’å–å¾—\n",
    "        bucket_sample_dict, _ = self.group_by_bucket()\n",
    "        log_.debug(f\"Bucket sample dict keys: {list(bucket_sample_dict.keys())}\")\n",
    "\n",
    "        self.clear_cache()\n",
    "\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(self.seed + self.epoch)\n",
    "        bucket_micro_batch_count = OrderedDict()\n",
    "        bucket_last_consumed = OrderedDict()\n",
    "\n",
    "        # 2) å„ãƒã‚±ãƒ„ã®ä¸­ã§ãƒãƒƒãƒã‚’ä½œã‚‹\n",
    "\n",
    "        for bucket_id, data_list in bucket_sample_dict.items():\n",
    "            log_.debug(f\"Processing bucket {bucket_id} with {len(data_list)} samples\")\n",
    "\n",
    "            # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å–å¾—\n",
    "            bs_per_gpu = self.bucket.get_batch_size(bucket_id)\n",
    "            remainder = len(data_list) % bs_per_gpu\n",
    "\n",
    "            if remainder > 0:\n",
    "                if not self.drop_last:\n",
    "                    # if there is remainder, we pad to make it divisible\n",
    "                    data_list += data_list[: bs_per_gpu - remainder]\n",
    "                else:\n",
    "                    # we just drop the remainder to make it divisible\n",
    "                    data_list = data_list[:-remainder]\n",
    "            bucket_sample_dict[bucket_id] = data_list\n",
    "\n",
    "            # ãƒã‚±ãƒ„å†…ã§ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "            if self.shuffle:\n",
    "                data_indices = torch.randperm(len(data_list), generator=g).tolist()\n",
    "                data_list = [data_list[i] for i in data_indices]\n",
    "                bucket_sample_dict[bucket_id] = data_list\n",
    "\n",
    "            # ãƒã‚±ãƒ„ã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã®æ•°ã‚’è¨ˆç®—\n",
    "            num_micro_batches = len(data_list) // bs_per_gpu\n",
    "            bucket_micro_batch_count[bucket_id] = num_micro_batches\n",
    "\n",
    "        # 3) ãƒã‚±ãƒ„ã®ã‚¢ã‚¯ã‚»ã‚¹é †åºã‚’æ±ºã‚ã‚‹ãƒªã‚¹ãƒˆã‚’ä½œã‚‹\n",
    "        # ä¾‹: [bucket_id_1, bucket_id_1, bucket_id_2, bucket_id_1, ...]\n",
    "\n",
    "        bucket_id_access_order = []\n",
    "\n",
    "        for bucket_id, num_micro_batch in bucket_micro_batch_count.items():\n",
    "            bucket_id_access_order.extend([bucket_id] * num_micro_batch)\n",
    "\n",
    "        # 4) ãƒã‚±ãƒ„ã®é †ç•ªã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "\n",
    "        if self.shuffle:\n",
    "            bucket_id_access_order_indices = torch.randperm(len(bucket_id_access_order), generator=g).tolist()\n",
    "            bucket_id_access_order = [bucket_id_access_order[i] for i in bucket_id_access_order_indices]\n",
    "\n",
    "        # make the number of bucket accesses divisible by dp size\n",
    "        remainder = len(bucket_id_access_order) % self.num_replicas\n",
    "        if remainder > 0:\n",
    "            if self.drop_last:\n",
    "                bucket_id_access_order = bucket_id_access_order[: len(bucket_id_access_order) - remainder]\n",
    "            else:\n",
    "                bucket_id_access_order += bucket_id_access_order[: self.num_replicas - remainder]\n",
    "\n",
    "        # 5) åˆ†æ•£ç’°å¢ƒã§ã€ãƒã‚±ãƒ„ã‹ã‚‰ãƒãƒƒãƒã‚’æº–å‚™\n",
    "\n",
    "        num_iters = len(bucket_id_access_order) // self.num_replicas\n",
    "        start_iter_idx = self.last_micro_batch_access_index // self.num_replicas\n",
    "\n",
    "        # re-compute the micro-batch consumption\n",
    "        # this is useful when resuming from a state dict with a different number of GPUs\n",
    "        self.last_micro_batch_access_index = start_iter_idx * self.num_replicas\n",
    "        for i in range(self.last_micro_batch_access_index):\n",
    "            bucket_id = bucket_id_access_order[i]\n",
    "            bucket_bs = self.bucket.get_batch_size(bucket_id)\n",
    "            if bucket_id in bucket_last_consumed:\n",
    "                bucket_last_consumed[bucket_id] += bucket_bs\n",
    "            else:\n",
    "                bucket_last_consumed[bucket_id] = bucket_bs\n",
    "\n",
    "        # 6) ãƒãƒƒãƒã‚’ä¸€ã¤å–ã‚Šå‡ºã—ã¦yieldã™ã‚‹\n",
    "\n",
    "        for i in range(start_iter_idx, num_iters):\n",
    "\n",
    "            log_.debug(f\"Sampling micro-batch {i+1}/{num_iters}\")\n",
    "\n",
    "            # \n",
    "            bucket_access_list = bucket_id_access_order[i * self.num_replicas : (i + 1) * self.num_replicas]\n",
    "            self.last_micro_batch_access_index += self.num_replicas\n",
    "\n",
    "            # compute the data samples consumed by each access\n",
    "            bucket_access_boundaries = []\n",
    "            for bucket_id in bucket_access_list:\n",
    "                bucket_bs = self.bucket.get_batch_size(bucket_id)\n",
    "                last_consumed_index = bucket_last_consumed.get(bucket_id, 0)\n",
    "                bucket_access_boundaries.append([last_consumed_index, last_consumed_index + bucket_bs])\n",
    "\n",
    "                # update consumption\n",
    "                if bucket_id in bucket_last_consumed:\n",
    "                    bucket_last_consumed[bucket_id] += bucket_bs\n",
    "                else:\n",
    "                    bucket_last_consumed[bucket_id] = bucket_bs\n",
    "\n",
    "            # GPUã”ã¨ã«ã‚¢ã‚¯ã‚»ã‚¹ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²ã‚’è¨ˆç®—\n",
    "            bucket_id = bucket_access_list[self.rank]\n",
    "            boundary = bucket_access_boundaries[self.rank]\n",
    "            cur_micro_batch = bucket_sample_dict[bucket_id][boundary[0] : boundary[1]]\n",
    "\n",
    "            # ã‚µãƒ³ãƒ—ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«t, h, wã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "            real_t, real_h, real_w = self.bucket.get_thw(bucket_id)\n",
    "\n",
    "            # idx-t-h-wã®å½¢å¼ã«å¤‰æ›\n",
    "            cur_micro_batch = [f\"{idx}-{real_t}-{real_h}-{real_w}\" for idx in cur_micro_batch]\n",
    "\n",
    "            # ç¾åœ¨ã®ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã‚’è¿”ã™\n",
    "            yield cur_micro_batch\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.get_num_batch() // self.num_groups\n",
    "\n",
    "    def get_num_batch(self) -> int:\n",
    "        _, num_total_batch = self.group_by_bucket()\n",
    "        return num_total_batch\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self._cached_bucket_sample_dict = None\n",
    "        self._cached_num_total_batch = 0\n",
    "\n",
    "    def group_by_bucket(self) -> dict:\n",
    "        \"\"\"\n",
    "        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒã‚±ãƒ„ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã™ã‚‹\n",
    "        __iter__ãƒ¡ã‚½ãƒƒãƒ‰ã®æœ€åˆã«å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "        Returns:\n",
    "            dict: ãƒã‚±ãƒ„IDã‚’ã‚­ãƒ¼ã€ã‚µãƒ³ãƒ—ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆã‚’å€¤ã¨ã™ã‚‹è¾æ›¸\n",
    "        \"\"\"\n",
    "\n",
    "        # ã™ã§ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¦ã„ã‚‹å ´åˆã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’è¿”ã™\n",
    "        if self._cached_bucket_sample_dict is not None:\n",
    "            return self._cached_bucket_sample_dict, self._cached_num_total_batch\n",
    "\n",
    "        log_message(\"Building buckets using %d workers...\", self.num_bucket_build_workers)\n",
    "\n",
    "        # ãƒã‚±ãƒ„IDã‚’è¨ˆç®—\n",
    "        bucket_ids = None\n",
    "        if dist.get_rank() == 0:\n",
    "            data = self.dataset.data.copy(deep=True)\n",
    "            data[\"id\"] = data.index\n",
    "            bucket_ids = data.parallel_apply(\n",
    "                apply,\n",
    "                axis=1,\n",
    "                method=self.bucket.get_bucket_id,\n",
    "                seed=self.seed + self.epoch,\n",
    "                num_bucket=self.bucket.num_bucket,\n",
    "                fps_max=self.dataset.fps_max,\n",
    "            )\n",
    "\n",
    "        # ã™ã¹ã¦ã®ãƒ‡ãƒã‚¤ã‚¹ã§ãƒã‚±ãƒ„IDã‚’åŒæœŸ\n",
    "        dist.barrier()\n",
    "        bucket_ids = sync_object_across_devices(bucket_ids)\n",
    "        log_.debug(f\"{bucket_ids=}\")\n",
    "        dist.barrier()\n",
    "\n",
    "        # ãƒã‚±ãƒ„ã”ã¨ã«å‹•ç”»ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆã‚’æ ¼ç´\n",
    "        bucket_sample_dict = defaultdict(list)\n",
    "        bucket_ids_np = np.array(bucket_ids)\n",
    "        valid_indices = np.where(bucket_ids_np != None)[0]\n",
    "        log_.debug(f\"{valid_indices=}\")\n",
    "\n",
    "        for i in valid_indices:\n",
    "            bucket_sample_dict[bucket_ids_np[i]].append(i)\n",
    "\n",
    "        # ãƒã‚±ãƒ„ã‚µãƒ³ãƒ—ãƒ«è¾æ›¸ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥\n",
    "        self._cached_bucket_sample_dict = bucket_sample_dict\n",
    "\n",
    "        # ã™ã¹ã¦ã®ãƒã‚±ãƒ„ã®ãƒãƒƒãƒæ•°ã‚’è¨ˆç®—\n",
    "        num_total_batch = self.print_bucket_info(bucket_sample_dict)\n",
    "        self._cached_num_total_batch = num_total_batch\n",
    "\n",
    "        return bucket_sample_dict, num_total_batch\n",
    "\n",
    "    def print_bucket_info(self, bucket_sample_dict: dict) -> int:\n",
    "        \"\"\"\n",
    "        ãƒã‚±ãƒ„ã®çµ±è¨ˆæƒ…å ±ã‚’åé›†ã—ã¦ãƒ­ã‚°ã«è¨˜éŒ²ã™ã‚‹\n",
    "        group_by_bucketãƒ¡ã‚½ãƒƒãƒ‰ã§å‘¼ã³å‡ºã•ã‚Œã‚‹\n",
    "\n",
    "        Args:\n",
    "            bucket_sample_dict (dict):\n",
    "                ãƒã‚±ãƒ„IDã‚’ã‚­ãƒ¼ã€ã‚µãƒ³ãƒ—ãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒªã‚¹ãƒˆã‚’å€¤ã¨ã™ã‚‹è¾æ›¸\n",
    "        Returns:\n",
    "            int: ç·ãƒãƒƒãƒæ•°\n",
    "        \"\"\"\n",
    "        # collect statistics\n",
    "        num_total_samples = num_total_batch = 0\n",
    "        num_total_img_samples = num_total_vid_samples = 0\n",
    "        num_total_img_batch = num_total_vid_batch = 0\n",
    "        num_total_vid_batch_256 = num_total_vid_batch_768 = 0\n",
    "        num_aspect_dict = defaultdict(lambda: [0, 0])\n",
    "        num_hwt_dict = defaultdict(lambda: [0, 0])\n",
    "        for k, v in bucket_sample_dict.items():\n",
    "            size = len(v)\n",
    "            num_batch = size // self.bucket.get_batch_size(k[:-1])\n",
    "\n",
    "            num_total_samples += size\n",
    "            num_total_batch += num_batch\n",
    "\n",
    "            if k[1] == 1:\n",
    "                num_total_img_samples += size\n",
    "                num_total_img_batch += num_batch\n",
    "            else:\n",
    "                if k[0] == \"256px\":\n",
    "                    num_total_vid_batch_256 += num_batch\n",
    "                elif k[0] == \"768px\":\n",
    "                    num_total_vid_batch_768 += num_batch\n",
    "                num_total_vid_samples += size\n",
    "                num_total_vid_batch += num_batch\n",
    "\n",
    "            num_aspect_dict[k[-1]][0] += size\n",
    "            num_aspect_dict[k[-1]][1] += num_batch\n",
    "            num_hwt_dict[k[:-1]][0] += size\n",
    "            num_hwt_dict[k[:-1]][1] += num_batch\n",
    "\n",
    "        # sort\n",
    "        num_aspect_dict = dict(sorted(num_aspect_dict.items(), key=lambda x: x[0]))\n",
    "        num_hwt_dict = dict(\n",
    "            sorted(num_hwt_dict.items(), key=lambda x: (get_num_pexels_from_name(x[0][0]), x[0][1]), reverse=True)\n",
    "        )\n",
    "        num_hwt_img_dict = {k: v for k, v in num_hwt_dict.items() if k[1] == 1}\n",
    "        num_hwt_vid_dict = {k: v for k, v in num_hwt_dict.items() if k[1] > 1}\n",
    "\n",
    "        # log\n",
    "        if dist.get_rank() == 0 and self.verbose:\n",
    "            log_message(\"Bucket Info:\")\n",
    "            log_message(\"Bucket [#sample, #batch] by aspect ratio:\")\n",
    "            for k, v in num_aspect_dict.items():\n",
    "                log_message(\"(%s): #sample: %s, #batch: %s\", k, format_numel_str(v[0]), format_numel_str(v[1]))\n",
    "            log_message(\"===== Image Info =====\")\n",
    "            log_message(\"Image Bucket by HxWxT:\")\n",
    "            for k, v in num_hwt_img_dict.items():\n",
    "                log_message(\"%s: #sample: %s, #batch: %s\", k, format_numel_str(v[0]), format_numel_str(v[1]))\n",
    "            log_message(\"--------------------------------\")\n",
    "            log_message(\n",
    "                \"#image sample: %s, #image batch: %s\",\n",
    "                format_numel_str(num_total_img_samples),\n",
    "                format_numel_str(num_total_img_batch),\n",
    "            )\n",
    "            log_message(\"===== Video Info =====\")\n",
    "            log_message(\"Video Bucket by HxWxT:\")\n",
    "            for k, v in num_hwt_vid_dict.items():\n",
    "                log_message(\"%s: #sample: %s, #batch: %s\", k, format_numel_str(v[0]), format_numel_str(v[1]))\n",
    "            log_message(\"--------------------------------\")\n",
    "            log_message(\n",
    "                \"#video sample: %s, #video batch: %s\",\n",
    "                format_numel_str(num_total_vid_samples),\n",
    "                format_numel_str(num_total_vid_batch),\n",
    "            )\n",
    "            log_message(\"===== Summary =====\")\n",
    "            log_message(\"#non-empty buckets: %s\", len(bucket_sample_dict))\n",
    "            log_message(\n",
    "                \"Img/Vid sample ratio: %.2f\",\n",
    "                num_total_img_samples / num_total_vid_samples if num_total_vid_samples > 0 else 0,\n",
    "            )\n",
    "            log_message(\n",
    "                \"Img/Vid batch ratio: %.2f\", num_total_img_batch / num_total_vid_batch if num_total_vid_batch > 0 else 0\n",
    "            )\n",
    "            log_message(\n",
    "                \"vid batch 256: %s, vid batch 768: %s\", format_numel_str(num_total_vid_batch_256), format_numel_str(num_total_vid_batch_768)\n",
    "            )\n",
    "            log_message(\n",
    "                \"Vid batch ratio (256px/768px): %.2f\", num_total_vid_batch_256 / num_total_vid_batch_768 if num_total_vid_batch_768 > 0 else 0\n",
    "            )\n",
    "            log_message(\n",
    "                \"#training sample: %s, #training batch: %s\",\n",
    "                format_numel_str(num_total_samples),\n",
    "                format_numel_str(num_total_batch),\n",
    "            )\n",
    "        return num_total_batch\n",
    "\n",
    "    def reset(self):\n",
    "        self.last_micro_batch_access_index = 0\n",
    "\n",
    "    def set_step(self, start_step: int):\n",
    "        self.last_micro_batch_access_index = start_step * self.num_replicas\n",
    "\n",
    "    def state_dict(self, num_steps: int) -> dict:\n",
    "        # the last_micro_batch_access_index in the __iter__ is often\n",
    "        # not accurate during multi-workers and data prefetching\n",
    "        # thus, we need the user to pass the actual steps which have been executed\n",
    "        # to calculate the correct last_micro_batch_access_index\n",
    "        return {\"seed\": self.seed, \"epoch\": self.epoch, \"last_micro_batch_access_index\": num_steps * self.num_replicas}\n",
    "\n",
    "    def load_state_dict(self, state_dict: dict) -> None:\n",
    "        self.__dict__.update(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ae84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_default(batch):\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ•´å½¢é–¢æ•°\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) ãƒãƒƒãƒã‹ã‚‰Noneã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "    batch = [x for x in batch if x is not None]\n",
    "\n",
    "    assert len(batch) > 0, \"batch is empty\"\n",
    "\n",
    "    # 2) ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã‚’ãƒãƒƒãƒã‹ã‚‰åˆ†é›¢\n",
    "\n",
    "    use_mask = False\n",
    "    if \"mask\" in batch[0] and isinstance(batch[0][\"mask\"], int):\n",
    "\n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¹ã‚¯ã‚’ãƒãƒƒãƒã‹ã‚‰åˆ†é›¢\n",
    "        masks = [x.pop(\"mask\") for x in batch]\n",
    "\n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒƒãƒã‹ã‚‰åˆ†é›¢\n",
    "        texts = [x.pop(\"text\") for x in batch]\n",
    "\n",
    "        # ãƒãƒƒãƒæ¬¡å…ƒã§æ‰‹å‹•ã§é€£çµ\n",
    "        texts = torch.cat(texts, dim=1)\n",
    "        use_mask = True\n",
    "\n",
    "    # 3) ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ã‚’é™¤ã„ãŸãƒãƒƒãƒã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®collateé–¢æ•°ã§æ•´å½¢\n",
    "\n",
    "    # ãƒãƒƒãƒæ¬¡å…ƒã‚’æŒã¤ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›\n",
    "    ret = torch.utils.data.default_collate(batch)\n",
    "\n",
    "    # 4) ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æˆ»ã™\n",
    "\n",
    "    if use_mask:\n",
    "        ret[\"mask\"] = masks\n",
    "        ret[\"text\"] = texts\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a3e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seed_worker(seed):\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã®å†ç¾æ€§ã‚’ç¢ºä¿ã™ã‚‹ã‚·ãƒ¼ãƒ‰ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’å–å¾—ã™ã‚‹é–¢æ•°\n",
    "    åˆ†æ•£ç’°å¢ƒã§è¨“ç·´ã®å†ç¾æ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã™ã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    # ã‚·ãƒ¼ãƒ‰ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°ã‚’ã‚·ãƒ¼ãƒ‰å€¤ã§åˆæœŸåŒ–ã—ã¦è¿”ã™\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = seed\n",
    "        if seed is not None:\n",
    "            np.random.seed(worker_seed)\n",
    "            torch.manual_seed(worker_seed)\n",
    "            random.seed(worker_seed)\n",
    "\n",
    "    return seed_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21b113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(dataset, batch_size=None, shuffle=False, seed=1024, drop_last=False, pin_memory=False, num_workers=0, process_group: ProcessGroup | None = None, bucket_config=None, num_bucket_build_workers=1, prefetch_factor=None, cache_pin_memory=False, num_groups=1, **kwargs):\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã‚’æº–å‚™ã™ã‚‹é–¢æ•°\n",
    "\n",
    "    Args:\n",
    "        dataset (VideoTextDataset | TextDataset):\n",
    "            ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã‚’æº–å‚™ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "        batch_size (int, optional):\n",
    "            ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã®ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "            å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯ã€ãƒã‚±ãƒ„ã«åŸºã¥ã„ã¦å¯å¤‰ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒä½¿ç”¨ã•ã‚Œã‚‹ãŸã‚ã€ç„¡è¦–ã•ã‚Œã‚‹\n",
    "            ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯None\n",
    "        shuffle (bool, optional):\n",
    "            ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯False\n",
    "        seed (int, optional):\n",
    "            å†ç¾æ€§ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰å€¤\n",
    "            ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1024\n",
    "        drop_last (bool, optional):\n",
    "            ãƒãƒƒãƒãŒä¸å®Œå…¨ãªå ´åˆã«æœ€å¾Œã®ãƒãƒƒãƒã‚’ãƒ‰ãƒ­ãƒƒãƒ—ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯False\n",
    "        pin_memory (bool, optional):\n",
    "            ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãŒãƒ”ãƒ³ãƒ¡ãƒ¢ãƒªã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯False\n",
    "        num_workers (int, optional):\n",
    "            ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹æ•°\n",
    "            ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯0\n",
    "        process_group (ProcessGroup | None, optional):\n",
    "            åˆ†æ•£è¨“ç·´ã®ãŸã‚ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—\n",
    "            Noneã®å ´åˆã€åˆ†æ•£ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯ä½¿ç”¨ã•ã‚Œãªã„\n",
    "            ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯None\n",
    "        bucket_config (dict, optional):\n",
    "            å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã®ãƒã‚±ãƒ„è¨­å®š\n",
    "            å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®ã¿ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "            ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯None\n",
    "        num_bucket_build_workers (int, optional):\n",
    "            ãƒã‚±ãƒ„æ§‹ç¯‰ã®ãŸã‚ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹æ•°\n",
    "            ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1\n",
    "        prefetch_factor (int, optional):\n",
    "            å„ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒäº‹å‰ã«ãƒ•ã‚§ãƒƒãƒã™ã‚‹ãƒãƒƒãƒæ•°\n",
    "            ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯None\n",
    "        cache_pin_memory (bool, optional):\n",
    "            å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ”ãƒ³ãƒ¡ãƒ¢ãƒªã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯False\n",
    "        num_groups (int, optional):\n",
    "            å¯å¤‰ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã®ã‚°ãƒ«ãƒ¼ãƒ—æ•°\n",
    "            ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯1\n",
    "        **kwargs:\n",
    "            DataLoaderã«æ¸¡ã•ã‚Œã‚‹è¿½åŠ ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°\n",
    "    Returns:\n",
    "        tuple:\n",
    "            DataLoaderã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¿ãƒ—ãƒ«  \n",
    "    \"\"\"\n",
    "    log_.info(f\"prepare_dataloader: {dataset=} {batch_size=} {shuffle=} {seed=} {drop_last=} {pin_memory=} {num_workers=} {process_group=} {bucket_config=} {num_bucket_build_workers=} {prefetch_factor=} {cache_pin_memory=} {num_groups=}\")\n",
    "\n",
    "    _kwargs = kwargs.copy()\n",
    "\n",
    "    # True\n",
    "    if isinstance(dataset, VideoTextDataset):\n",
    "\n",
    "        # å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã®å¯å¤‰ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã‚’ä½œæˆ\n",
    "        batch_sampler = VariableVideoBatchSampler(\n",
    "            dataset,\n",
    "            bucket_config,\n",
    "            num_replicas=process_group.size(),\n",
    "            rank=process_group.rank(),\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            drop_last=drop_last,\n",
    "            verbose=True,\n",
    "            num_bucket_build_workers=num_bucket_build_workers,\n",
    "            num_groups=num_groups,\n",
    "        )\n",
    "\n",
    "        # DataLoaderã‚¯ãƒ©ã‚¹ã‚’é¸æŠ\n",
    "        dl_cls = DataloaderForVideo if cache_pin_memory else DataLoader\n",
    "\n",
    "        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã¨ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã‚’è¿”ã™\n",
    "        return (\n",
    "            dl_cls(\n",
    "                dataset,\n",
    "                batch_sampler=batch_sampler,\n",
    "                worker_init_fn=get_seed_worker(seed),\n",
    "                pin_memory=pin_memory,\n",
    "                num_workers=num_workers,\n",
    "                collate_fn=collate_fn_default,\n",
    "                prefetch_factor=prefetch_factor,\n",
    "                **_kwargs,\n",
    "            ),\n",
    "            batch_sampler,\n",
    "        )\n",
    "    elif isinstance(dataset, TextDataset):\n",
    "        if process_group is None:\n",
    "            return (\n",
    "                DataLoader(\n",
    "                    dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=shuffle,\n",
    "                    worker_init_fn=get_seed_worker(seed),\n",
    "                    drop_last=drop_last,\n",
    "                    pin_memory=pin_memory,\n",
    "                    num_workers=num_workers,\n",
    "                    prefetch_factor=prefetch_factor,\n",
    "                    **_kwargs,\n",
    "                ),\n",
    "                None,\n",
    "            )\n",
    "        else:\n",
    "            sampler = DistributedSampler(\n",
    "                dataset,\n",
    "                num_replicas=process_group.size(),\n",
    "                rank=process_group.rank(),\n",
    "                shuffle=shuffle,\n",
    "                seed=seed,\n",
    "                drop_last=drop_last,\n",
    "            )\n",
    "            return (\n",
    "                DataLoader(\n",
    "                    dataset,\n",
    "                    sampler=sampler,\n",
    "                    worker_init_fn=get_seed_worker(seed),\n",
    "                    pin_memory=pin_memory,\n",
    "                    num_workers=num_workers,\n",
    "                    collate_fn=collate_fn_default,\n",
    "                    prefetch_factor=prefetch_factor,\n",
    "                    **_kwargs,\n",
    "                ),\n",
    "                sampler,\n",
    "            )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset type: {type(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb906c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_parallel_group(get_mixed_dp_pg : bool = False):\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‹ã‚‰å–å¾—ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        get_mixed_dp_pg (bool):\n",
    "            æ··åˆãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ã‚°ãƒ«ãƒ¼ãƒ—ã‚’å–å¾—ã™ã‚‹ã‹ã©ã†ã‹\n",
    "            æ··åˆãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ã¯ã€ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ã¨ä»–ã®ä¸¦åˆ—æ‰‹æ³•ã‚’æ··åˆã—ãŸæ‰‹æ³•\n",
    "    Returns:\n",
    "        dist.ProcessGroup: ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ã‚°ãƒ«ãƒ¼ãƒ—\n",
    "    \"\"\"\n",
    "\n",
    "    # æ··åˆãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ã‚°ãƒ«ãƒ¼ãƒ—ãŒè¦æ±‚ã•ã‚Œã€ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«å­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "    if get_mixed_dp_pg and \"mixed_dp_group\" in _GLOBAL_PARALLEL_GROUPS:\n",
    "        # æ··åˆãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ã‚°ãƒ«ãƒ¼ãƒ—ã‚’è¿”ã™\n",
    "        return _GLOBAL_PARALLEL_GROUPS[\"mixed_dp_group\"]\n",
    "\n",
    "    # é€šå¸¸ã®ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ã‚°ãƒ«ãƒ¼ãƒ—ã‚’è¿”ã™\n",
    "    return _GLOBAL_PARALLEL_GROUPS.get(\"data\", dist.group.WORLD)\n",
    "\n",
    "get_data_parallel_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb78843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã®å¼•æ•°ã‚’è¨­å®š\n",
    "\n",
    "cache_pin_memory = pin_memory_cache_pre_alloc_numels is not None\n",
    "\n",
    "dataloader_args = dict(\n",
    "    dataset=dataset,\n",
    "    batch_size=cfg.get(\"batch_size\", None),\n",
    "    num_workers=cfg.get(\"num_workers\", 4),\n",
    "    seed=cfg.get(\"seed\", 1024),\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    process_group=get_data_parallel_group(),\n",
    "    prefetch_factor=cfg.get(\"prefetch_factor\", None),\n",
    "    cache_pin_memory=cache_pin_memory,\n",
    ")\n",
    "\n",
    "dataloader_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07538605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã¨ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã‚’æº–å‚™\n",
    "dataloader, sampler = prepare_dataloader(\n",
    "    bucket_config=cfg.get(\"bucket_config\", None),\n",
    "    num_bucket_build_workers=cfg.get(\"num_bucket_build_workers\", 1),\n",
    "    **dataloader_args,\n",
    ")\n",
    "dataloader, sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = next(iter(dataloader))\n",
    "first_batch[\"sampling_interval\"], first_batch[\"video\"].shape, first_batch[\"path\"], first_batch[\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471580c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_frame = first_batch[\"video\"][:, :, 0].to(device=device, dtype=torch.float32)\n",
    "first_frame.shape\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(first_frame[0].permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca3f11d",
   "metadata": {},
   "source": [
    "### ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5cbab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b3a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @MODELS.register_module(\"dc_ae\")\n",
    "def DC_AE(\n",
    "    model_name: str,\n",
    "    device_map: str | torch.device = \"cuda\",\n",
    "    torch_dtype: torch.dtype = torch.bfloat16,\n",
    "    from_scratch: bool = False,\n",
    "    from_pretrained: str | None = None,\n",
    "    is_training: bool = False,\n",
    "    use_spatial_tiling: bool = False,\n",
    "    use_temporal_tiling: bool = False,\n",
    "    spatial_tile_size: int = 256,\n",
    "    temporal_tile_size: int = 32,\n",
    "    tile_overlap_factor: float = 0.25,\n",
    "    scaling_factor: float = None,\n",
    "    disc_off_grad_ckpt: bool = False,\n",
    ") -> DCAE_HF:\n",
    "    if not from_scratch:\n",
    "        model = DCAE_HF.from_pretrained(model_name).to(device_map, torch_dtype)\n",
    "    else:\n",
    "        model = DCAE_HF(model_name).to(device_map, torch_dtype)\n",
    "\n",
    "    if from_pretrained is not None:\n",
    "        model = load_checkpoint(model, from_pretrained, device_map=device_map)\n",
    "        print(f\"loaded dc_ae from ckpt path: {from_pretrained}\")\n",
    "\n",
    "    model.cfg.is_training = is_training\n",
    "    model.use_spatial_tiling = use_spatial_tiling\n",
    "    model.use_temporal_tiling = use_temporal_tiling\n",
    "    model.spatial_tile_size = spatial_tile_size\n",
    "    model.temporal_tile_size = temporal_tile_size\n",
    "    model.tile_overlap_factor = tile_overlap_factor\n",
    "    if scaling_factor is not None:\n",
    "        model.scaling_factor = scaling_factor\n",
    "    model.decoder.disc_off_grad_ckpt = disc_off_grad_ckpt\n",
    "    return model\n",
    "\n",
    "# {'type': 'dc_ae',\n",
    "#  'model_name': 'dc-ae-f32t4c128',\n",
    "#  'from_scratch': True,\n",
    "#  'from_pretrained': None}\n",
    "\n",
    "model = DC_AE(\n",
    "    model_name=cfg.model.model_name,\n",
    "    from_scratch=cfg.model.get(\"from_scratch\", False),\n",
    "    from_pretrained=cfg.model.get(\"from_pretrained\", None),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41da836",
   "metadata": {},
   "source": [
    "### ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¨˜éŒ²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f0ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_numel(model: torch.nn.Module) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    ãƒ­ã‚°ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å–å¾—ã™ã‚‹é–¢æ•°\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«\n",
    "\n",
    "    Returns:\n",
    "        tuple[int, int]: ãƒ¢ãƒ‡ãƒ«ã®ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¨å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®ã‚¿ãƒ—ãƒ«\n",
    "    \"\"\"\n",
    "    num_params = 0\n",
    "    num_params_trainable = 0\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ«ãƒ¼ãƒ—\n",
    "    for p in model.parameters():\n",
    "\n",
    "        # ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’åŠ ç®—\n",
    "        num_params += p.numel()\n",
    "\n",
    "        # å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’åŠ ç®—\n",
    "        if p.requires_grad:\n",
    "            num_params_trainable += p.numel()\n",
    "\n",
    "    # ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¨å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’è¿”ã™\n",
    "    return num_params, num_params_trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca80224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_model_params(model: nn.Module):\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ãƒ­ã‚°ã«è¨˜éŒ²ã™ã‚‹\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«\n",
    "    \"\"\"\n",
    "\n",
    "    # ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¨å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å–å¾—\n",
    "    num_params, num_params_trainable = get_model_numel(model)\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã®ã‚¯ãƒ©ã‚¹åã‚’å–å¾—\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    # ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "    log_message(f\"[{model_name}] Number of parameters: {format_numel_str(num_params)}\")\n",
    "    log_message(f\"[{model_name}] Number of trainable parameters: {format_numel_str(num_params_trainable)}\")\n",
    "\n",
    "log_model_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e7d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒ†ã‚£ãƒ³ã‚°ã‚’è¨­å®š\n",
    "# False\n",
    "if cfg.get(\"grad_checkpoint\", False):\n",
    "    set_grad_checkpoint(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f8be46",
   "metadata": {},
   "source": [
    "### LPIPSã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12ff759",
   "metadata": {},
   "source": [
    "LPIPS = Learned Perceptual Image Patch Similarity\n",
    "\n",
    "äººé–“ã®ç›®ã‹ã‚‰è¦‹ã¦ã€2ã¤ã®ç”»åƒãŒã©ã®ãã‚‰ã„ä¼¼ã¦ã„ã‚‹ã‹ã‚’æ¸¬ã‚‹ãŸã‚ã®è©•ä¾¡æŒ‡æ¨™\n",
    "\n",
    "ç”»åƒèªè­˜ãƒ¢ãƒ‡ãƒ«VGG16ã«æ­£è§£ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ç”Ÿæˆã—ãŸãƒ•ãƒ¬ãƒ¼ãƒ ã®é¡ä¼¼åº¦ã‚’è¨ˆæ¸¬ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e004e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vgg16(torch.nn.Module):\n",
    "    def __init__(self, requires_grad=False, pretrained=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            requires_grad (bool): ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…è¨ˆç®—ã‚’è¡Œã†ã‹ã©ã†ã‹\n",
    "            pretrained (bool): äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "        \"\"\"\n",
    "        super(vgg16, self).__init__()\n",
    "\n",
    "        # 1) äº‹å‰å­¦ç¿’æ¸ˆã¿VGG16ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´æŠ½å‡ºéƒ¨åˆ†ã‚’å–å¾—        \n",
    "        vgg_pretrained_features = models.vgg16(pretrained=pretrained).features\n",
    "\n",
    "        # 2) VGG16ã®å±¤ã‚’5ã¤ã®ã‚¹ãƒ©ã‚¤ã‚¹ã«åˆ†å‰²\n",
    "\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        self.slice5 = torch.nn.Sequential()\n",
    "        self.N_slices = 5\n",
    "\n",
    "        # 0~3å±¤\n",
    "        # ã‚¨ãƒƒã‚¸ãªã©ã®ä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´ã‚’æŠ½å‡º\n",
    "        for x in range(4):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "\n",
    "        # 4~8å±¤\n",
    "        for x in range(4, 9):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "\n",
    "        # 9~15å±¤\n",
    "        for x in range(9, 16):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "\n",
    "        # 16~22å±¤\n",
    "        for x in range(16, 23):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "\n",
    "        # 23~29å±¤\n",
    "        # ç‰©ä½“ã®ä¸€éƒ¨ãªã©ã®é«˜ãƒ¬ãƒ™ãƒ«ç‰¹å¾´ã‚’æŠ½å‡º\n",
    "        for x in range(23, 30):\n",
    "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        vgg16ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ’­ã‚’å®Ÿè¡Œã—ã€ç‰¹å®šã®å±¤ã®å‡ºåŠ›ã‚’å–å¾—ã™ã‚‹\n",
    "        \"\"\"\n",
    "\n",
    "        # 1) é †ä¼æ¬ã‚’å®Ÿè¡Œã—ã€å„ã‚¹ãƒ©ã‚¤ã‚¹ã®å‡ºåŠ›ã‚’å–å¾—\n",
    "\n",
    "        h = self.slice1(X)\n",
    "        h_relu1_2 = h\n",
    "\n",
    "        h = self.slice2(h)\n",
    "        h_relu2_2 = h\n",
    "\n",
    "        h = self.slice3(h)\n",
    "        h_relu3_3 = h\n",
    "\n",
    "        h = self.slice4(h)\n",
    "        h_relu4_3 = h\n",
    "\n",
    "        h = self.slice5(h)\n",
    "        h_relu5_3 = h\n",
    "\n",
    "        # 2) namedtupleã‚’ä½¿ç”¨ã—ã¦å‡ºåŠ›ã‚’æ•´ç†\n",
    "\n",
    "        vgg_outputs = namedtuple(\"VggOutputs\", [\n",
    "            \"relu1_2\",\n",
    "            \"relu2_2\",\n",
    "            \"relu3_3\",\n",
    "            \"relu4_3\",\n",
    "            \"relu5_3\"\n",
    "        ])\n",
    "\n",
    "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13237555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLinLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    VGGã®ç‰¹å¾´ãƒãƒƒãƒ—ã«é©ç”¨ã•ã‚Œã‚‹ç·šå½¢å±¤\n",
    "    1x1ã®ç•³ã¿è¾¼ã¿å±¤ã¨ã—ã¦å®Ÿè£…ã•ã‚Œã€ãƒãƒ£ãƒãƒ«æ•°ã‚’èª¿æ•´ã™ã‚‹\n",
    "    ç”»åƒã®çŸ¥è¦šçš„æå¤±ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chn_in, chn_out=1, use_dropout=False):\n",
    "        super(NetLinLayer, self).__init__()\n",
    "\n",
    "        # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå±¤ã‚’ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§è¿½åŠ \n",
    "        layers = ([ nn.Dropout() ] if (use_dropout) else [])\n",
    "\n",
    "        # 1x1ã®ç•³ã¿è¾¼ã¿å±¤ã‚’è¿½åŠ \n",
    "        # ãƒãƒ£ãƒ³ãƒãƒ«æ•°ã‚’chn_inã‹ã‚‰chn_outã«å‰Šæ¸›\n",
    "        layers += [\n",
    "            nn.Conv2d(\n",
    "                chn_in,\n",
    "                chn_out,\n",
    "                1,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=False, # ãƒã‚¤ã‚¢ã‚¹ãªã—\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745f07fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, local_path, chunk_size=1024):\n",
    "    \"\"\"\n",
    "    URLã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã«ä¿å­˜ã™ã‚‹é–¢æ•°\n",
    "    LPIPSãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ\n",
    "    os.makedirs(os.path.split(local_path)[0], exist_ok=True)\n",
    "\n",
    "    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã§HTTP GETãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡\n",
    "    with requests.get(url, stream=True) as r:\n",
    "\n",
    "        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ˜ãƒƒãƒ€ã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç·ã‚µã‚¤ã‚ºã‚’å–å¾—\n",
    "        total_size = int(r.headers.get(\"content-length\", 0))\n",
    "\n",
    "        # é€²æ—ãƒãƒ¼ã‚’è¡¨ç¤ºã—ãªãŒã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«æ›¸ãè¾¼ã‚€\n",
    "        with tqdm(total=total_size, unit=\"B\", unit_scale=True) as pbar:\n",
    "            with open(local_path, \"wb\") as f:\n",
    "                for data in r.iter_content(chunk_size=chunk_size):\n",
    "                    if data:\n",
    "                        f.write(data)\n",
    "                        pbar.update(chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6983847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md5_hash(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        content = f.read()\n",
    "    return hashlib.md5(content).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d5af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒãƒƒãƒ”ãƒ³ã‚°\n",
    "CKPT_MAP = {\"vgg_lpips\": \"vgg.pth\"}\n",
    "\n",
    "# MD5ãƒã‚§ãƒƒã‚¯ç”¨ã®ãƒãƒƒã‚·ãƒ¥å€¤\n",
    "MD5_MAP = {\"vgg_lpips\": \"d507d7349b931f0638a25a48a722f98a\"}\n",
    "\n",
    "# äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URLãƒãƒƒãƒ”ãƒ³ã‚°\n",
    "URL_MAP = {\"vgg_lpips\": \"https://heibox.uni-heidelberg.de/f/607503859c864bc1b30b/?dl=1\"}\n",
    "\n",
    "\n",
    "def get_ckpt_path(name, root=\".\", check=False):\n",
    "    \"\"\"\n",
    "    äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‘ã‚¹ã‚’å–å¾—ã™ã‚‹é–¢æ•°\n",
    "    \"\"\"\n",
    "\n",
    "    assert name in URL_MAP\n",
    "\n",
    "    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜ãƒ‘ã‚¹ã‚’ä½œæˆ\n",
    "    path = os.path.join(root, CKPT_MAP[name])\n",
    "\n",
    "    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒå­˜åœ¨ã—ãªã„ã‹ã€MD5ãƒã‚§ãƒƒã‚¯ãŒæœ‰åŠ¹ã§ä¸ä¸€è‡´ã®å ´åˆã€ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹\n",
    "    if not os.path.exists(path) or (check and not md5_hash(path) == MD5_MAP[name]):\n",
    "        print(\"Downloading {} model from {} to {}\".format(name, URL_MAP[name], path))\n",
    "        download(URL_MAP[name], path)\n",
    "        md5 = md5_hash(path)\n",
    "        assert md5 == MD5_MAP[name], md5\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13056512",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch._disable_dynamo\n",
    "def checkpoint(function, *args, use_reentrant: Optional[bool] = None, context_fn: Callable[[], Tuple[ContextManager, ContextManager]] = noop_context_fn, determinism_check: str = _DEFAULT_DETERMINISM_MODE, debug: bool = False, **kwargs):\n",
    "    \"\"\"\n",
    "    ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒ†ã‚£ãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°\n",
    "    \"\"\"\n",
    "    if use_reentrant is None:\n",
    "        warnings.warn(\n",
    "            \"torch.utils.checkpoint: the use_reentrant parameter should be \"\n",
    "            \"passed explicitly. In version 2.4 we will raise an exception \"\n",
    "            \"if use_reentrant is not passed. use_reentrant=False is \"\n",
    "            \"recommended, but if you need to preserve the current default \"\n",
    "            \"behavior, you can pass use_reentrant=True. Refer to docs for more \"\n",
    "            \"details on the differences between the two variants.\",\n",
    "            stacklevel=2,\n",
    "        )\n",
    "        use_reentrant = True\n",
    "\n",
    "    # Hack to mix *args with **kwargs in a python 2.7-compliant way\n",
    "    preserve = kwargs.pop(\"preserve_rng_state\", True)\n",
    "    if kwargs and use_reentrant:\n",
    "        raise ValueError(\"Unexpected keyword arguments: \" + \",\".join(arg for arg in kwargs))\n",
    "\n",
    "    if use_reentrant:\n",
    "        if context_fn is not noop_context_fn or debug is not False:\n",
    "            raise ValueError(\"Passing `context_fn` or `debug` is only supported when \" \"use_reentrant=False.\")\n",
    "        return CheckpointFunctionWithOffload.apply(function, preserve, *args)\n",
    "    else:\n",
    "        gen = _checkpoint_without_reentrant_generator(\n",
    "            function, preserve, context_fn, determinism_check, debug, *args, **kwargs\n",
    "        )\n",
    "        # Runs pre-forward logic\n",
    "        next(gen)\n",
    "        ret = function(*args, **kwargs)\n",
    "        # Runs post-forward logic\n",
    "        try:\n",
    "            next(gen)\n",
    "        except StopIteration:\n",
    "            return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35765ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tensor(x, eps=1e-10):\n",
    "    \"\"\"\n",
    "    ãƒ†ãƒ³ã‚½ãƒ«ã‚’ãƒãƒ£ãƒ³ãƒãƒ«ã”ã¨ã«L2æ­£è¦åŒ–ã™ã‚‹é–¢æ•°\n",
    "    å„ã‚µãƒ³ãƒ—ãƒ«ã®L2ãƒãƒ«ãƒ ã‚’1ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹\n",
    "    LPIPSã®æå¤±è¨ˆç®—ã§ä½¿ç”¨\n",
    "\n",
    "    x = x / (||x||_2 + eps)\n",
    "    \"\"\"\n",
    "    # (B, C, H, W)ã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’æƒ³å®š\n",
    "    norm_factor = torch.sqrt(torch.sum(x**2, dim=1, keepdim=True))\n",
    "    return x / (norm_factor + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da396c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_average(x, keepdim=True):\n",
    "    \"\"\"\n",
    "    ç©ºé–“æ¬¡å…ƒï¼ˆé«˜ã•ã¨å¹…ï¼‰ã«ã‚ãŸã‚‹ãƒ†ãƒ³ã‚½ãƒ«ã®å¹³å‡ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°\n",
    "    LPIPSã®æå¤±è¨ˆç®—ã§ã€ãƒ”ã‚¯ã‚»ãƒ«ã”ã¨ã®æå¤±ã‚’é›†ç´„ã™ã‚‹ãŸã‚ã«ä½¿ç”¨\n",
    "    \"\"\"\n",
    "\n",
    "    # (B, C, H, W)ã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’æƒ³å®š\n",
    "    return x.mean([2, 3], keepdim=keepdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234b81b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    å…¥åŠ›ç”»åƒã‚’LPIPSãƒ¢ãƒ‡ãƒ«ã®æœŸå¾…ã™ã‚‹ç¯„å›²ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹å±¤\n",
    "    å…¥åŠ›ç”»åƒã¯[-1, 1]ã®ç¯„å›²ã«æ­£è¦åŒ–ã•ã‚Œã¦ã„ã‚‹ã¨ä»®å®š\n",
    "    LPIPSãƒ¢ãƒ‡ãƒ«ã¯ã€[-1.030, 0.920]ã®ç¯„å›²ã§å‹•ä½œã™ã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ã„ã‚‹\n",
    "    LPIPSã®å…¥åŠ›æ™‚ã«ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ScalingLayer, self).__init__()\n",
    "        self.register_buffer(\"shift\", torch.Tensor([-0.030, -0.088, -0.188])[None, :, None, None])\n",
    "        self.register_buffer(\"scale\", torch.Tensor([0.458, 0.448, 0.450])[None, :, None, None])\n",
    "\n",
    "    def forward(self, inp):\n",
    "        return (inp - self.shift) / self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c95a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LPIPS(nn.Module):\n",
    "    \"\"\"\n",
    "    Learned Perceptual Image Patch Similarity (LPIPS) ãƒ¢ãƒ‡ãƒ«\n",
    "    ç”»åƒã®çŸ¥è¦šçš„é¡ä¼¼åº¦ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_dropout=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # å…¥åŠ›ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å±¤\n",
    "        self.scaling_layer = ScalingLayer()\n",
    "\n",
    "        # VGG16ã®ãƒãƒ£ãƒ³ãƒãƒ«æ•°\n",
    "        self.chns = [64, 128, 256, 512, 512]\n",
    "\n",
    "        # äº‹å‰å­¦ç¿’æ¸ˆã¿VGG16ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "        self.net = vgg16(pretrained=True, requires_grad=False)\n",
    "\n",
    "        # å„VGG16ã®ç‰¹å¾´ãƒãƒƒãƒ—ã«å¯¾å¿œã™ã‚‹ç·šå½¢å±¤ã‚’ä½œæˆ\n",
    "        # å‡ºåŠ›ãƒãƒ£ãƒãƒ«æ•°ã‚’1ã«ã™ã‚‹\n",
    "        self.lin0 = NetLinLayer(self.chns[0], use_dropout=use_dropout)\n",
    "        self.lin1 = NetLinLayer(self.chns[1], use_dropout=use_dropout)\n",
    "        self.lin2 = NetLinLayer(self.chns[2], use_dropout=use_dropout)\n",
    "        self.lin3 = NetLinLayer(self.chns[3], use_dropout=use_dropout)\n",
    "        self.lin4 = NetLinLayer(self.chns[4], use_dropout=use_dropout)\n",
    "\n",
    "        # ç·šå½¢å±¤ã‚’ãƒªã‚¹ãƒˆã«ã¾ã¨ã‚ã‚‹\n",
    "        self.lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]\n",
    "\n",
    "        # äº‹å‰å­¦ç¿’æ¸ˆã¿ã®LPIPSãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "        self.load_from_pretrained()\n",
    "\n",
    "        # ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ–\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def load_from_pretrained(self, name=\"vgg_lpips\"):\n",
    "        \"\"\"\n",
    "        äº‹å‰å­¦ç¿’æ¸ˆã¿ã®LPIPSãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹\n",
    "        \"\"\"\n",
    "        path = os.path.expanduser(\"~/.cache/opensora/taming/modules/autoencoder/lpips\")\n",
    "        ckpt = get_ckpt_path(name, path)\n",
    "        self.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, name=\"vgg_lpips\"):\n",
    "        if name != \"vgg_lpips\":\n",
    "            raise NotImplementedError\n",
    "        model = cls()\n",
    "        ckpt = get_ckpt_path(name)\n",
    "        model.load_state_dict(torch.load(ckpt, map_location=torch.device(\"cpu\")), strict=False)\n",
    "        return model\n",
    "\n",
    "    def forward_old(self, input, target):\n",
    "        in0_input, in1_input = (self.scaling_layer(input), self.scaling_layer(target))\n",
    "        outs0, outs1 = self.net(in0_input), self.net(in1_input)\n",
    "        feats0, feats1, diffs = {}, {}, {}\n",
    "        lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]\n",
    "        for kk in range(len(self.chns)):\n",
    "            feats0[kk], feats1[kk] = normalize_tensor(outs0[kk]), normalize_tensor(outs1[kk])\n",
    "            diffs[kk] = (feats0[kk] - feats1[kk]) ** 2\n",
    "\n",
    "        res = [spatial_average(lins[kk].model(diffs[kk]), keepdim=True) for kk in range(len(self.chns))]\n",
    "        val = res[0]\n",
    "        for l in range(1, len(self.chns)):\n",
    "            val += res[l]\n",
    "        return val\n",
    "\n",
    "    def get_layer_loss(self, input, target, i):\n",
    "        \"\"\"\n",
    "        æŒ‡å®šã•ã‚ŒãŸå±¤ã§ã®LPIPSæå¤±ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°\n",
    "\n",
    "        Args:\n",
    "            input (torch.Tensor): å…¥åŠ›ç”»åƒã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            target (torch.Tensor): ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”»åƒã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "            i (int): è¨ˆç®—ã™ã‚‹å±¤ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "        Returns:\n",
    "            tuple: å¹³å‡æå¤±ã€å…¥åŠ›ç‰¹å¾´ãƒãƒƒãƒ—ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹å¾´ãƒãƒƒãƒ—\n",
    "        \"\"\"\n",
    "        log_.info(f\"LPIPS get_layer_loss: {input.shape=} {target.shape=} {i=}\")\n",
    "\n",
    "        # æŒ‡å®šã•ã‚ŒãŸå±¤ã®é †ä¼æ¬ã‚’ã€å…¥åŠ›ç”»åƒã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”»åƒã«å¯¾ã—ã¦å®Ÿè¡Œ\n",
    "        input, target = getattr(self.net, f\"slice{i+1}\")(input), getattr(self.net, f\"slice{i+1}\")(target)\n",
    "\n",
    "        # ç‰¹å¾´ãƒãƒƒãƒ—ã‚’æ­£è¦åŒ–\n",
    "        feats0, feats1 = normalize_tensor(input), normalize_tensor(target)\n",
    "\n",
    "        # ç‰¹å¾´ãƒãƒƒãƒ—ã®å·®ã®äºŒä¹—ã‚’è¨ˆç®—\n",
    "        diff = (feats0 - feats1) ** 2\n",
    "\n",
    "        # ç©ºé–“å¹³å‡ã‚’è¨ˆç®—ã—ã¦å¹³å‡æå¤±ã‚’å–å¾—\n",
    "        avg = spatial_average(self.lins[i].model(diff), keepdim=True)\n",
    "\n",
    "        return avg, input, target\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        LPIPSãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ’­ã‚’å®Ÿè¡Œã—ã€çŸ¥è¦šçš„æå¤±ã‚’è¨ˆç®—ã™ã‚‹\n",
    "        \"\"\"\n",
    "        log_.info(f\"LPIPS forward: {input.shape=} {target.shape=}\")\n",
    "\n",
    "        # å…¥åŠ›ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "        input, target = (self.scaling_layer(input), self.scaling_layer(target))\n",
    "\n",
    "        val = None\n",
    "        for i in range(len(self.chns)):\n",
    "            # ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦å±¤ã”ã¨ã®æå¤±ã‚’è¨ˆç®—\n",
    "            avg, input, target = checkpoint(\n",
    "                self.get_layer_loss,\n",
    "                input,\n",
    "                target,\n",
    "                i,\n",
    "                use_reentrant=False\n",
    "            )\n",
    "\n",
    "            # æå¤±ã‚’ç´¯ç©\n",
    "            val = avg if val is None else val + avg\n",
    "\n",
    "        return val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa97af",
   "metadata": {},
   "source": [
    "### VAELossã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d04748",
   "metadata": {},
   "source": [
    "VAELossã¯ã€å…ƒã®å‹•ç”»ã¨å¾©å…ƒã—ãŸå‹•ç”»ã‚’æ¯”è¼ƒã™ã‚‹è©•ä¾¡æŒ‡æ¨™:\n",
    "\n",
    "- å†æ§‹æˆæå¤±ï¼ˆL1æå¤±ï¼‰: ãƒ”ã‚¯ã‚»ãƒ«ã”ã¨ã®è‰²ã®ãšã‚Œ `recon_loss`\n",
    "- çŸ¥è¦šæå¤±ï¼ˆLPIPSï¼‰: äººãŒè¦‹ã¦ã‚¯ãƒƒã‚­ãƒªã—ã¦ã„ã‚‹ã‹ `perceptual_loss`\n",
    "- KLæå¤±: æ½œåœ¨è¡¨ç¾ãŒç¶ºéº—ãªã‚¬ã‚¦ã‚¹åˆ†å¸ƒã«è¿‘ã„ã‹ã©ã†ã‹ `kl_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f78c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1(x, y):\n",
    "    \"\"\"\n",
    "    L1æå¤±é–¢æ•°\n",
    "    2ã¤ã®ãƒ†ãƒ³ã‚½ãƒ«é–“ã®çµ¶å¯¾å·®ã®è¦ç´ ã”ã¨ã®å’Œã‚’è¨ˆç®—ã™ã‚‹\n",
    "    \"\"\"\n",
    "    return torch.abs(x - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7797a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_mean(x):\n",
    "    \"\"\"\n",
    "    ãƒãƒƒãƒæ¬¡å…ƒã«ã‚ãŸã‚‹ãƒ†ãƒ³ã‚½ãƒ«ã®å¹³å‡ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°\n",
    "    \"\"\"\n",
    "    return torch.sum(x) / x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19bad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAELoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        logvar_init=0.0,\n",
    "        perceptual_loss_weight=1.0,\n",
    "        kl_loss_weight=5e-4,\n",
    "        device=\"cpu\",\n",
    "        dtype=\"bf16\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if type(dtype) == str:\n",
    "            if dtype == \"bf16\":\n",
    "                dtype = torch.bfloat16\n",
    "            elif dtype == \"fp16\":\n",
    "                dtype = torch.float16\n",
    "            elif dtype == \"fp32\":\n",
    "                dtype = torch.float32\n",
    "            else:\n",
    "                raise NotImplementedError(f\"dtype: {dtype}\")\n",
    "\n",
    "        # KLæå¤±\n",
    "        self.kl_weight = kl_loss_weight\n",
    "\n",
    "        # çŸ¥è¦šæå¤±\n",
    "        self.perceptual_loss_fn = LPIPS().eval().to(device, dtype)\n",
    "        self.perceptual_loss_fn.requires_grad_(False)\n",
    "        self.perceptual_loss_weight = perceptual_loss_weight\n",
    "\n",
    "        # å†æ§‹æˆæå¤±ã¨KLæå¤±ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ãŸã‚ã®å¯¾æ•°åˆ†æ•£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        self.logvar = nn.Parameter(torch.ones(size=()) * logvar_init)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        video,\n",
    "        recon_video,\n",
    "        posterior,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        VAEã®æå¤±è¨ˆç®—ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°\n",
    "\n",
    "        Args:\n",
    "            video (torch.Tensor):\n",
    "                å…ƒã®å‹•ç”»ãƒ†ãƒ³ã‚½ãƒ«ã€å½¢çŠ¶ã¯ (B, C, T, H, W)\n",
    "            recon_video (torch.Tensor):\n",
    "                å†æ§‹æˆã•ã‚ŒãŸå‹•ç”»ãƒ†ãƒ³ã‚½ãƒ«ã€å½¢çŠ¶ã¯ (B, C, T, H, W)\n",
    "            posterior (Distribution | None):\n",
    "                æ½œåœ¨å¤‰æ•°ã®ç¢ºç‡åˆ†å¸ƒã€KLæå¤±ã®è¨ˆç®—ã«ä½¿ç”¨\n",
    "                Noneã®å ´åˆã€KLæå¤±ã¯0ã«ãªã‚‹\n",
    "        Returns:\n",
    "            dict:\n",
    "                å„æå¤±æˆåˆ†ã‚’å«ã‚€è¾æ›¸\n",
    "                - \"nll_loss\": è² ã®å¯¾æ•°å°¤åº¦æå¤±\n",
    "                - \"kl_loss\": KLæå¤±\n",
    "                - \"recon_loss\": å†æ§‹æˆæå¤±\n",
    "                - \"perceptual_loss\": çŸ¥è¦šæå¤±\n",
    "        \"\"\"\n",
    "        log_.info(f\"VAELoss forward: {video.shape=} {recon_video.shape=}\")\n",
    "\n",
    "\n",
    "        # 1) å‹•ç”»ã‚’å¤§é‡ã®ç”»åƒã«å¤‰æ›\n",
    "\n",
    "        video.size(0)\n",
    "\n",
    "        # (B, C, T, H, W) -> (B*T, C, H, W)\n",
    "        video = rearrange(video, \"b c t h w -> (b t) c h w\").contiguous()\n",
    "\n",
    "        # (B, C, T, H, W) -> (B*T, C, H, W)\n",
    "        recon_video = rearrange(recon_video, \"b c t h w -> (b t) c h w\").contiguous()\n",
    "\n",
    "        # 2) å†æ§‹æˆæå¤±ã‚’è¨ˆç®—\n",
    "\n",
    "        recon_loss = l1(video, recon_video)\n",
    "\n",
    "        # 3) çŸ¥è¦šæå¤±ã‚’è¨ˆç®—\n",
    "\n",
    "        perceptual_loss = self.perceptual_loss_fn(video, recon_video)\n",
    "\n",
    "        # 4) NLLæå¤±ã‚’è¨ˆç®—\n",
    "\n",
    "        # NLL = å†æ§‹æˆæå¤± + çŸ¥è¦šæå¤± * é‡ã¿\n",
    "        nll_loss = recon_loss + perceptual_loss * self.perceptual_loss_weight\n",
    "\n",
    "        # å­¦ç¿’å¯èƒ½ãªå¯¾æ•°åˆ†æ•£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦NLLæå¤±ã‚’èª¿æ•´\n",
    "        # KLæå¤±ã¨å†æ§‹æˆæå¤±ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è‡ªå‹•çš„ã«å–ã‚Œã‚‹\n",
    "        nll_loss = nll_loss / torch.exp(self.logvar) + self.logvar\n",
    "\n",
    "        # 5) ãƒãƒƒãƒå¹³å‡ã‚’è¨ˆç®—\n",
    "\n",
    "        # NLLæå¤±ã¯ãƒãƒƒãƒå†…ã®å„è¦ç´ ã«å¯¾ã—ã¦å¹³å‡ã‚’è¨ˆç®—\n",
    "        nll_loss = batch_mean(nll_loss)\n",
    "\n",
    "        # å†æ§‹æˆæå¤±ã¯ãƒãƒƒãƒå†…ã®å„è¦ç´ ã«å¯¾ã—ã¦å¹³å‡ã‚’è¨ˆç®—\n",
    "        recon_loss = batch_mean(recon_loss)\n",
    "\n",
    "        # çŸ¥è¦šæå¤±ã¯ãƒãƒƒãƒå†…ã®å„è¦ç´ ã«å¯¾ã—ã¦å¹³å‡ã‚’è¨ˆç®—ã—ã€å…ƒã®å‹•ç”»ã®è¦ç´ æ•°ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°\n",
    "        numel_elements = video.numel() // video.size(0)\n",
    "        perceptual_loss = batch_mean(perceptual_loss) * numel_elements\n",
    "\n",
    "        # 6) KLæå¤±ã‚’è¨ˆç®—\n",
    "\n",
    "        if posterior is None:\n",
    "            kl_loss = torch.tensor(0.0).to(video.device, video.dtype)\n",
    "        else:\n",
    "            # KLæå¤±ã‚’è¨ˆç®—\n",
    "            # æ½œåœ¨å¤‰æ•°ã®ç¢ºç‡åˆ†å¸ƒãŒæ¨™æº–æ­£è¦åˆ†å¸ƒã‹ã‚‰ã©ã‚Œã ã‘æ­ªã‚“ã§ã„ã‚‹ã‹\n",
    "            kl_loss = posterior.kl()\n",
    "\n",
    "            kl_loss = batch_mean(kl_loss)\n",
    "\n",
    "        # KLæå¤±ã«é‡ã¿ã‚’é©ç”¨\n",
    "        weighted_kl_loss = kl_loss * self.kl_weight\n",
    "\n",
    "        return {\n",
    "            \"nll_loss\": nll_loss,\n",
    "            \"kl_loss\": weighted_kl_loss,\n",
    "            \"recon_loss\": recon_loss,\n",
    "            \"perceptual_loss\": perceptual_loss,\n",
    "        }\n",
    "\n",
    "vae_loss_fn = VAELoss(**cfg.vae_loss_config, device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9edd96b",
   "metadata": {},
   "source": [
    "### EMAãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45567576",
   "metadata": {},
   "source": [
    "EMA = Exponential Moving Averageï¼ˆæŒ‡æ•°ç§»å‹•å¹³å‡ï¼‰\n",
    "\n",
    "è¨“ç·´ä¸­ã®ãƒ¢ãƒ‡ãƒ«ã®ç§»å‹•å¹³å‡ã‚’ã¨ã£ãŸæ±åŒ–æ€§èƒ½ãŒç›¸å¯¾çš„ã«é«˜ã„ãƒ¢ãƒ‡ãƒ«ã§ã€è©•ä¾¡ã‚„æ¨è«–ã«ä½¿ç”¨ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adafc774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_model_param_shape(model: torch.nn.Module) -> dict:\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½¢çŠ¶ã‚’è¨˜éŒ²ã™ã‚‹é–¢æ•°\n",
    "    ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹ã‚’ä¿å­˜ã™ã‚‹ã¨ãã«ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): å½¢çŠ¶ã‚’è¨˜éŒ²ã™ã‚‹ãƒ¢ãƒ‡ãƒ«\n",
    "    Returns:\n",
    "        dict: ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½¢çŠ¶\n",
    "    \"\"\"\n",
    "\n",
    "    param_shape = {}\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½¢çŠ¶ã‚’è¨˜éŒ²\n",
    "        param_shape[name] = param.shape\n",
    "\n",
    "    return param_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc340ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EMAãƒ¢ãƒ‡ãƒ«ãŒæœ‰åŠ¹ãªå ´åˆ\n",
    "# True\n",
    "if cfg.get(\"ema_decay\", None) is not None:\n",
    "    # ãƒ¡ã‚¤ãƒ³ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã€EMAãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ\n",
    "    ema = deepcopy(model).cpu().eval().requires_grad_(False)\n",
    "\n",
    "    # EMAãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å½¢çŠ¶ã‚’è¨˜éŒ²\n",
    "    ema_shape_dict = record_model_param_shape(ema)\n",
    "    logger.info(\"EMA model created.\")\n",
    "else:\n",
    "    ema = ema_shape_dict = None\n",
    "    logger.info(\"No EMA model created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59017f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def update_ema(\n",
    "    ema_model: torch.nn.Module, model: torch.nn.Module, optimizer=None, decay: float = 0.9999, sharded: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    EMAãƒ¢ãƒ‡ãƒ«ã‚’ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã«å‘ã‘ã¦æ›´æ–°ã™ã‚‹é–¢æ•°\n",
    "\n",
    "    Args:\n",
    "        ema_model (torch.nn.Module): EMAãƒ¢ãƒ‡ãƒ«\n",
    "        model (torch.nn.Module): ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«\n",
    "        optimizer (torch.optim.Optimizer): ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶\n",
    "        decay (float): æ¸›è¡°ç‡\n",
    "        sharded (bool): ãƒ¢ãƒ‡ãƒ«ãŒã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹\n",
    "    \"\"\"\n",
    "    ema_params = OrderedDict(ema_model.named_parameters())\n",
    "    model_params = OrderedDict(model.named_parameters())\n",
    "\n",
    "    for name, param in model_params.items():\n",
    "        if name == \"pos_embed\":\n",
    "            continue\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        if not sharded:\n",
    "            param_data = param.data\n",
    "            ema_params[name].mul_(decay).add_(param_data, alpha=1 - decay)\n",
    "        else:\n",
    "            if param.data.dtype != torch.float32:\n",
    "                param_id = id(param)\n",
    "                master_param = optimizer.get_working_to_master_map()[param_id]\n",
    "                param_data = master_param.data\n",
    "            else:\n",
    "                param_data = param.data\n",
    "            ema_params[name].mul_(decay).add_(param_data, alpha=1 - decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d1e10b",
   "metadata": {},
   "source": [
    "### ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒŸãƒãƒ¼ã‚¿ãƒ¼ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd16f9e3",
   "metadata": {},
   "source": [
    "DC-AEã®ç¬¬2ã‚¹ãƒ†ãƒƒãƒ—ã§ä½¿ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18820c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_discriminator = cfg.get(\"discriminator\", None) is not None\n",
    "use_discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a32a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False\n",
    "if use_discriminator:\n",
    "    discriminator = build_module(cfg.discriminator, MODELS).to(device, dtype).train()\n",
    "    log_model_params(discriminator)\n",
    "    generator_loss_fn = GeneratorLoss(**cfg.gen_loss_config)\n",
    "    discriminator_loss_fn = DiscriminatorLoss(**cfg.disc_loss_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe8087",
   "metadata": {},
   "source": [
    "### ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53e5e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b7f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = create_optimizer(model, cfg.optim)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c532a5",
   "metadata": {},
   "source": [
    "### å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®ä½œæˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef3a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚¨ãƒãƒƒã‚¯ã‚ãŸã‚Šã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’è¨ˆç®—\n",
    "num_steps_per_epoch = len(dataloader)\n",
    "num_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be06189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26438bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = create_lr_scheduler(\n",
    "    optimizer=optimizer,\n",
    "    num_steps_per_epoch=num_steps_per_epoch,\n",
    "    epochs=cfg.get(\"epochs\", 1000),\n",
    "    **cfg.lr_scheduler\n",
    ")\n",
    "lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f2a7a1",
   "metadata": {},
   "source": [
    "### ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒŸãƒãƒ¼ã‚¿ãƒ¼ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1f0e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False\n",
    "if use_discriminator:\n",
    "    disc_optimizer = create_optimizer(discriminator, cfg.optim_discriminator)\n",
    "    disc_lr_scheduler = create_lr_scheduler(\n",
    "        optimizer=disc_optimizer,\n",
    "        num_steps_per_epoch=num_steps_per_epoch,\n",
    "        epochs=cfg.get(\"epochs\", 1000),\n",
    "        **cfg.disc_lr_scheduler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb642e84",
   "metadata": {},
   "source": [
    "### è¨“ç·´ã®æº–å‚™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b580b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’è¨­å®š\n",
    "torch.set_default_dtype(dtype)\n",
    "dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e1931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosterã‚’ä½¿ç”¨ã—ã¦ã€åˆ†æ•£è¨“ç·´ã«å¯¾å¿œã•ã›ã‚‹\n",
    "model, optimizer, _, dataloader, lr_scheduler = booster.boost(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    lr_scheduler=lr_scheduler,\n",
    "    dataloader=dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73f47b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False\n",
    "if use_discriminator:\n",
    "    discriminator, disc_optimizer, _, _, disc_lr_scheduler = booster.boost(\n",
    "        model=discriminator,\n",
    "        optimizer=disc_optimizer,\n",
    "        lr_scheduler=disc_lr_scheduler,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a9344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_default_dtype(torch.float)\n",
    "# logger.info(\"Boosted model for distributed training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49de0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’è¨­å®š\n",
    "\n",
    "# ã‚¨ãƒãƒƒã‚¯æ•°\n",
    "cfg_epochs = cfg.get(\"epochs\", 1000)\n",
    "\n",
    "# å‹•ç”»ã¨ç”»åƒã®æ··åˆæˆ¦ç•¥\n",
    "# mixed_video_image\n",
    "mixed_strategy = cfg.get(\"mixed_strategy\", None)\n",
    "\n",
    "# å‹•ç”»ã«å¯¾ã—ã¦ç”»åƒã‚’æ··åˆã™ã‚‹æ¯”ç‡\n",
    "# 0.2\n",
    "mixed_image_ratio = cfg.get(\"mixed_image_ratio\", 0.0)\n",
    "\n",
    "# åˆ†æ•£è¨“ç·´ã®ãƒ©ãƒ³ã‚¯æ•°ã‚’å–å¾—\n",
    "num_ranks = dist.get_world_size()\n",
    "print(\"num_ranks:\", num_ranks)\n",
    "\n",
    "# æ··åˆç”»åƒæ¯”ç‡ã‚’ãƒ©ãƒ³ã‚¯æ•°ã«åŸºã¥ã„ã¦èª¿æ•´\n",
    "# è¤‡æ•°ãƒ©ãƒ³ã‚¯ã®å ´åˆã€å„ãƒ©ãƒ³ã‚¯ãŒç•°ãªã‚‹ç”»åƒã‚’å‡¦ç†ã§ãã‚‹ã‚ˆã†ã«èª¿æ•´\n",
    "# 0.2\n",
    "modulated_mixed_image_ratio = (\n",
    "    num_ranks * mixed_image_ratio / (num_ranks - 1) if num_ranks > 1 else mixed_image_ratio\n",
    ")\n",
    "\n",
    "if is_log_process(plugin_type, plugin_config):\n",
    "    print(\"modulated mixed image ratio:\", modulated_mixed_image_ratio)\n",
    "\n",
    "# ã‚¨ãƒãƒƒã‚¯æ•°ã€ã‚¹ãƒ†ãƒƒãƒ—æ•°ã€ãƒ­ã‚°ã‚¹ãƒ†ãƒƒãƒ—æ•°ã€ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®åˆæœŸåŒ–\n",
    "start_epoch = start_step = log_step = acc_step = 0\n",
    "\n",
    "# 1ã‚¨ãƒãƒƒã‚¯ã‚ãŸã‚Šã®ç´¯ç©æå¤±\n",
    "running_loss = dict(\n",
    "    all=0.0, # å…¨æå¤±ã®åˆè¨ˆ\n",
    "    nll=0.0, # è² ã®å¯¾æ•°å°¤åº¦æå¤±\n",
    "    nll_rec=0.0, # å†æ§‹æˆæå¤±\n",
    "    nll_per=0.0, # çŸ¥è¦šæå¤±\n",
    "    kl=0.0, # KLæå¤±\n",
    "    gen=0.0, # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿æå¤±\n",
    "    gen_w=0.0, # é‡ã¿ä»˜ãã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿æå¤±\n",
    "    disc=0.0, # è­˜åˆ¥å™¨æå¤±\n",
    "    debug=0.0, # ãƒ‡ãƒãƒƒã‚°ç”¨æå¤±\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_reduce_sum(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    å…¨ã¦ã®ãƒ—ãƒ­ã‚»ã‚¹ã§ãƒ†ãƒ³ã‚½ãƒ«ã‚’åˆè¨ˆã™ã‚‹é–¢æ•°\n",
    "    log_lossé–¢æ•°ã§ä½¿ç”¨ã•ã‚Œã‚‹\n",
    "    \"\"\"\n",
    "    dist.all_reduce(tensor=tensor, group=get_data_parallel_group())\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d6fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(name, loss, loss_dict, use_video):\n",
    "    \"\"\"\n",
    "    æå¤±ã‚’ãƒ­ã‚°ã«è¨˜éŒ²ã™ã‚‹é–¢æ•°\n",
    "    Args:\n",
    "        name (str): æå¤±ã®åå‰\n",
    "        loss (torch.Tensor): æå¤±å€¤ã®ãƒ†ãƒ³ã‚½ãƒ«\n",
    "        loss_dict (dict): æå¤±å€¤ã‚’æ ¼ç´ã™ã‚‹è¾æ›¸\n",
    "        use_video (int): å‹•ç”»ãƒ‡ãƒ¼ã‚¿ãŒä½¿ç”¨ã•ã‚ŒãŸã‹ã©ã†ã‹ã®ãƒ•ãƒ©ã‚°ï¼ˆ0ã¾ãŸã¯1ï¼‰\n",
    "    \"\"\"\n",
    "    # å‹•ç”»ã‚’ä½¿ç”¨ã—ãªã„å ´åˆã€æå¤±ã‚’0ã«è¨­å®š\n",
    "    if use_video == 0:\n",
    "        loss.data = torch.tensor(0.0, device=device, dtype=dtype)\n",
    "\n",
    "    # å…¨ã¦ã®ãƒ—ãƒ­ã‚»ã‚¹ã§æå¤±ã‚’åˆè¨ˆ\n",
    "    all_reduce_sum(loss.data)\n",
    "\n",
    "    # å‹•ç”»ã‚’ä½¿ç”¨ã—ãŸãƒ—ãƒ­ã‚»ã‚¹æ•°ã‚’åˆè¨ˆ\n",
    "    num_video = torch.tensor(use_video, device=device, dtype=dtype)\n",
    "    all_reduce_sum(num_video)\n",
    "\n",
    "    # å‹•ç”»ã‚’æ‰±ã£ãŸãƒ—ãƒ­ã‚»ã‚¹æ•°ã§æå¤±ã®å¹³å‡ã‚’è¨ˆç®—\n",
    "    loss_item = loss.item() / num_video.item()\n",
    "\n",
    "    # è¾æ›¸ã«æå¤±ã‚’è¨˜éŒ²\n",
    "    loss_dict[name] = loss_item\n",
    "\n",
    "    # ç´¯ç©æå¤±ã‚’æ›´æ–°\n",
    "    running_loss[name] += loss_item\n",
    "\n",
    "logger.info(\"Training for %s epochs with %s steps per epoch\", cfg_epochs, num_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90f234e",
   "metadata": {},
   "source": [
    "### EMAãƒ¢ãƒ‡ãƒ«ã‚’ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602f0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_sharding(model: torch.nn.Module, device: torch.device = None):\n",
    "    \"\"\"\n",
    "    è¤‡æ•°GPUã«ã‚ãŸã£ã¦ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã™ã‚‹é–¢æ•°\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): åˆ†å‰²ã™ã‚‹ãƒ¢ãƒ‡ãƒ«\n",
    "        device (torch.device): ãƒ¢ãƒ‡ãƒ«ã‚’åˆ†å‰²ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹\n",
    "    \"\"\"\n",
    "    log_.info(f\"model_sharding {device=}\")\n",
    "\n",
    "    # 1) ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ©ãƒ³ã‚¯ã¨ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚ºã‚’å–å¾—\n",
    "\n",
    "    global_rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "\n",
    "    # 2) å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã”ã¨ã«åˆ†å‰²ã‚’å®Ÿè¡Œ\n",
    "    for _, param in model.named_parameters():\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ‡ãƒã‚¤ã‚¹ã‚’è¨­å®š\n",
    "        if device is None:\n",
    "            device = param.device\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¦ç´ æ•°ã«åŸºã¥ã„ã¦ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚µã‚¤ã‚ºã‚’è¨ˆç®—\n",
    "        padding_size = (world_size - param.numel() % world_size) % world_size\n",
    "        log_.debug(f\"{param.shape=} {param.numel()=} {padding_size=}\")\n",
    "\n",
    "        if padding_size > 0:\n",
    "            padding_param = torch.nn.functional.pad(param.data.view(-1), [0, padding_size])\n",
    "        else:\n",
    "            padding_param = param.data.view(-1)\n",
    "\n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‡ç­‰ã«åˆ†å‰²\n",
    "\n",
    "        splited_params = padding_param.split(padding_param.numel() // world_size)\n",
    "\n",
    "        splited_params = splited_params[global_rank]\n",
    "\n",
    "        # ãƒ‡ãƒã‚¤ã‚¹ã¸ã®è»¢é€\n",
    "\n",
    "        param.data = splited_params.to(device)\n",
    "\n",
    "if ema is not None:\n",
    "    model_sharding(ema)\n",
    "    ema = ema.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26bb06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False\n",
    "if cfg.get(\"freeze_layers\", None) == \"all\":\n",
    "    for param in model.module.parameters():\n",
    "        param.requires_grad = False\n",
    "    print(\"all layers frozen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c0f68",
   "metadata": {},
   "source": [
    "### è¨“ç·´ã®å†é–‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b9521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# False\n",
    "if cfg.get(\"load\", None) is not None:\n",
    "    logger.info(\"Loading checkpoint from %s\", cfg.load)\n",
    "    start_epoch = cfg.get(\"start_epoch\", None)\n",
    "    start_step = cfg.get(\"start_step\", None)\n",
    "    ret = checkpoint_io.load(\n",
    "        booster,\n",
    "        cfg.load,\n",
    "        model=model,\n",
    "        ema=ema,\n",
    "        optimizer=optimizer,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        sampler=(\n",
    "            None if start_step is not None else sampler\n",
    "        ),  # if specify start step, set last_micro_batch_access_index of a new sampler instead\n",
    "    )\n",
    "    if start_step is not None:\n",
    "        # if start step exceeds data length, go to next epoch\n",
    "        if start_step > num_steps_per_epoch:\n",
    "            start_epoch = (\n",
    "                start_epoch + start_step // num_steps_per_epoch\n",
    "                if start_epoch is not None\n",
    "                else start_step // num_steps_per_epoch\n",
    "            )\n",
    "            start_step = start_step % num_steps_per_epoch\n",
    "        sampler.set_step(start_step)\n",
    "\n",
    "    start_epoch = start_epoch if start_epoch is not None else ret[0]\n",
    "    start_step = start_step if start_step is not None else ret[1]\n",
    "\n",
    "    if (\n",
    "        use_discriminator\n",
    "        and os.path.exists(os.path.join(cfg.load, \"discriminator\"))\n",
    "        and not cfg.get(\"restart_disc\", False)\n",
    "    ):\n",
    "        booster.load_model(discriminator, os.path.join(cfg.load, \"discriminator\"))\n",
    "        if cfg.get(\"load_optimizer\", True):\n",
    "            booster.load_optimizer(disc_optimizer, os.path.join(cfg.load, \"disc_optimizer\"))\n",
    "            if disc_lr_scheduler is not None:\n",
    "                booster.load_lr_scheduler(disc_lr_scheduler, os.path.join(cfg.load, \"disc_lr_scheduler\"))\n",
    "            if cfg.get(\"disc_lr\", None) is not None:\n",
    "                set_lr(disc_optimizer, disc_lr_scheduler, cfg.disc_lr)\n",
    "\n",
    "    logger.info(\"Loaded checkpoint %s at epoch %s step %s\", cfg.load, start_epoch, start_step)\n",
    "\n",
    "    if cfg.get(\"lr\", None) is not None:\n",
    "        set_lr(optimizer, lr_scheduler, cfg.lr, cfg.get(\"initial_lr\", None))\n",
    "\n",
    "    if cfg.get(\"update_warmup_steps\", False):\n",
    "        assert (\n",
    "            cfg.lr_scheduler.get(\"warmup_steps\", None) is not None\n",
    "        ), \"you need to set lr_scheduler.warmup_steps in order to pass --update-warmup-steps True\"\n",
    "        set_warmup_steps(lr_scheduler, cfg.lr_scheduler.warmup_steps)\n",
    "        if use_discriminator:\n",
    "            assert (\n",
    "                cfg.disc_lr_scheduler.get(\"warmup_steps\", None) is not None\n",
    "            ), \"you need to set disc_lr_scheduler.warmup_steps in order to pass --update-warmup-steps True\"\n",
    "            set_warmup_steps(disc_lr_scheduler, cfg.disc_lr_scheduler.warmup_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3d82a8",
   "metadata": {},
   "source": [
    "### è¨“ç·´ãƒ«ãƒ¼ãƒ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d9a658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã¨ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã‚’éš›åˆæœŸåŒ–\n",
    "dataloader, sampler = prepare_dataloader(\n",
    "    bucket_config=cfg.get(\"bucket_config\", None),\n",
    "    num_bucket_build_workers=cfg.get(\"num_bucket_build_workers\", 1),\n",
    "    **dataloader_args,\n",
    ")\n",
    "dataloader, sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e12596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æ•£è¨“ç·´ã®åŒæœŸã‚’å¾…æ©Ÿ\n",
    "dist.barrier()\n",
    "\n",
    "# å‹¾é…ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’å–å¾—\n",
    "accumulation_steps = int(cfg.get(\"accumulation_steps\", 1))\n",
    "logger.info(\"Using gradient accumulation steps: %s\", accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7127104",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, cfg_epochs):\n",
    "\n",
    "    # 1) ã‚¨ãƒãƒƒã‚¯ã®åˆæœŸåŒ–\n",
    "\n",
    "    # ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã®ã‚¨ãƒãƒƒã‚¯ã‚’è¨­å®š\n",
    "    sampler.set_epoch(epoch)\n",
    "\n",
    "    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ã‚’ä½œæˆ\n",
    "    dataiter = iter(dataloader)\n",
    "\n",
    "    logger.info(\"Beginning epoch %s...\", epoch)\n",
    "\n",
    "    # ãã‚Œãã‚Œã®ãƒ©ãƒ³ã‚¯ã§ç•°ãªã‚‹ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®š\n",
    "    random.seed(1024 + dist.get_rank())\n",
    "\n",
    "    # 2) ã‚¨ãƒãƒƒã‚¯å†…ã®ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹\n",
    "\n",
    "    with tqdm(\n",
    "        enumerate(dataiter, start=start_step),\n",
    "        desc=f\"Epoch {epoch}\",\n",
    "        disable=not coordinator.is_master(),\n",
    "        total=num_steps_per_epoch,\n",
    "        initial=start_step,\n",
    "    ) as pbar:\n",
    "\n",
    "        # 2-1) ãƒãƒƒãƒã‚’å–å¾—\n",
    "\n",
    "        pbar_iter = iter(pbar)\n",
    "\n",
    "        def fetch_data():\n",
    "            \"éåŒæœŸã«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€ã™ã‚‹\"\n",
    "            # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã‹ã‚‰ãƒãƒƒãƒã‚’å–å¾—\n",
    "            step, batch = next(pbar_iter)\n",
    "            # ãƒ”ãƒ³ç•™ã‚ã•ã‚ŒãŸãƒ“ãƒ‡ã‚ªãƒ†ãƒ³ã‚½ãƒ«ã‚’å–å¾—\n",
    "            pinned_video = batch[\"video\"]\n",
    "            # ãƒ“ãƒ‡ã‚ªãƒ†ãƒ³ã‚½ãƒ«ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€ï¼ˆnon_blocking=Trueã§éåŒæœŸè»¢é€ï¼‰\n",
    "            batch[\"video\"] = pinned_video.to(device, dtype, non_blocking=True)\n",
    "            return batch, step, pinned_video\n",
    "\n",
    "        # æœ€åˆã®ãƒãƒƒãƒã‚’å…ˆèª­ã¿\n",
    "        batch_, step_, pinned_video_ = fetch_data()\n",
    "\n",
    "        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ\n",
    "        # torch.profilerã‚’ä½¿ç”¨ã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¸¬å®šã™ã‚‹\n",
    "        profiler_ctxt = (\n",
    "            profile(\n",
    "                activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "                schedule=my_schedule,\n",
    "                on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./log/profile\"),\n",
    "                record_shapes=True,\n",
    "                profile_memory=True,\n",
    "                with_stack=True,\n",
    "            )\n",
    "            if cfg.get(\"profile\", False)\n",
    "            else nullcontext()\n",
    "        )\n",
    "\n",
    "        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚’é–‹å§‹\n",
    "        with profiler_ctxt:\n",
    "\n",
    "            # 3) ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹\n",
    "\n",
    "            for _ in range(start_step, num_steps_per_epoch):\n",
    "                if cfg.get(\"profile\", False) and _ == WARMUP + ACTIVE + WAIT + 3:\n",
    "                    break\n",
    "\n",
    "                # 3-1) æ¬¡ã®ãƒãƒƒãƒã‚’å…ˆèª­ã¿\n",
    "\n",
    "                batch, step, pinned_video = batch_, step_, pinned_video_\n",
    "\n",
    "                if step + 1 < num_steps_per_epoch:\n",
    "                    batch_, step_, pinned_video_ = fetch_data()\n",
    "\n",
    "                # 3-2) ã‚¹ãƒ†ãƒƒãƒ—ã‚’æ›´æ–°\n",
    "\n",
    "                global_step = epoch * num_steps_per_epoch + step\n",
    "\n",
    "                actual_update_step = (global_step + 1) // accumulation_steps\n",
    "\n",
    "                log_step += 1\n",
    "                acc_step += 1\n",
    "\n",
    "                # 3-3) å‹•ç”»ã¨ç”»åƒã®æ··åˆæˆ¦ç•¥ã‚’é©ç”¨\n",
    "\n",
    "                x = batch[\"video\"]\n",
    "                t_length = x.size(2)\n",
    "                use_video = 1\n",
    "\n",
    "                # True\n",
    "                if mixed_strategy == \"mixed_video_image\":\n",
    "                    # 20%ã®ç¢ºç‡ã§ç”»åƒã®ã¿ã‚’ä½¿ç”¨\n",
    "                    if random.random() < modulated_mixed_image_ratio and dist.get_rank() != 0:\n",
    "                        # NOTE: enable the first rank to use video\n",
    "                        t_length = 1\n",
    "                        use_video = 0\n",
    "                # False\n",
    "                elif mixed_strategy == \"mixed_video_random\":\n",
    "                    t_length = random.randint(1, x.size(2))\n",
    "\n",
    "                # å¿…è¦ã«å¿œã˜ã¦å‹•ç”»ã‚’ç”»åƒã«å¤‰æ›\n",
    "                x = x[:, :, :t_length, :, :]\n",
    "\n",
    "                # ã‚¿ã‚¤ãƒãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’é–‹å§‹\n",
    "                with Timer(\"model\", log=True) if cfg.get(\"profile\", False) else nullcontext():\n",
    "\n",
    "                    # 3-4) ãƒ¢ãƒ‡ãƒ«ã®é †ä¼æ¬ã‚’å®Ÿè¡Œ\n",
    "\n",
    "                    x_rec, posterior, z = model(x)\n",
    "\n",
    "                    # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—ã‚’é€²ã‚ã‚‹\n",
    "                    if cfg.get(\"profile\", False):\n",
    "                        profiler_ctxt.step()\n",
    "\n",
    "                    # ãƒ”ãƒ³ç•™ã‚ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾\n",
    "                    if cache_pin_memory:\n",
    "                        dataiter.remove_cache(pinned_video)\n",
    "\n",
    "                    # 3-5) ã™ã¹ã¦ã®æå¤±ã‚’åˆæœŸåŒ–\n",
    "\n",
    "                    vae_loss = torch.tensor(0.0, device=device, dtype=dtype)\n",
    "                    loss_dict = {}  # loss at every step\n",
    "\n",
    "                    # 3-6) æå¤±ã‚’è¨ˆç®—\n",
    "\n",
    "                    ret = vae_loss_fn(x, x_rec, posterior)\n",
    "                    nll_loss = ret[\"nll_loss\"]\n",
    "                    kl_loss = ret[\"kl_loss\"]\n",
    "                    recon_loss = ret[\"recon_loss\"]\n",
    "                    perceptual_loss = ret[\"perceptual_loss\"]\n",
    "\n",
    "                    # ç·VAEæå¤±ã‚’è¨ˆç®—\n",
    "                    vae_loss += nll_loss + kl_loss\n",
    "\n",
    "                    # GANã®ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿æå¤±ã‚’è¨ˆç®—\n",
    "                    # False\n",
    "                    if use_discriminator:\n",
    "                        # turn off grad update for disc\n",
    "                        discriminator.requires_grad_(False)\n",
    "                        fake_logits = discriminator(x_rec.contiguous())\n",
    "\n",
    "                        generator_loss, g_loss = generator_loss_fn(\n",
    "                            fake_logits,\n",
    "                            nll_loss,\n",
    "                            model.module.get_last_layer(),\n",
    "                            actual_update_step,\n",
    "                            is_training=model.training,\n",
    "                        )\n",
    "\n",
    "                        vae_loss += generator_loss\n",
    "                        # turn on disc training\n",
    "                        discriminator.requires_grad_(True)\n",
    "\n",
    "                    # 3-7) é€†ä¼æ¬ã‚’å®Ÿè¡Œ\n",
    "\n",
    "                    # å‹¾é…ã®åŒæœŸã‚’åˆ¶å¾¡ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¨­å®š\n",
    "                    ctx = (\n",
    "                        booster.no_sync(model, optimizer)\n",
    "                        if cfg.get(\"plugin\", \"zero2\") in (\"zero1\", \"zero1-seq\")\n",
    "                        and (step + 1) % accumulation_steps != 0\n",
    "                        else nullcontext()\n",
    "                    )\n",
    "\n",
    "                    # ã‚¿ã‚¤ãƒãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’é–‹å§‹\n",
    "                    with Timer(\"backward\", log=True) if cfg.get(\"profile\", False) else nullcontext():\n",
    "                        with ctx:\n",
    "                            booster.backward(loss=vae_loss / accumulation_steps, optimizer=optimizer)\n",
    "\n",
    "                    # 3-8) ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°\n",
    "\n",
    "                    # ã‚¿ã‚¤ãƒãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’é–‹å§‹\n",
    "                    with Timer(\"optimizer\", log=True) if cfg.get(\"profile\", False) else nullcontext():\n",
    "\n",
    "                        if (step + 1) % accumulation_steps == 0:\n",
    "                            optimizer.step()\n",
    "                            optimizer.zero_grad()\n",
    "                            if lr_scheduler is not None:\n",
    "                                lr_scheduler.step(\n",
    "                                    actual_update_step,\n",
    "                                )\n",
    "\n",
    "                            # 3-9) EMAãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°\n",
    "\n",
    "                            if ema is not None:\n",
    "                                update_ema(\n",
    "                                    ema,\n",
    "                                    model.unwrap(),\n",
    "                                    optimizer=optimizer,\n",
    "                                    decay=cfg.get(\"ema_decay\", 0.9999),\n",
    "                                )\n",
    "\n",
    "                # 3-10) æå¤±ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "\n",
    "                log_loss(\"all\", vae_loss, loss_dict, use_video)\n",
    "                log_loss(\"nll\", nll_loss, loss_dict, use_video)\n",
    "                log_loss(\"nll_rec\", recon_loss, loss_dict, use_video)\n",
    "                log_loss(\"nll_per\", perceptual_loss, loss_dict, use_video)\n",
    "                log_loss(\"kl\", kl_loss, loss_dict, use_video)\n",
    "                if use_discriminator:\n",
    "                    log_loss(\"gen_w\", generator_loss, loss_dict, use_video)\n",
    "                    log_loss(\"gen\", g_loss, loss_dict, use_video)\n",
    "\n",
    "                # 3-11) è­˜åˆ¥å™¨ã®æå¤±ã®è¨ˆç®—ã¨æ›´æ–°\n",
    "\n",
    "                # False\n",
    "                if use_discriminator:\n",
    "                    real_logits = discriminator(x.detach().contiguous())\n",
    "                    fake_logits = discriminator(x_rec.detach().contiguous())\n",
    "                    disc_loss = discriminator_loss_fn(\n",
    "                        real_logits,\n",
    "                        fake_logits,\n",
    "                        actual_update_step,\n",
    "                    )\n",
    "\n",
    "                    # è­˜åˆ¥å™¨ã®é€†ä¼æ’­ã¨æ›´æ–°\n",
    "\n",
    "                    ctx = (\n",
    "                        booster.no_sync(discriminator, disc_optimizer)\n",
    "                        if cfg.get(\"plugin\", \"zero2\") in (\"zero1\", \"zero1-seq\")\n",
    "                        and (step + 1) % accumulation_steps != 0\n",
    "                        else nullcontext()\n",
    "                    )\n",
    "                    with ctx:\n",
    "                        booster.backward(loss=disc_loss / accumulation_steps, optimizer=disc_optimizer)\n",
    "                    if (step + 1) % accumulation_steps == 0:\n",
    "                        disc_optimizer.step()\n",
    "                        disc_optimizer.zero_grad()\n",
    "                        if disc_lr_scheduler is not None:\n",
    "                            disc_lr_scheduler.step(actual_update_step)\n",
    "\n",
    "                    # è­˜åˆ¥å™¨ã®æå¤±ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "                    log_loss(\"disc\", disc_loss, loss_dict, use_video)\n",
    "\n",
    "\n",
    "                if (global_step + 1) % accumulation_steps == 0:\n",
    "\n",
    "                    # 3-12) tensorboardã¨wandbã¸ã®ãƒ­ã‚°è¨˜éŒ²\n",
    "\n",
    "                    if coordinator.is_master() and actual_update_step % cfg.get(\"log_every\", 1) == 0:\n",
    "                        avg_loss = {k: v / log_step for k, v in running_loss.items()}\n",
    "                        # progress bar\n",
    "                        pbar.set_postfix(\n",
    "                            {\n",
    "                                # \"step\": step,\n",
    "                                # \"global_step\": global_step,\n",
    "                                # \"actual_update_step\": actual_update_step,\n",
    "                                # \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                                **{k: f\"{v:.2f}\" for k, v in avg_loss.items()},\n",
    "                            }\n",
    "                        )\n",
    "                        # tensorboard\n",
    "                        tb_writer.add_scalar(\"loss\", vae_loss.item(), actual_update_step)\n",
    "                        # wandb\n",
    "                        if cfg.get(\"wandb\", False):\n",
    "                            wandb.log(\n",
    "                                {\n",
    "                                    \"iter\": global_step,\n",
    "                                    \"epoch\": epoch,\n",
    "                                    \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                                    \"avg_loss_\": avg_loss,\n",
    "                                    \"avg_loss\": avg_loss[\"all\"],\n",
    "                                    \"loss_\": loss_dict,\n",
    "                                    \"loss\": vae_loss.item(),\n",
    "                                    \"global_grad_norm\": optimizer.get_grad_norm(),\n",
    "                                },\n",
    "                                step=actual_update_step,\n",
    "                            )\n",
    "\n",
    "                        running_loss = {k: 0.0 for k in running_loss}\n",
    "                        log_step = 0\n",
    "\n",
    "                    # 3-13) ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜\n",
    "\n",
    "                    ckpt_every = cfg.get(\"ckpt_every\", 0)\n",
    "                    if ckpt_every > 0 and actual_update_step % ckpt_every == 0 and coordinator.is_master():\n",
    "                        subprocess.run(\"sudo drop_cache\", shell=True)\n",
    "\n",
    "                    if ckpt_every > 0 and actual_update_step % ckpt_every == 0:\n",
    "                        # mannually garbage collection\n",
    "                        gc.collect()\n",
    "\n",
    "                        save_dir = checkpoint_io.save(\n",
    "                            booster,\n",
    "                            exp_dir,\n",
    "                            model=model,\n",
    "                            ema=ema,\n",
    "                            optimizer=optimizer,\n",
    "                            lr_scheduler=lr_scheduler,\n",
    "                            sampler=sampler,\n",
    "                            epoch=epoch,\n",
    "                            step=step + 1,\n",
    "                            global_step=global_step + 1,\n",
    "                            batch_size=cfg.get(\"batch_size\", None),\n",
    "                            actual_update_step=actual_update_step,\n",
    "                            ema_shape_dict=ema_shape_dict,\n",
    "                            async_io=True,\n",
    "                        )\n",
    "\n",
    "                        if is_log_process(plugin_type, plugin_config):\n",
    "                            os.system(f\"chgrp -R share {save_dir}\")\n",
    "\n",
    "                        if use_discriminator:\n",
    "                            booster.save_model(discriminator, os.path.join(save_dir, \"discriminator\"), shard=True)\n",
    "                            booster.save_optimizer(\n",
    "                                disc_optimizer,\n",
    "                                os.path.join(save_dir, \"disc_optimizer\"),\n",
    "                                shard=True,\n",
    "                                size_per_shard=4096,\n",
    "                            )\n",
    "                            if disc_lr_scheduler is not None:\n",
    "                                booster.save_lr_scheduler(\n",
    "                                    disc_lr_scheduler, os.path.join(save_dir, \"disc_lr_scheduler\")\n",
    "                                )\n",
    "                        dist.barrier()\n",
    "\n",
    "                        logger.info(\n",
    "                            \"Saved checkpoint at epoch %s, step %s, global_step %s to %s\",\n",
    "                            epoch,\n",
    "                            step + 1,\n",
    "                            actual_update_step,\n",
    "                            save_dir,\n",
    "                        )\n",
    "\n",
    "                        # remove old checkpoints\n",
    "                        rm_checkpoints(exp_dir, keep_n_latest=cfg.get(\"keep_n_latest\", -1))\n",
    "                        logger.info(\n",
    "                            \"Removed old checkpoints and kept %s latest ones.\", cfg.get(\"keep_n_latest\", -1)\n",
    "                        )\n",
    "\n",
    "        if cfg.get(\"profile\", False):\n",
    "            profiler_ctxt.export_chrome_trace(\"./log/profile/trace.json\")\n",
    "\n",
    "    sampler.reset()\n",
    "    start_step = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
